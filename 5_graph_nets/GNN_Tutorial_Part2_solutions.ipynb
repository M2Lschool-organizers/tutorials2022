{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qRtBIRzjqxh"
      },
      "source": [
        "**Outline part 2:**\n",
        "* [Discovering GraphNetwork](#scrollTo=n5TxaTGzBkBa)\n",
        "* [Graph Classification on MUTAG (Molecules)](#scrollTo=n5TxaTGzBkBa)\n",
        "* [Link Prediction on CORA (Citation Network)](#scrollTo=OwVE88dTRC6V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyH_VgjW7Bp3"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/deepmind/jraph.git\n",
        "!pip install flax\n",
        "!pip install dm-haiku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJm7y6GH3WyB"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "%matplotlib inline\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as tree\n",
        "import jraph\n",
        "import flax\n",
        "import haiku as hk\n",
        "import optax\n",
        "import pickle\n",
        "import numpy as onp\n",
        "import networkx as nx\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwNtsA9cj9E7"
      },
      "outputs": [],
      "source": [
        "# Utils for visualization\n",
        "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
        "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
        "  nx_graph = nx.DiGraph()\n",
        "  if nodes is None:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n)\n",
        "  else:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n, node_feature=nodes[n])\n",
        "  if edges is None:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
        "  else:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(\n",
        "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
        "  return nx_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7q5ySSmVL3x"
      },
      "outputs": [],
      "source": [
        "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
        "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(\n",
        "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np1NBMZW9YOp"
      },
      "source": [
        "## [Optional for fun I] Graph Network Model Definition\n",
        "\n",
        "We will use `jraph.GraphNetwork()` to build our graph model. The `GraphNetwork` architecture is defined in [Battaglia et al. (2018)](https://arxiv.org/pdf/1806.01261.pdf).\n",
        "\n",
        "We first define update functions for nodes and edges. We will use MLP blocks for all three. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkn2WgDsarq7"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  # Define an MLP(128,128)\n",
        "\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # net = ...\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(128)])\n",
        "  ################\n",
        "\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  # Define an MLP(128,128)\n",
        "\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # net = ...\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(128)])\n",
        "  \n",
        "  ################\n",
        "  return net(feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXG18tdo0hgx"
      },
      "source": [
        "Now we want to make our Graph Network suitable for the task of graph classification.\n",
        "\n",
        "The main difference from node classification task is that instead of observing individual node latents, we are now attempting to summarize them into one embedding vector, representative of the entire graph, which we then use to predict the class of this graph.\n",
        "\n",
        "Hence, we need to implement the update function for the full graph (global), using an MLP that eventually classifies it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56Y3KzO90hgy"
      },
      "outputs": [],
      "source": [
        "@jraph.concatenated_args\n",
        "def update_global_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Global update function for graph net.\"\"\"\n",
        "  # Define an MLP(128,2) as it must provide a prediction for the graph\n",
        "  # assuming a standard binary classification task\n",
        "\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # net = ...\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(2)])\n",
        "  ################\n",
        "  return net(feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl4fccTX0hgy"
      },
      "source": [
        "We are ready to define the network builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKtjYmCK0hgy"
      },
      "outputs": [],
      "source": [
        "def net_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  # Add a global paramater for graph classification.\n",
        "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "\n",
        "  # 1. Define an embedder function that will linearly project\n",
        "  # the input graph, i.e., the nodes, the edges, and the globals.\n",
        "  # Project them to 128-dimensional space. \n",
        "  # Check out jraph.GraphMapFeatures to quickly perform this step.\n",
        "\n",
        "  # 2. Define the network using jraph.GraphNetwork and the \n",
        "  # update functions defined previously\n",
        "  \n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # embedder = ...\n",
        "  # net = ...\n",
        "  embedder = jraph.GraphMapFeatures(\n",
        "      hk.Linear(128), hk.Linear(128), hk.Linear(128))\n",
        "  net = jraph.GraphNetwork(\n",
        "      update_node_fn=node_update_fn,\n",
        "      update_edge_fn=edge_update_fn,\n",
        "      update_global_fn=update_global_fn)\n",
        "  ################\n",
        "\n",
        "  return net(embedder(graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5TxaTGzBkBa"
      },
      "source": [
        "## [Optional for fun II] Graph Classification on MUTAG (Molecules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WSZyACzkZKu"
      },
      "source": [
        "\n",
        "Let's now try the model on a real graph classification scenario -- **molecular property prediction**, where molecules are represented as graphs. Nodes correspond to atoms, and edges represent the bonds between them.\n",
        "\n",
        "We use the **MUTAG** dataset for this example, a common dataset from the [TUDatasets](https://chrsmrrs.github.io/datasets/) collection.\n",
        "\n",
        "We have converted this dataset to be compatible with jraph and will download it in the cell below.\n",
        "\n",
        "Citation for TUDatasets: [Morris, Christopher, et al. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663. 2020.](https://chrsmrrs.github.io/datasets/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO_SOVcmOH_q"
      },
      "outputs": [],
      "source": [
        "# Download jraph version of MUTAG.\n",
        "!wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/mutag.pickle\n",
        "with open('/tmp/mutag.pickle', 'rb') as f:\n",
        "  mutag_ds = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs32Jv8chdLM"
      },
      "source": [
        "The dataset is saved as a list of examples, each example is a dictionary containing an input_graph and its corresponding target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1oCfEIoJypp"
      },
      "outputs": [],
      "source": [
        "len(mutag_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVLUk5LAKBgT"
      },
      "outputs": [],
      "source": [
        "# Inspect the first graph\n",
        "g = mutag_ds[0]['input_graph']\n",
        "print(f'Number of nodes: {g.n_node[0]}')\n",
        "print(f'Number of edges: {g.n_edge[0]}')\n",
        "print(f'Node features shape: {g.nodes.shape}')\n",
        "print(f'Edge features shape: {g.edges.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ62jkKuyWRk"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4_uZqBxKjtz"
      },
      "outputs": [],
      "source": [
        "# Target for first graph\n",
        "print(f\"Target: {mutag_ds[0]['target']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yVF5gt8oz2N"
      },
      "source": [
        "We see that there are 188 graphs, to be classified in one of 2 classes, representing \"their mutagenic effect on a specific gram negative bacterium\". Node features represent the 1-hot encoding of the atom type (0=C, 1=N, 2=O, 3=F, 4=I, 5=Cl, 6=Br). Edge features (`edge_attr`) represent the bond type, which we will here ignore.\n",
        "\n",
        "Let's split the dataset to use the first 150 graphs as the training set (and the rest as the test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwWngXmKtWa5"
      },
      "outputs": [],
      "source": [
        "train_mutag_ds = mutag_ds[:150]\n",
        "test_mutag_ds = mutag_ds[150:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOIDRiUq8nNK"
      },
      "source": [
        "#### Padding Graphs to Speed Up Training\n",
        "\n",
        "Since jax recompiles the program for each graph size, training would take a long time due to recompilation for different graph sizes. To address that, we pad the number of nodes and edges in the graphs to nearest power of two. Since jax maintains a cache\n",
        "of compiled programs, the compilation cost is amortized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGhnsIovZQpo"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "def _nearest_bigger_power_of_two(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y < x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6qHvGkbKqS"
      },
      "source": [
        "#### Loss and Accuracy Function\n",
        "Define the classification cross-entropy loss and accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vEmAsr5bKN8"
      },
      "outputs": [],
      "source": [
        "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple, label: jnp.ndarray,\n",
        "                 net: jraph.GraphsTuple) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Computes loss and accuracy.\"\"\"\n",
        "  pred_graph = net.apply(params, graph)\n",
        "  preds = jax.nn.log_softmax(pred_graph.globals)\n",
        "  targets = jax.nn.one_hot(label, 2)\n",
        "\n",
        "  # Since we have an extra 'dummy' graph in our batch due to padding, we want\n",
        "  # to mask out any loss associated with the dummy graph.\n",
        "  # Since we padded with `pad_with_graphs` we can recover the mask by using\n",
        "  # get_graph_padding_mask.\n",
        "  mask = jraph.get_graph_padding_mask(pred_graph)\n",
        "\n",
        "  # Cross entropy loss.\n",
        "  loss = -jnp.mean(preds * targets * mask[:, None])\n",
        "\n",
        "  # Accuracy taking into account the mask.\n",
        "  accuracy = jnp.sum(\n",
        "      (jnp.argmax(pred_graph.globals, axis=1) == label) * mask) / jnp.sum(mask)\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OdXLDCibivA"
      },
      "source": [
        "#### Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8rTdn3Jbh9-"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "def train(dataset: List[Dict[str, Any]], num_train_steps: int) -> hk.Params:\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "\n",
        "  # Initialize the network.\n",
        "  params = net.init(jax.random.PRNGKey(42), graph)\n",
        "  # Initialize the optimizer.\n",
        "  opt_init, opt_update = optax.adam(1e-4)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(jax.value_and_grad(\n",
        "      compute_loss_fn, has_aux=True))\n",
        "\n",
        "  for idx in range(num_train_steps):\n",
        "    graph = dataset[idx % len(dataset)]['input_graph']\n",
        "    label = dataset[idx % len(dataset)]['target']\n",
        "    # Jax will re-jit your graphnet every time a new graph shape is encountered.\n",
        "    # In the limit, this means a new compilation every training step, which\n",
        "    # will result in *extremely* slow training. To prevent this, pad each\n",
        "    # batch of graphs to the nearest power of two. Since jax maintains a cache\n",
        "    # of compiled programs, the compilation cost is amortized.\n",
        "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
        "\n",
        "    # Since padding is implemented with pad_with_graphs, an extra graph has\n",
        "    # been added to the batch, which means there should be an extra label.\n",
        "    label = jnp.concatenate([label, jnp.array([0])])\n",
        "\n",
        "    (loss, acc), grad = compute_loss_fn(params, graph, label)\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if idx % 50 == 0:\n",
        "      print(f'step: {idx}, loss: {loss}, acc: {acc}')\n",
        "  print('Training finished')\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOuB_EtDZQrw"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataset: List[Dict[str, Any]],\n",
        "             params: hk.Params) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Evaluation Script.\"\"\"\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "  accumulated_loss = 0\n",
        "  accumulated_accuracy = 0\n",
        "  compute_loss_fn = jax.jit(functools.partial(compute_loss, net=net))\n",
        "  for idx in range(len(dataset)):\n",
        "    graph = dataset[idx]['input_graph']\n",
        "    label = dataset[idx]['target']\n",
        "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
        "    label = jnp.concatenate([label, jnp.array([0])])\n",
        "    loss, acc = compute_loss_fn(params, graph, label)\n",
        "    accumulated_accuracy += acc\n",
        "    accumulated_loss += loss\n",
        "    if idx % 100 == 0:\n",
        "      print(f'Evaluated {idx + 1} graphs')\n",
        "  print('Completed evaluation.')\n",
        "  loss = accumulated_loss / idx\n",
        "  accuracy = accumulated_accuracy / idx\n",
        "  print(f'Eval loss: {loss}, accuracy {accuracy}')\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzCaMsTRZQti"
      },
      "outputs": [],
      "source": [
        "params = train(train_mutag_ds, num_train_steps=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6Yn4WX3c7ol"
      },
      "outputs": [],
      "source": [
        "evaluate(test_mutag_ds, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjEWtqZ14GnM"
      },
      "source": [
        "We converge at ~76% test accuracy. We could of course further tune the parameters to improve this result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwVE88dTRC6V"
      },
      "source": [
        "## [Optional for fun III] Link prediction on CORA (Citation Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP-j7Zq59uRZ"
      },
      "source": [
        "The final problem type we will explore is **link prediction**, an instance of an **edge-level** task. Given a graph, our goal is to predict whether a certain edge $(u,v)$ should be present or not. This is often useful in the recommender system settings (e.g., propose new friends in a social network, propose a movie to a user).\n",
        "\n",
        "As before, the first step is to obtain node latents $h_i$ using a GNN. In this context we will use the autoencoder language and call this GNN **encoder**. Then, we learn a binary classifier $f: (h_i, h_j) \\to z_{i,j}$ (**decoder**), predicting if an edge $(i,j)$ should exist or not. While we could use a more elaborate decoder (e.g., an MLP), a common approach we will also use here is to focus on obtaining good node embeddings, and for the decoder simply use the similarity between node latents, i.e. $z_{i,j} = h_i^T h_j$. Note that when defining the decoder, we need to consider whether the graph have undirected or directed graph; in the first case the function $f$ must be invariant w.r.t. the order of the nodes in each edge, i.e., $f(h_i, h_j) = z_{i,j} = z_{j,i} = f(h_j, h_i)$, while in the second case $f$ can also be variant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfxJdcNCeY7"
      },
      "source": [
        "For this problem we will use the [**Cora** dataset](https://linqs.github.io/linqs-website/datasets/#cora), a citation graph containing 2708 scientific publications. For each publication we have a 1433-dimensional feature vector, which is a bag-of-words representation (with a small, fixed dictionary) of the paper text. The edges in this graph represent citations, and are commonly treated as undirected. Each paper is in one of seven topics (classes) so you can also use this dataset for node classification.\n",
        "\n",
        "Similar to MUTAG, we have converted this dataset to jraph for you.\n",
        "\n",
        "Citation for the use of the Cora dataset:\n",
        "- [Qing Lu and Lise Getoor. Link-Based Classification. International Conference on Machine Learning. 2003.](https://linqs.github.io/linqs-website/publications/#id:lu-icml03)\n",
        "- [Sen, Prithviraj, et al. Collective classification in network data. AI magazine 29.3. 2008.](https://linqs.github.io/linqs-website/datasets/#cora)\n",
        "- [Dataset download link](https://linqs.github.io/linqs-website/datasets/#cora)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qo4jSfHtdQ9"
      },
      "outputs": [],
      "source": [
        "# Download jraph version of Cora.\n",
        "!wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/cora.pickle\n",
        "with open('/tmp/cora.pickle', 'rb') as f:\n",
        "  cora_ds = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lc0O4_h9Bfv"
      },
      "source": [
        "#### Splitting Edges and Adding \"Negative\" Edges\n",
        "For the link prediction task, we split the edges into train, val and test sets and also add \"negative\" examples (edges that do not correspond to a citation). We will ignore the topic classes.\n",
        "\n",
        "For the validation and test splits, we add the same number of existing edges (\"positive examples\") and non-existing edges (\"negative examples\").\n",
        "\n",
        "In contrast to the validation and test splits, the training split only contains positive examples (set $T_+$). The $|T_+|$ negative examples to be used during training will be sampled ad hoc in each epoch and uniformly at random from all edges that are not in $T_+$. This allows the model to see a wider range of negative examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqZyzwpC3p-u"
      },
      "outputs": [],
      "source": [
        "def train_val_test_split_edges(graph: jraph.GraphsTuple,\n",
        "                               val_perc: float = 0.05,\n",
        "                               test_perc: float = 0.1):\n",
        "  \"\"\"Split edges in input graph into train, val and test splits.\n",
        "\n",
        "  For val and test sets, also include negative edges.\n",
        "  Based on torch_geometric.utils.train_test_split_edges.\n",
        "  \"\"\"\n",
        "  mask = graph.senders < graph.receivers\n",
        "  senders = graph.senders[mask]\n",
        "  receivers = graph.receivers[mask]\n",
        "  num_val = int(val_perc * senders.shape[0])\n",
        "  num_test = int(test_perc * senders.shape[0])\n",
        "  permuted_indices = onp.random.permutation(range(senders.shape[0]))\n",
        "  senders = senders[permuted_indices]\n",
        "  receivers = receivers[permuted_indices]\n",
        "  if graph.edges is not None:\n",
        "    edges = graph.edges[permuted_indices]\n",
        "\n",
        "  val_senders = senders[:num_val]\n",
        "  val_receivers = receivers[:num_val]\n",
        "  if graph.edges is not None:\n",
        "    val_edges = edges[:num_val]\n",
        "\n",
        "  test_senders = senders[num_val:num_val + num_test]\n",
        "  test_receivers = receivers[num_val:num_val + num_test]\n",
        "  if graph.edges is not None:\n",
        "    test_edges = edges[num_val:num_val + num_test]\n",
        "\n",
        "  train_senders = senders[num_val + num_test:]\n",
        "  train_receivers = receivers[num_val + num_test:]\n",
        "  train_edges = None\n",
        "  if graph.edges is not None:\n",
        "    train_edges = edges[num_val + num_test:]\n",
        "\n",
        "  # make training edges undirected by adding reverse edges back in\n",
        "  train_senders_undir = jnp.concatenate((train_senders, train_receivers))\n",
        "  train_receivers_undir = jnp.concatenate((train_receivers, train_senders))\n",
        "  train_senders = train_senders_undir\n",
        "  train_receivers = train_receivers_undir\n",
        "\n",
        "  # Negative edges.\n",
        "  num_nodes = graph.n_node[0]\n",
        "  # Create a negative adjacency mask, s.t. mask[i, j] = True iff edge i->j does\n",
        "  # not exist in the original graph.\n",
        "  neg_adj_mask = onp.ones((num_nodes, num_nodes), dtype=onp.uint8)\n",
        "  # upper triangular part\n",
        "  neg_adj_mask = onp.triu(neg_adj_mask, k=1)\n",
        "  neg_adj_mask[graph.senders, graph.receivers] = 0\n",
        "  neg_adj_mask = neg_adj_mask.astype(onp.bool)\n",
        "  neg_senders, neg_receivers = neg_adj_mask.nonzero()\n",
        "\n",
        "  perm = onp.random.permutation(range(len(neg_senders)))\n",
        "  neg_senders = neg_senders[perm]\n",
        "  neg_receivers = neg_receivers[perm]\n",
        "\n",
        "  val_neg_senders = neg_senders[:num_val]\n",
        "  val_neg_receivers = neg_receivers[:num_val]\n",
        "  test_neg_senders = neg_senders[num_val:num_val + num_test]\n",
        "  test_neg_receivers = neg_receivers[num_val:num_val + num_test]\n",
        "\n",
        "  train_graph = jraph.GraphsTuple(\n",
        "      nodes=graph.nodes,\n",
        "      edges=train_edges,\n",
        "      senders=train_senders,\n",
        "      receivers=train_receivers,\n",
        "      n_node=graph.n_node,\n",
        "      n_edge=jnp.array([len(train_senders)]),\n",
        "      globals=graph.globals)\n",
        "\n",
        "  return train_graph, neg_adj_mask, val_senders, val_receivers, val_neg_senders, val_neg_receivers, test_senders, test_receivers, test_neg_senders, test_neg_receivers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cE2CNtN-G3D"
      },
      "source": [
        "#### Test the Edge Splitting Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhBruorYJPm6"
      },
      "outputs": [],
      "source": [
        "graph = cora_ds[0]['input_graph']\n",
        "train_graph, neg_adj_mask, val_pos_senders, val_pos_receivers, val_neg_senders, val_neg_receivers, test_pos_senders, test_pos_receivers, test_neg_senders, test_neg_receivers = train_val_test_split_edges(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOMs84n4Gz1e"
      },
      "outputs": [],
      "source": [
        "print(f'Train set: {train_graph.senders.shape[0]} positive edges, we will sample the same number of negative edges at runtime')\n",
        "print(f'Val set: {val_pos_senders.shape[0]} positive edges, {val_neg_senders.shape[0]} negative edges')\n",
        "print(f'Test set: {test_pos_senders.shape[0]} positive edges, {test_neg_senders.shape[0]} negative edges')\n",
        "print(f'Negative adjacency mask shape: {neg_adj_mask.shape}')\n",
        "print(f'Numbe of negative edges to sample from: {neg_adj_mask.sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzwvygx-BS6e"
      },
      "source": [
        "\n",
        "*Note*: It will often happen during training that as a negative example, we sample an initially existing edge (that is now e.g. a positive example in the test set). We are however not allowed to check for this, as we should be unaware of the existence of test edges during training.\n",
        "\n",
        "Assuming our dot product decoder, we are essentially attempting to bring the latents of endpoints of edges from $T_+$ closer together, and make the latents of all other pairs of nodes as distant as possible. As this is impossible to fully satisfy, the hope is that the model will \"fail\" to distance those pairs of nodes where the edges should actually exist (positive examples from the test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGRJAyfLwed"
      },
      "source": [
        "#### Graph Network Model Definition\n",
        "\n",
        "We will use jraph.GraphNetwork to build our graph net model.\n",
        "\n",
        "We first define update functions for node features. We are not using edge or global features for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC3AMCrJLwek"
      },
      "outputs": [],
      "source": [
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  # Define an MLP(128,64)\n",
        "\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # net = ...\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(64)])\n",
        "  ################\n",
        "  return net(feats)\n",
        "\n",
        "\n",
        "def net_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Network definition.\"\"\"\n",
        "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "  net = jraph.GraphNetwork(\n",
        "      update_node_fn=node_update_fn, update_edge_fn=None, update_global_fn=None)\n",
        "  return net(graph)\n",
        "\n",
        "\n",
        "def decode(pred_graph: jraph.GraphsTuple, senders: jnp.ndarray,\n",
        "           receivers: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Given a set of candidate edges, take dot product of respective nodes.\n",
        "\n",
        "  Args:\n",
        "    pred_graph: input graph.\n",
        "    senders: Senders of candidate edges.\n",
        "    receivers: Receivers of candidate edges.\n",
        "\n",
        "  Returns:\n",
        "    For each edge, computes dot product of the features of the two nodes.\n",
        "\n",
        "  \"\"\" \n",
        "  # Implement the decoder as the dot product between node latents.\n",
        "\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # similarity = ...\n",
        "  similarity = jnp.sum(pred_graph.nodes[senders] * pred_graph.nodes[receivers], axis=1)\n",
        "  ################\n",
        "  return jnp.squeeze(similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmSjMB_TGAUE"
      },
      "source": [
        "To evaluate our model, we first apply the sigmoid function to obtained dot products to get a score $s_{i,j} \\in [0,1]$ for each edge. Now, we can pick a threshold $\\tau$ and say that we predict all pairs $(i,j)$ s.t. $s_{i,j} \\geq \\tau$ as edges (and all the rest as non-edges)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGz7iRGmLwel"
      },
      "source": [
        "#### Loss and ROC-AUC-Metric Function\n",
        "Define the binary classification cross-entropy loss.\n",
        "To aggregate the results over all choices of $\\tau$, we will use ROC-AUC (the area under the ROC curve) as our evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ld4b3D6Lwel"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_bce_with_logits_loss(x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Computes binary cross-entropy with logits loss.\n",
        "\n",
        "  Combines sigmoid and BCE, and uses log-sum-exp trick for numerical stability.\n",
        "  See https://stackoverflow.com/a/66909858 if you want to learn more.\n",
        "\n",
        "  Args:\n",
        "    x: Predictions (logits).\n",
        "    y: Labels.\n",
        "\n",
        "  Returns:\n",
        "    Binary cross-entropy loss with mean aggregation.\n",
        "\n",
        "  \"\"\"\n",
        "  max_val = jnp.clip(x, 0, None)\n",
        "  loss = x - x * y + max_val + jnp.log(\n",
        "      jnp.exp(-max_val) + jnp.exp((-x - max_val)))\n",
        "  return loss.mean()\n",
        "\n",
        "\n",
        "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple,\n",
        "                 senders: jnp.ndarray, receivers: jnp.ndarray,\n",
        "                 labels: jnp.ndarray,\n",
        "                 net: hk.Transformed) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Computes loss.\"\"\"\n",
        "  pred_graph = net.apply(params, graph)\n",
        "  preds = decode(pred_graph, senders, receivers)\n",
        "  loss = compute_bce_with_logits_loss(preds, labels)\n",
        "  return loss, preds\n",
        "\n",
        "\n",
        "def compute_roc_auc_score(preds: jnp.ndarray,\n",
        "                          labels: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Computes roc auc (area under the curve) score for classification.\"\"\"\n",
        "  s = jax.nn.sigmoid(preds)\n",
        "  roc_auc = roc_auc_score(labels, s)\n",
        "  return roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58D3mm9blJFD"
      },
      "source": [
        "Helper function for sampling negative edges during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTZ4CwVDWrc9"
      },
      "outputs": [],
      "source": [
        "def negative_sampling(\n",
        "    graph: jraph.GraphsTuple, num_neg_samples: int,\n",
        "    key: jnp.DeviceArray) -> Tuple[jnp.DeviceArray, jnp.DeviceArray]:\n",
        "  \"\"\"Samples negative edges, i.e. edges that don't exist in the input graph.\"\"\"\n",
        "  num_nodes = graph.n_node[0]\n",
        "  total_possible_edges = num_nodes**2\n",
        "  # convert 2D edge indices to 1D representation.\n",
        "  pos_idx = graph.senders * num_nodes + graph.receivers\n",
        "\n",
        "  # Percentage to oversample edges, so most likely will sample enough neg edges.\n",
        "  alpha = jnp.abs(1 / (1 - 1.1 *\n",
        "                       (graph.senders.shape[0] / total_possible_edges)))\n",
        "\n",
        "  perm = jax.random.randint(\n",
        "      key,\n",
        "      shape=(int(alpha * num_neg_samples),),\n",
        "      minval=0,\n",
        "      maxval=total_possible_edges,\n",
        "      dtype=jnp.uint32)\n",
        "\n",
        "  # mask where sampled edges are positive edges.\n",
        "  mask = jnp.isin(perm, pos_idx)\n",
        "  # remove positive edges.\n",
        "  perm = perm[~mask][:num_neg_samples]\n",
        "\n",
        "  # convert 1d back to 2d edge indices.\n",
        "  neg_senders = perm // num_nodes\n",
        "  neg_receivers = perm % num_nodes\n",
        "\n",
        "  return neg_senders, neg_receivers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5xUW5XkL3YS"
      },
      "source": [
        "Let's write the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAjfWFduSESI"
      },
      "outputs": [],
      "source": [
        "def train(dataset: List[Dict[str, Any]], num_epochs: int) -> hk.Params:\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "\n",
        "  train_graph, _, val_pos_s, val_pos_r, val_neg_s, val_neg_r, test_pos_s, \\\n",
        "      test_pos_r, test_neg_s, test_neg_r = train_val_test_split_edges(\n",
        "      graph)\n",
        "\n",
        "  # Prepare the validation and test data.\n",
        "  val_senders = jnp.concatenate((val_pos_s, val_neg_s))\n",
        "  val_receivers = jnp.concatenate((val_pos_r, val_neg_r))\n",
        "  val_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(val_pos_s)), jnp.zeros(len(val_neg_s))))\n",
        "  test_senders = jnp.concatenate((test_pos_s, test_neg_s))\n",
        "  test_receivers = jnp.concatenate((test_pos_r, test_neg_r))\n",
        "  test_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(test_pos_s)), jnp.zeros(len(test_neg_s))))\n",
        "  # Initialize the network.\n",
        "  params = net.init(key, train_graph)\n",
        "  # Initialize the optimizer.\n",
        "  opt_init, opt_update = optax.adam(1e-4)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(jax.value_and_grad(compute_loss_fn, has_aux=True))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    num_neg_samples = train_graph.senders.shape[0]\n",
        "    train_neg_senders, train_neg_receivers = negative_sampling(\n",
        "        train_graph, num_neg_samples=num_neg_samples, key=key)\n",
        "    train_senders = jnp.concatenate((train_graph.senders, train_neg_senders))\n",
        "    train_receivers = jnp.concatenate(\n",
        "        (train_graph.receivers, train_neg_receivers))\n",
        "    train_labels = jnp.concatenate(\n",
        "        (jnp.ones(len(train_graph.senders)), jnp.zeros(len(train_neg_senders))))\n",
        "\n",
        "    (train_loss,\n",
        "     train_preds), grad = compute_loss_fn(params, train_graph, train_senders,\n",
        "                                          train_receivers, train_labels)\n",
        "\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if epoch % 10 == 0 or epoch == (num_epochs - 1):\n",
        "      train_roc_auc = compute_roc_auc_score(train_preds, train_labels)\n",
        "      val_loss, val_preds = compute_loss(params, train_graph, val_senders,\n",
        "                                         val_receivers, val_labels, net)\n",
        "      val_roc_auc = compute_roc_auc_score(val_preds, val_labels)\n",
        "      print(f'epoch: {epoch}, train_loss: {train_loss:.3f}, '\n",
        "            f'train_roc_auc: {train_roc_auc:.3f}, val_loss: {val_loss:.3f}, '\n",
        "            f'val_roc_auc: {val_roc_auc:.3f}')\n",
        "  test_loss, test_preds = compute_loss(params, train_graph, test_senders,\n",
        "                                       test_receivers, test_labels, net)\n",
        "  test_roc_auc = compute_roc_auc_score(test_preds, test_labels)\n",
        "  print('Training finished')\n",
        "  print(\n",
        "      f'epoch: {epoch}, test_loss: {test_loss:.3f}, test_roc_auc: {test_roc_auc:.3f}'\n",
        "  )\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ealPntyvdxyT"
      },
      "source": [
        "Let's train the model! We expect the model to reach roughly test_roc_auc of 0.84.\n",
        "\n",
        "(Note that ROC-AUC is a scalar between 0 and 1, with 1 being the ROC-AUC of a perfect classifier.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LJtk4WKenoE"
      },
      "outputs": [],
      "source": [
        "params = train(cora_ds, num_epochs=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S8X2UovqE-I"
      },
      "source": [
        "## Summary\n",
        "In this tutorial, we learned about how to represent directed and undirected graphs in jraph, how to create the basic GNN blocks (GCN + GAT) and how to compose them into GNNs. We also learned about three types of graph predictions tasks (node classification, link prediction and graph classification) and trained a model on each of these tasks.\n",
        "\n",
        "\n",
        "To practice your knowledge, you can:\n",
        "* Create your own graphs, e.g. following the toy graph template and adding nodes, edges, features.\n",
        "* Modify the GNN blocks, e.g. by changing the `update_node_fn()` / `update_edge_fn()` functions.\n",
        "* Train a model on a different dataset and/or task."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('dl_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b70282eba0616fa5e4ed64a45ae049bb260824bbe5dfdc033db1152a6f241582"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}