{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8WeEpdX4Vx2"
      },
      "source": [
        "# Introduction to Graph Neural Nets with JAX/jraph\n",
        "\n",
        "\n",
        "by *Pietro Astolfi (pietroastolfi92@gmail.com)* and *Marco Ciccone, Politecnico di Torino (marco.ciccone@polito.it)*; \n",
        "\n",
        "based on the original notebook from *Lisa Wang, DeepMind (wanglisa@deepmind.com), Nikola Jovanović, ETH Zurich (nikola.jovanovic@inf.ethz.ch)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUxD4gOd-AuO"
      },
      "source": [
        "## Setup: Install and Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyH_VgjW7Bp3"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/deepmind/jraph.git\n",
        "!pip install flax\n",
        "!pip install dm-haiku\n",
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJm7y6GH3WyB"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "%matplotlib inline\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as tree\n",
        "import jraph\n",
        "import flax\n",
        "import haiku as hk\n",
        "import optax\n",
        "import pickle\n",
        "import numpy as onp\n",
        "import networkx as nx\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwNtsA9cj9E7"
      },
      "outputs": [],
      "source": [
        "# Utils for visualization\n",
        "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
        "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
        "  nx_graph = nx.DiGraph()\n",
        "  if nodes is None:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n)\n",
        "  else:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n, node_feature=nodes[n])\n",
        "  if edges is None:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
        "  else:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(\n",
        "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
        "  return nx_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsKA-syx_LUi"
      },
      "source": [
        "## Fundamental Graph Concepts\n",
        "A graph consists of a set of nodes and a set of edges, where edges form connections between nodes.\n",
        "\n",
        "More formally, a graph is defined as $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ where $\\mathcal{V}$ is the set of vertices / nodes, and $\\mathcal{E}$ is the set of edges.\n",
        "\n",
        "In an **undirected** graph, each edge is an unordered pair of two nodes $ \\in \\mathcal{V}$. E.g. a friend network can be represented as an undirected graph, assuming that the relationship \"*A is friends with B*\" implies \"*B is friends with A*\".\n",
        "\n",
        "In a **directed** graph, each edge is an ordered pair of nodes $ \\in \\mathcal{V}$. E.g. a citation network would be best represented with a directed graph, since the relationship \"*A cites B*\" does not imply \"*B cites A*\".\n",
        "\n",
        "The **degree** of a node is defined as the number of edges incident on it, i.e. the sum of incoming and outgoing edges for that node.\n",
        "\n",
        "The **in-degree** is the sum of incoming edges only, and the **out-degree** is the sum of outgoing edges only.\n",
        "\n",
        "There are several ways to represent $\\mathcal{E}$:\n",
        "1. As a **list of edges**: a list of pairs $(u,v)$, where $(u,v)$ means that there is an edge going from node $u$ to node $v$. <mark>NOTE: in Jraph we are going to use a similar structure!</mark>\n",
        "2. As an **adjacency matrix**: a binary square matrix $A$ of size $|\\mathcal{V}| \\times |\\mathcal{V}|$, where $A_{u,v}=1$ iff there is a connection between nodes $u$ and $v$.\n",
        "3. As an **adjacency list**: An array of $|\\mathcal{V}|$ unordered lists, where the $i$th list corresponds to the $i$th node, and contains all the nodes directly connected to node $i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U6zwCe8b9i2"
      },
      "source": [
        "Example: Below is a directed graph with four nodes and five edges.\n",
        "\n",
        "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/toy_graph.png\" width=\"400px\"></center>\n",
        "\n",
        "The arrows on the edges indicate the direction of each edge, e.g. there is an edge going from node 0 to node 1. Between node 0 and node 3, there are two edges: one going from node 0 to node 3 and one from node 3 to node 0.\n",
        "\n",
        "Node 0 has out-degree of 2, since it has two outgoing edges, and an in-degree of 2, since it has two incoming edges.\n",
        "\n",
        "The list of edges is:\n",
        "$$[(0, 1), (0, 3), (1, 2), (2, 0), (3, 0)]$$\n",
        "\n",
        "As adjacency matrix:\n",
        "\n",
        "$$\\begin{array}{l|llll}\n",
        " source \\setminus dest    & n_0 & n_1 & n_2 & n_3 \\\\ \\hline\n",
        "n_0 & 0    & 1    & 0    & 1    \\\\\n",
        "n_1 & 0    & 0    & 1    & 0    \\\\\n",
        "n_2 & 1    & 0    & 0    & 0    \\\\\n",
        "n_3 & 1    & 0    & 0    & 0\n",
        "\\end{array}$$\n",
        "\n",
        "As adjacency list:\n",
        "\n",
        "$$[\\{1, 3\\}, \\{2\\}, \\{0\\}, \\{0\\}]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spQGRxhPN8Eo"
      },
      "source": [
        "## Graph Prediction Tasks\n",
        "What are the kinds of problems we want to solve on graphs?\n",
        "\n",
        "\n",
        "The tasks fall into roughly three categories:\n",
        "\n",
        "1. **Node Classification**: E.g. what is the topic of a paper given a citation network of papers?\n",
        "2. **Link Prediction / Edge Classification**: E.g. are two people in a social network friends?\n",
        "3. **Graph Classification**: E.g. is this protein molecule (represented as a graph) likely going to be effective?\n",
        "\n",
        "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_tasks.png\" width=\"700px\"></center>\n",
        "\n",
        "*The three main graph learning tasks. Image source: Petar Veličković.*\n",
        "\n",
        "Which examples of graph prediction tasks come to your mind? Which task types do they correspond to?\n",
        "\n",
        "We will create and train models on all three task types in this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro to the jraph Library\n",
        "\n",
        "In the following sections, we will learn how to represent graphs and build GNNs in Python. We will use\n",
        "[jraph](https://github.com/deepmind/jraph), a lightweight library for working with GNNs in [JAX](https://github.com/google/jax)."
      ],
      "metadata": {
        "id": "TYpZfudwt970"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C5YI9M0vwvb"
      },
      "source": [
        "\n",
        "### Representing a graph in jraph\n",
        "\n",
        "In jraph, a graph is represented with a `GraphsTuple` object. In addition to defining the graph structure of nodes and edges, you can also store node features, edge features and global graph features in a `GraphsTuple`.\n",
        "\n",
        "In the `GraphsTuple`, edges are represented in two aligned arrays of node indices: senders (source nodes) and receivers (destinaton nodes).\n",
        "Each index corresponds to one edge, e.g. edge `i` goes from `senders[i]` to `receivers[i]`. This corresponds to concatenating all the $(u, v)$ tuples, and then unzipping them into separete lists (`senders` are all the $u$, `receivers` are all the $v$). Each list will be stored in a JAX array of shape $1 \\times |\\mathcal{E}|$. Note that this way of representing the graph takes advantage of graph **sparsity** (the space required for `senders` and `receivers` is usually significantly lower than the space needed for the full adjacency matrix $|\\mathcal{V}| \\times |\\mathcal{V}|$).\n",
        "\n",
        "You can even store multiple graphs in one `GraphsTuple` object.\n",
        "\n",
        "We will start with creating a simple directed graph with 4 nodes and 5 edges. We will also add toy features to the nodes, using `2*node_index` as the feature.\n",
        "\n",
        "We will later use this toy graph in the GCN demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK0rGWf56-Uq"
      },
      "outputs": [],
      "source": [
        "def build_toy_graph() -> jraph.GraphsTuple:\n",
        "  \"\"\"Define a four node graph, each node has a scalar as its feature.\"\"\"\n",
        "\n",
        "  # Nodes are defined implicitly by their features.\n",
        "  # We will add four nodes, each with a feature, e.g.\n",
        "  # node 0 has feature [0.],\n",
        "  # node 1 has featre [2.] etc.\n",
        "  # len(node_features) is the number of nodes.\n",
        "  node_features = jnp.array([[0.], [2.], [4.], [6.]])\n",
        "\n",
        "  # We will now specify 5 directed edges connecting the nodes we defined above.\n",
        "  # We define this with `senders` (source node indices) and `receivers`\n",
        "  # (destination node indices).\n",
        "  # For example, to add an edge from node 0 to node 1, we append 0 to senders,\n",
        "  # and 1 to receivers.\n",
        "  # We can do the same for all 5 edges:\n",
        "  # 0 -> 1\n",
        "  # 1 -> 2\n",
        "  # 2 -> 0\n",
        "  # 3 -> 0\n",
        "  # 0 -> 3\n",
        "  senders = jnp.array([0, 1, 2, 3, 0])\n",
        "  receivers = jnp.array([1, 2, 0, 0, 3])\n",
        "\n",
        "  # You can optionally add edge attributes to the 5 edges.\n",
        "  edges = jnp.array([[5.], [6.], [7.], [8.], [8.]])\n",
        "\n",
        "  # We then save the number of nodes and the number of edges.\n",
        "  # This information is used to make running GNNs over multiple graphs\n",
        "  # in a GraphsTuple possible.\n",
        "  n_node = jnp.array([4])\n",
        "  n_edge = jnp.array([5])\n",
        "\n",
        "  # Optionally you can add `global` information, such as a graph label.\n",
        "  global_context = jnp.array([[1]]) # Same feature dims as nodes and edges.\n",
        "  graph = jraph.GraphsTuple(\n",
        "      nodes=node_features,\n",
        "      edges=edges,\n",
        "      senders=senders,\n",
        "      receivers=receivers,\n",
        "      n_node=n_node,\n",
        "      n_edge=n_edge,\n",
        "      globals=global_context\n",
        "      )\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Jtg_y-TqCW"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Pwh9e7d8gN"
      },
      "source": [
        "### Visualizing the Graph\n",
        "To visualize the graph structure of the graph we created above, we will use the [`networkx`](networkx.org) library because it already has functions for drawing graphs.\n",
        "\n",
        "We first convert the `jraph.GraphsTuple` to a `networkx.DiGraph`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7q5ySSmVL3x"
      },
      "outputs": [],
      "source": [
        "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
        "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(\n",
        "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNK5SeajWQWO"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helpers functions: `segment_sum`, `segment_softmax`\n",
        "Before starting with the GCN implementation, let's take some time to familiarize with JAX functions that we need for Jraph sparse graph representation. Specifically, we inspect `jax.ops.segment_<fn>` functions, e.g., `segment_sum` and `segment_softmax`.\n",
        "These functions allows the application of `<fn>` to chunks (segments) of data present in a single array. They have two arguments, the array on which we want to apply `<fn>`, and a vector of the same size defining the segments; each segment is identified by an integer value. \n",
        "\n",
        "See the following example:"
      ],
      "metadata": {
        "id": "SyjlbTWbQqv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- DOCUMENTATION\n",
        "# Check how `jax.ops.segment_sum` can simplify your life with jraph sparse structures!\n",
        "# https://jax.readthedocs.io/en/latest/_autosummary/jax.ops.segment_sum.html\n",
        "# See also TF documentation for a visual illutration: \n",
        "# https://www.tensorflow.org/api_docs/python/tf/math/segment_sum\n",
        "jax.ops.segment_sum?"
      ],
      "metadata": {
        "id": "Tkb7VlZix-JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- EXAMPLE: segment_sum\n",
        "\n",
        "# let's define a simple vector of 10 values\n",
        "dummy_v = jnp.arange(10)\n",
        "\n",
        "# now let's create three segments for the array dummy_v\n",
        "dummy_seg = jnp.array([0,0,0,0,1,1,1,2,2,2])\n",
        "\n",
        "# now we apply the summation over segments\n",
        "print(jax.ops.segment_sum(dummy_v, dummy_seg, num_segments=3))\n",
        "\n",
        "# note that the segments vector does not have to be sorted nor\n",
        "# the segment positions need to be consecutives\n",
        "dummy_seg = jnp.array([0,1,1,2,1,1,1,2,2,0])\n",
        "\n",
        "# now we apply the summation over segments\n",
        "print(jax.ops.segment_sum(dummy_v, dummy_seg, num_segments=3))"
      ],
      "metadata": {
        "id": "ghQ6UXpnS6CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZRMF2d-h2pd"
      },
      "source": [
        "## Graph Convolutional Network (GCN) Layer\n",
        "\n",
        "Now let us implement our first graph convolutional network (GCN)!\n",
        "\n",
        "A GCN aims to emulate the learning process that a traditional convolutional neural network (CNN) does on images. However, differently from images, the structure of graphs is not constrained to a regular grid, and this prevents the use of traditional convolutional filters. \n",
        "\n",
        "The regularity of grids reflects two properties that are crucial to applying traditional convolutional filters: (i) pixels are ordered, (ii) pixels have fixed-size neighborhoods (each pixel is surrounded by 8 pixels). Because of these properties, the content of images is *invariant* or *equivariant* to translation (shift operation), allowing the shifting of a local convolutional filter (e.g., 3x3) across the different parts of a larger image (e.g., 64x64). Note that a filter performs a weighted sum aggregation over the inputs (pixels) with learnable weights, where each filter input has its own weight.\n",
        "\n",
        "In graphs, there is no ordering of nodes, and the edges determine the size of each node neighborhood. Hence, we cannot learn a weighted sum of the neighbors as such weights depend on the neighbors' order that is not present. Instead, we need to apply a learnable filter invariant to the neighbors' order. The simplest solution is to apply the same weight to all neighbors without learning the aggregation function. In other words, each neighbor contributes equally. This type of convolutional filter identifies GCNs that are *isotropic*.\n",
        "\n",
        "\n",
        "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/cnn_vs_gnn.png\" width=\"400px\"></center>\n",
        "\n",
        "Comparison of CNN and GCN filters.\n",
        "Image source: https://arxiv.org/pdf/1901.00596.pdf\n",
        "\n",
        "## Message-Passing\n",
        "GCNs can be defined using different paradigms. The most widely adopted and easy to understand is *Message-Passing (MP)*. The basic idea of MP is to perform local convolution through two steps:\n",
        "\n",
        "1. _Compute messages / update node features_: Create a feature vector $\\vec{h}_n$ for each node $n$ (e.g. with an MLP). This is going to be the message that this node will pass to neighboring nodes.\n",
        "2. _Message-passing / aggregate node features_: For each node, calculate a new feature vector $\\vec{h}'_n$ based on the messages (features) from the nodes in its neighborhood. In a directed graph, only nodes from incoming edges are counted as neighbors. The image below shows this aggregation step. There are multiple options for aggregation; in *isotropic* GCNs, common aggregations are the mean, the sum, the min, or the max, while in *anisotropic* GCNs, the aggregation depends on variable edge weights (either learned or fixed).\n",
        "\n",
        "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_conv.png\" width=\"500px\"></center>\n",
        "\n",
        "*\\\"A generic overview of a graph convolution operation, highlighting the relevant information for deriving the next-level features for every node in the graph.\\\"* Image source: Petar Veličković (https://github.com/PetarV-/TikZ)\n",
        "\n",
        "In the following steps, we will implement the Graph Convolutional Network as introduced by Kipf et al. (2017) in https://arxiv.org/abs/1609.02907, which is one of the basic graph network architectures. We start by building its core building block, the graph convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc6fCm0zzOv-"
      },
      "source": [
        "### Simple GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc7u-eHvg5Zp"
      },
      "outputs": [],
      "source": [
        "def apply_simplified_gcn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  # Unpack GraphsTuple\n",
        "  nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "  # 1. Update node features (compute messages)\n",
        "  # For simplicity, we will first use an identify function here, \n",
        "  # and replace it with a trainable MLP block later.\n",
        "  update_node_fn = lambda nodes: nodes\n",
        "  nodes = update_node_fn(nodes)\n",
        "\n",
        "  # 2. Aggregate node features over nodes in neighborhood (Message-Passing)\n",
        "  # Equivalent to jnp.sum(n_node), but jittable\n",
        "  total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "  aggregate_nodes_fn = jax.ops.segment_sum\n",
        "\n",
        "  # Compute new node features by aggregating messages from neighboring nodes\n",
        "  # -- take all nodes sending a message with x[senders], \n",
        "  # -- and aggregate by receiver with `aggregate_nodes_fn`\n",
        "  \n",
        "  # Check how `jax.ops.segment_sum` can simplify your life with jraph sparse structures!\n",
        "  # https://jax.readthedocs.io/en/latest/_autosummary/jax.ops.segment_sum.html\n",
        "  # See also TF documentation for a visual illutration: \n",
        "  # https://www.tensorflow.org/api_docs/python/tf/math/segment_sum\n",
        "\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  nodes = tree.tree_map( ... )\n",
        "  ################\n",
        "\n",
        "  # Create a new graph with same structure and the updated nodes representation\n",
        "  out_graph = graph._replace(nodes=nodes)\n",
        "  return out_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs66eJ1ZawD2"
      },
      "source": [
        "We can now run the graph convolution on our toy graph from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhG3r8P-aiBb"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHJ_CYthY8P"
      },
      "source": [
        "Here is the visualized graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsaiSIZYbPzw"
      },
      "outputs": [],
      "source": [
        "draw_jraph_graph_structure(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zAwAgy2bB1P"
      },
      "outputs": [],
      "source": [
        "out_graph = apply_simplified_gcn(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ryPhwtcGTx"
      },
      "source": [
        "Since we used the identity function for updating nodes and sum aggregation, we can verify the results pretty easily. As a reminder, in this toy graph, the node features are the same as the node index.\n",
        "\n",
        "Node 0: sum of features from node 2 and node 3 $\\rightarrow$ 10.\n",
        "\n",
        "Node 1: sum of features from node 0 $\\rightarrow$ 0.\n",
        "\n",
        "Node 2: sum of features from node 1 $\\rightarrow$ 2.\n",
        "\n",
        "Node 3: sum of features from node 0 $\\rightarrow$ 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwEtzddrbHri"
      },
      "outputs": [],
      "source": [
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rt3AB2tvv1o"
      },
      "source": [
        "### Add Trainable Parameters to GCN layer\n",
        "So far our graph convolution operation doesn't have any learnable parameters.\n",
        "Let's add an MLP block to the update function to make it trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK9W9J4GsDbC"
      },
      "outputs": [],
      "source": [
        "class MLP(hk.Module):\n",
        "  def __init__(self, features: jnp.ndarray):\n",
        "    super().__init__()\n",
        "    self.features = features\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "    layers = []\n",
        "    for feat in self.features[:-1]:\n",
        "      layers.append(hk.Linear(feat))\n",
        "      layers.append(jax.nn.relu)\n",
        "    layers.append(hk.Linear(self.features[-1]))\n",
        "\n",
        "    mlp = hk.Sequential(layers)\n",
        "    return mlp(x)\n",
        "\n",
        "################\n",
        "# YOUR CODE HERE\n",
        "# instantiate an MLP with dimensions [8,4] and create a function from it\n",
        "update_node_fn = ...\n",
        "################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQCaukgyOue"
      },
      "source": [
        "#### Check outputs of `update_node_fn` with MLP Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ImWUoi9s2-x"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BCg7cYwyBRw"
      },
      "outputs": [],
      "source": [
        "################\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# 1. create haiku module transform, without applying rng,\n",
        "# 2. initialize the model parameters.\n",
        "# 3. Apply the update_node_module on graph.nodes\n",
        "\n",
        "update_node_module = ...\n",
        "params = ...\n",
        "out = ...\n",
        "\n",
        "################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooUu13gBzm2S"
      },
      "source": [
        "As output, we expect the updated node features. We should see one array of dim 4 for each of the 4 nodes, which is the result of applying a single MLP block to the features of each node individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PblLCX67ubZ-"
      },
      "outputs": [],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD1Qc7xK-SkI"
      },
      "source": [
        "#### Add Self-Edges (Edges connecting a node to itself)\n",
        "For each node, add an edge of the node onto itself. This way, nodes will include themselves in the aggregation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8LO_UMN-ReR"
      },
      "outputs": [],
      "source": [
        "def add_self_edges_fn(receivers: jnp.ndarray, senders: jnp.ndarray,\n",
        "                      total_num_nodes: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Adds self edges. Assumes self edges are not in the graph yet.\"\"\"\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # for each node, add a connection to itself, both from the sender and receiver perspective\n",
        "  # you can easily implement it in a pythonic and vectorized way, by knowing total_num_nodes.\n",
        "  receivers = ...\n",
        "  senders = ...\n",
        "  ################\n",
        "  return receivers, senders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RCGC0XbkjCV"
      },
      "source": [
        "#### Add Symmetric Normalization\n",
        "\n",
        "Note that the nodes may have different numbers of neighbors/degrees, possibly leading to instabilities during neural network training, e.g., exploding or vanishing gradients. To address that, normalization is a commonly used method. In this case, we will normalize by node degrees.\n",
        "\n",
        "As a first attempt, we could count the number of incoming edges (including self-edge) and divide by that value.\n",
        "\n",
        "More formally, let $A$ be the adjacency matrix defining the edges of the graph.\n",
        "\n",
        "Then we define the degree matrix $D$ as a diagonal matrix with $D_{ii} = \\sum_jA_{ij}$ (the degree of node $i$)\n",
        "\n",
        "\n",
        "Now we can normalize $AH$ by dividing it by the node degrees:\n",
        "$${D}^{-1}AH$$\n",
        "Note that, with this normalization, we are computing the mean over the neighbors' messages.\n",
        "\n",
        "Instead, if we want to compute a more dynamic aggregation than the mere mean, we can take into \n",
        "account both the in and out degrees by computing a *symmetric normalization*, which is also what Kipf and Welling proposed in their [paper](https://arxiv.org/abs/1609.02907):\n",
        "$$D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}H$$\n",
        "\n",
        "**Implementation tip**: \n",
        "Keep in mind that we are using sparse matrix operations; thus, we never explicit the adjacency matrix $A$ in its dense form. \n",
        "We will directly pre-normalize the feature of each node present in each edge by the square root of the in-degrees, and later we will post-normalize by the square root of the out-degrees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdxzqrpbj-jZ"
      },
      "source": [
        "### General GCN Layer\n",
        "Now we can write a more general and configurable version of the Graph Convolution layer, allowing the caller to specify:\n",
        "\n",
        "*   **`update_node_fn`**: Function to use to update node features (e.g. the MLP block version we just implemented)\n",
        "*   **`aggregate_nodes_fn`**: Aggregation function to use to aggregate messages from neighbourhood.\n",
        "*  **`add_self_edges`**: Whether to add self edges for aggregation step.\n",
        "* **`symmetric_normalization`**: Whether to add symmetric normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkkOolOVh3nR"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
        "def GraphConvolution(update_node_fn: Callable,\n",
        "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
        "                     add_self_edges: bool = False,\n",
        "                     symmetric_normalization: bool = True) -> Callable:\n",
        "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
        "\n",
        "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
        "  NOTE: This implementation does not add an activation after aggregation.\n",
        "  If you are stacking layers, you may want to add an activation between\n",
        "  each layer.\n",
        "  Args:\n",
        "    update_node_fn: function used to update the nodes. In the paper a single\n",
        "      layer MLP is used.\n",
        "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
        "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
        "      paper definition of GCN. Defaults to False.\n",
        "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
        "      True.\n",
        "\n",
        "  Returns:\n",
        "    A method that applies a Graph Convolution layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
        "    nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "    # First, pass the nodes through the node updater.\n",
        "\n",
        "    ################\n",
        "    # YOUR CODE HERE\n",
        "    nodes = ...\n",
        "    ################\n",
        "\n",
        "    # Equivalent to jnp.sum(n_node), but jittable\n",
        "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "    # -- \n",
        "    \n",
        "    if add_self_edges:\n",
        "      # We add self edges to the senders and receivers so that each node\n",
        "      # includes itself in aggregation.\n",
        "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
        "      # this case it is not required since a GCN is agnostic to whether\n",
        "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
        "      \n",
        "      ################\n",
        "      # YOUR CODE HERE\n",
        "      conv_receivers, conv_senders = ...\n",
        "      ################\n",
        "    else:\n",
        "      ################\n",
        "      # YOUR CODE HERE\n",
        "      conv_senders = ...\n",
        "      conv_receivers = ...\n",
        "      ################\n",
        "\n",
        "    # pylint: disable=g-long-lambda\n",
        "    if symmetric_normalization:\n",
        "      # Calculate the normalization values.\n",
        "      \n",
        "      # -- Compute in-degree (sender_degree) and out-degree (receiver_degree) of each node\n",
        "      # -- Remember: The in-degree is the sum of incoming edges only, \n",
        "      # and the out-degree is the sum of outgoing edges only.\n",
        "\n",
        "      # Tip: all the operations should be jittable!\n",
        "      # -- take again advantage of `jax.ops.segment_sum`\n",
        "\n",
        "      ################\n",
        "      # YOUR CODE HERE\n",
        "      count_edges_fn = ...\n",
        "  \n",
        "      sender_degree = count_edges_fn(conv_senders)\n",
        "      receiver_degree = count_edges_fn(conv_receivers)\n",
        "\n",
        "\n",
        "      # 1. Pre normalize by sqrt sender degree: \n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "      # check https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.rsqrt.html\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "      nodes = tree.tree_map(...)\n",
        "\n",
        "      # 2. Aggregate the pre-normalized nodes.\n",
        "      \n",
        "      # YOUR CODE HERE\n",
        "      nodes = tree.tree_map(...)\n",
        "    \n",
        "      # 3. Post normalize by sqrt receiver degree.\n",
        "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "      nodes = tree.tree_map(...)\n",
        "      ################\n",
        "\n",
        "    else:\n",
        "       \n",
        "      # If no normalization is applied, just aggregate\n",
        "      ################\n",
        "      # YOUR CODE HERE\n",
        "      nodes = tree.tree_map(...)\n",
        "      ################\n",
        "\n",
        "    # pylint: enable=g-long-lambda\n",
        "    return graph._replace(nodes=nodes)\n",
        "\n",
        "  return _ApplyGCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKxcA6XAza33"
      },
      "source": [
        "#### Test General GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TgyCkE9KlUG"
      },
      "outputs": [],
      "source": [
        "gcn_layer = GraphConvolution(\n",
        "    update_node_fn=lambda n: MLP(features=[8, 4])(n),\n",
        "    aggregate_nodes_fn=jax.ops.segment_sum,\n",
        "    add_self_edges=True,\n",
        "    symmetric_normalization=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gTPEWvU0DDB"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()\n",
        "network = hk.without_apply_rng(hk.transform(gcn_layer))\n",
        "params = network.init(jax.random.PRNGKey(42), graph)\n",
        "out_graph = network.apply(params, graph)\n",
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lha8rbQ78l3S"
      },
      "source": [
        "### Build GCN Model with Multiple Layers\n",
        "With a single GCN layer, a node's representation after the GCN layer is only\n",
        "influenced by its direct neighbourhood. However, we may want to consider larger neighbourhoods, i.e. more than just 1 hop away. To achieve that, we can stack\n",
        "multiple GCN layers, similar to how stacking CNN layers expands the input region.\n",
        "\n",
        "We will define a network with two GCN layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYOQBh848k2U"
      },
      "outputs": [],
      "source": [
        "def gcn_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Defines a graph neural network with 3 GCN layers.\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # 1. Build the first GCN layer having a single non-linear projection \n",
        "  # with dimensionality 8 as update_node_fn.\n",
        "  \n",
        "\n",
        "  gn = ...\n",
        "  graph = ...\n",
        "\n",
        "  # 2. Build the classification head of the model as a GCN layer that project nodes\n",
        "  # into 2 classes, without applying any non-linearity.\n",
        "  \n",
        "  gn = ...\n",
        "  graph = ...\n",
        "\n",
        "  ################\n",
        "  return graph\n",
        "\n",
        "  # --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6CisEhz_A-a"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()\n",
        "network = hk.without_apply_rng(hk.transform(gcn_fn))\n",
        "params = network.init(jax.random.PRNGKey(42), graph)\n",
        "out_graph = network.apply(params, graph)\n",
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5t7kw7SE_h4"
      },
      "source": [
        "### Node Classification with GCN on Karate Club Dataset\n",
        "\n",
        "Time to try out our GCN on our first graph prediction task! We can use the one just implemented as in this task with have a binary classification of the nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ_w2kkWoAq4"
      },
      "source": [
        "#### Zachary's Karate Club Dataset\n",
        "\n",
        "[Zachary's karate club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) is a small dataset commonly used as an example for a social graph. It's great for demo purposes, as it's easy to visualize and quick to train a model on it.\n",
        "\n",
        "A node represents a student or instructor in the club. An edge means that those two people have interacted outside of the class. There are two instructors in the club.\n",
        "\n",
        "Each student is assigned to one of two instructors.\n",
        "\n",
        "#### Optimizing the GCN on the Karate Club Node Classification Task\n",
        "\n",
        "The task is to predict the assignment of students to instructors, given the social graph and only knowing the assignment of two nodes (the two instructors) a priori.\n",
        "\n",
        "In other words, out of the 34 nodes, only two nodes are labeled, and we are trying to optimize the assignment of the other 32 nodes, by **maximizing the log-likelihood of the two known node assignments**.\n",
        "\n",
        "We will compute the accuracy of our node assignments by comparing to the ground-truth assignments. **Note that the ground-truth for the 32 student nodes is not used in the loss function itself.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SE5DQoXWQJR"
      },
      "source": [
        "Let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YGQXeGN_E3J"
      },
      "outputs": [],
      "source": [
        "\"\"\"Zachary's karate club example.\n",
        "From https://github.com/deepmind/jraph/blob/master/jraph/examples/zacharys_karate_club.py.\n",
        "Here we train a graph neural network to process Zachary's karate club.\n",
        "https://en.wikipedia.org/wiki/Zachary%27s_karate_club\n",
        "Zachary's karate club is used in the literature as an example of a social graph.\n",
        "Here we use a graphnet to optimize the assignments of the students in the\n",
        "karate club to two distinct karate instructors (Mr. Hi and John A).\n",
        "\"\"\"\n",
        "\n",
        "def get_zacharys_karate_club() -> jraph.GraphsTuple:\n",
        "  \"\"\"Returns GraphsTuple representing Zachary's karate club.\"\"\"\n",
        "  social_graph = [\n",
        "      (1, 0), (2, 0), (2, 1), (3, 0), (3, 1), (3, 2),\n",
        "      (4, 0), (5, 0), (6, 0), (6, 4), (6, 5), (7, 0), (7, 1),\n",
        "      (7, 2), (7, 3), (8, 0), (8, 2), (9, 2), (10, 0), (10, 4),\n",
        "      (10, 5), (11, 0), (12, 0), (12, 3), (13, 0), (13, 1), (13, 2),\n",
        "      (13, 3), (16, 5), (16, 6), (17, 0), (17, 1), (19, 0), (19, 1),\n",
        "      (21, 0), (21, 1), (25, 23), (25, 24), (27, 2), (27, 23),\n",
        "      (27, 24), (28, 2), (29, 23), (29, 26), (30, 1), (30, 8),\n",
        "      (31, 0), (31, 24), (31, 25), (31, 28), (32, 2), (32, 8),\n",
        "      (32, 14), (32, 15), (32, 18), (32, 20), (32, 22), (32, 23),\n",
        "      (32, 29), (32, 30), (32, 31), (33, 8), (33, 9), (33, 13),\n",
        "      (33, 14), (33, 15), (33, 18), (33, 19), (33, 20), (33, 22),\n",
        "      (33, 23), (33, 26), (33, 27), (33, 28), (33, 29), (33, 30),\n",
        "      (33, 31), (33, 32)]\n",
        "  # Add reverse edges.\n",
        "  social_graph += [(edge[1], edge[0]) for edge in social_graph]\n",
        "  n_club_members = 34\n",
        "\n",
        "  return jraph.GraphsTuple(\n",
        "      n_node=jnp.asarray([n_club_members]),\n",
        "      n_edge=jnp.asarray([len(social_graph)]),\n",
        "      # One-hot encoding for nodes, i.e. argmax(nodes) = node index.\n",
        "      nodes=jnp.eye(n_club_members),\n",
        "      # No edge features.\n",
        "      edges=None,\n",
        "      globals=None,\n",
        "      senders=jnp.asarray([edge[0] for edge in social_graph]),\n",
        "      receivers=jnp.asarray([edge[1] for edge in social_graph]))\n",
        "\n",
        "def get_ground_truth_assignments_for_zacharys_karate_club() -> jnp.ndarray:\n",
        "  \"\"\"Returns ground truth assignments for Zachary's karate club.\"\"\"\n",
        "  return jnp.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
        "                    0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyJk-Mq7EKoU"
      },
      "outputs": [],
      "source": [
        "graph = get_zacharys_karate_club()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4v5LnKgEN8l"
      },
      "outputs": [],
      "source": [
        "print(f'Number of nodes: {graph.n_node[0]}')\n",
        "print(f'Number of edges: {graph.n_edge[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi2DQYzjngR8"
      },
      "source": [
        "Visualize the karate club graph with circular node layout:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MVtGGJwg_B8"
      },
      "outputs": [],
      "source": [
        "nx_graph = convert_jraph_to_networkx_graph(graph)\n",
        "pos = nx.circular_layout(nx_graph)\n",
        "plt.figure(figsize=(6, 6))\n",
        "nx.draw(nx_graph, pos=pos, with_labels = True, node_size=500, font_color='yellow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCZI26T9Q2vy"
      },
      "source": [
        "Training and evaluation code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n1kCuqtkvfm"
      },
      "outputs": [],
      "source": [
        "def optimize_club(network: hk.Transformed, num_steps: int) -> jnp.ndarray:\n",
        "  \"\"\"Solves the karate club problem by optimizing the assignments of students.\"\"\"\n",
        "  zacharys_karate_club = get_zacharys_karate_club()\n",
        "  labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
        "  params = network.init(jax.random.PRNGKey(42), zacharys_karate_club)\n",
        "\n",
        "  @jax.jit\n",
        "  def predict(params: hk.Params) -> jnp.ndarray:\n",
        "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
        "    return jnp.argmax(decoded_graph.nodes, axis=1)\n",
        "\n",
        "  @jax.jit\n",
        "  def prediction_loss(params: hk.Params) -> jnp.ndarray:\n",
        "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
        "    # We interpret the decoded nodes as a pair of logits for each node.\n",
        "    log_prob = jax.nn.log_softmax(decoded_graph.nodes)\n",
        "    # The only two assignments we know a-priori are those of Mr. Hi (Node 0)\n",
        "    # and John A (Node 33).\n",
        "    return -(log_prob[0, 0] + log_prob[33, 1])\n",
        "\n",
        "  opt_init, opt_update = optax.adam(1e-2)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  @jax.jit\n",
        "  def update(params: hk.Params, opt_state) -> Tuple[hk.Params, Any]:\n",
        "    \"\"\"Returns updated params and state.\"\"\"\n",
        "    g = jax.grad(prediction_loss)(params)\n",
        "    updates, opt_state = opt_update(g, opt_state)\n",
        "    return optax.apply_updates(params, updates), opt_state\n",
        "\n",
        "  @jax.jit\n",
        "  def accuracy(params: hk.Params) -> jnp.ndarray:\n",
        "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
        "    return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
        "\n",
        "  for step in range(num_steps):\n",
        "    print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
        "    params, opt_state = update(params, opt_state)\n",
        "\n",
        "  return predict(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkyao6e5RBug"
      },
      "source": [
        "Let's train the GCN! We expect this model reach an accuracy of about 0.91."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcp00eY46vTE"
      },
      "outputs": [],
      "source": [
        "network = hk.without_apply_rng(hk.transform(gcn_fn))\n",
        "result = optimize_club(network, num_steps=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rUwloZhTFW7"
      },
      "source": [
        "\n",
        "Try modifying the model parameters to see if you can improve the accuracy! For instance try to add additional layers to the GCN; you should notice that having too many layers harms the performances! Can you guess why?\n",
        "I fyou don't know, take a look to the following paper: https://arxiv.org/abs/1905.10947"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3rPI44_VbaY"
      },
      "source": [
        "Node assignments predicted by the model at the end of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBKh7f-SPBnX"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0gNu67xP-_K"
      },
      "source": [
        "Visualize ground truth and predicted node assignments:\n",
        "\n",
        "What do you think of the results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndklBMr6PHLY"
      },
      "outputs": [],
      "source": [
        "zacharys_karate_club = get_zacharys_karate_club()\n",
        "nx_graph = convert_jraph_to_networkx_graph(zacharys_karate_club)\n",
        "pos = nx.circular_layout(nx_graph)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 7))\n",
        "ax1 = fig.add_subplot(121)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=result.tolist(),\n",
        "    font_color='white')\n",
        "ax1.title.set_text('Predicted Node Assignments with GCN')\n",
        "\n",
        "gt_labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
        "ax2 = fig.add_subplot(122)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=gt_labels.tolist(),\n",
        "    font_color='white')\n",
        "ax2.title.set_text('Ground-Truth Node Assignments')\n",
        "fig.suptitle('Do you spot the difference? 😐', y=-0.01)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg8g96NdBCK6"
      },
      "source": [
        "## Graph Attention (GAT) Layer\n",
        "\n",
        "While the GCN we covered in the previous section can learn meaningful representations, it also has some shortcomings. Can you think of any?\n",
        "\n",
        "In the GCN layer, the messages from all its neighbors and the node itself are equally weighted -- well, this is not exactly true because of the symmetric normalization. However, the main limitation is that the aggregation weights are **hard-crafted** (division by the in- and out-degree). This may lead to loss of node-specific information. E.g., consider the case when a set of nodes shares the same set of neighbors, and start out with different node features. Then because of averaging, their resulting output features would be the same. Adding self-edges mitigates this issue by a small amount, but this problem is magnified with increasing number of GCN layers and number of edges connecting to a node.\n",
        "\n",
        "In more formal words, the implemented GCN uses *isotropic* learnable filters, while we want\n",
        "*anisotropic* filters as they can catch more complex pattern -- as it happens for traditional convolutional filters on images.\n",
        "\n",
        "The graph attention (GAT) mechanism, as proposed by [Velickovic et al. ( 2017)](https://arxiv.org/abs/1710.10903), allows the network to learn how to weight / assign importance to the node features from the neighborhood when computing the new node features. This is very similar to the idea of using attention in Transformers, which were introduced in [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762). Indeed, \n",
        "transformers has been shown to be a special case of graph attention networks, where a fully-connected graph structure is assumed (see articles from [Joshi (2020)](https://graphdeeplearning.github.io/files/transformers-are-gnns-slides.pdf) and [Dwivedi et al. (2020)](https://arxiv.org/abs/2012.09699)).\n",
        "\n",
        "In the figure below, $\\vec{h}$ are the node features and $\\vec{\\alpha}$ are the learned attention weights.\n",
        "\n",
        "\n",
        "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/gat1.png\" width=\"400px\"></center>\n",
        "\n",
        "Figure Credit: [Velickovic et al. ( 2017)](https://arxiv.org/abs/1710.10903).\n",
        "(Detail: This image is showing multi-headed attention with 3 heads, each color corresponding to a different head. At the end, an aggregation function is applied over all the heads.)\n",
        "\n",
        "To obtain the output node features of a single head GAT layer, we compute:\n",
        "\n",
        "$$ \\vec{h}'_i = \\sum _{j \\in \\mathcal{N}(i)}\\alpha_{ij} \\mathbf{W} \\vec{h}_j$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qa6dOLBGLlP"
      },
      "source": [
        "\n",
        "Let's now decompose the formula above into single computation steps that we will implement in the GAT layer. \n",
        "\n",
        "**Step 0.**\n",
        "Initially, we use the weight matrix $\\mathbf{W}$ to performs a linear transformation on the input.\n",
        "\n",
        "**Step 1.**\n",
        "How do we obtain $\\alpha$, or in other words, learn what to pay attention to?\n",
        "\n",
        "**Step 1a.**\n",
        "Intuitively, the attention coefficient $\\alpha_{ij}$ should rely on both the transformed features from nodes $i$ and $j$. So let's first define an attention mechanism function $\\mathrm{attention\\_fn}$ that computes the intermediary attention coefficients $e_{ij}$:\n",
        "$$ e_{ij} = \\mathrm{attention\\_fn}(\\mathbf{W}\\vec{h}_i, \\mathbf{W}\\vec{h}_j)$$\n",
        "\n",
        "**Step 1b.**\n",
        "For the $\\mathrm{attention\\_fn}$ function, the authors of the GAT paper chose to concatenate the transformed node features (denoted by $||$) and apply a single-layer feedforward network, parameterized by a weight vector $\\vec{\\mathbf{a}}$ and with LeakyRelu as non-linearity.\n",
        "\n",
        "In the implementation below, we refer to $\\mathbf{W}$ as `attention_query_fn` and $\\mathrm{attention\\_fn}$ as `attention_logit_fn`.\n",
        "\n",
        "$$\\mathrm{attention\\_fn}(\\mathbf{W}\\vec{h}_i, \\mathbf{W}\\vec{h}_j) =  \\text{LeakyReLU}(\\vec{\\mathbf{a}}(\\mathbf{W}\\vec{h}_i || \\mathbf{W}\\vec{h}_j))$$\n",
        "\n",
        "**Step 2.**\n",
        "To obtain normalized attention weights $\\alpha$, we apply a softmax:\n",
        "$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum _{j \\in \\mathcal{N}(i)}\\exp(e_{ij})}$$\n",
        "\n",
        "The figure below summarizes this attention mechanism visually.\n",
        "\n",
        "<center><image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/gat2.png\" width=\"300px\"></center>\n",
        "\n",
        "Figure Credit: Petar Velickovic.\n",
        "<!-- $\\sum_{j \\in \\mathcal{N}(i)}\\vec{\\alpha}_{ij} \\stackrel{!}{=}\n",
        "1 $  -->\n",
        "\n",
        "**Step 3.**\n",
        "We compute the messages re-weighted by the obtained attention:\n",
        "$$ \\alpha_{ij} \\mathbf{W} \\vec{h}_j, j \\in \\mathcal{N}(i)$$\n",
        "\n",
        "**Step 4.**\n",
        "We pass the messages through aggregation:\n",
        "$$\\sum _{j \\in \\mathcal{N}(i)}\\alpha_{ij} \\mathbf{W} \\vec{h}_j$$\n",
        "Note that differently from GCN here we don't need additional normalization as the attention $\\alpha_{ij}$ is already pre-normalized by the softmax.\n",
        "\n",
        "**Step 5.**\n",
        "Each node is finally updated after aggregating the information from $M$ heads.\n",
        "\n",
        "**Step 5a.**\n",
        "$$ \\vec{h}'_i = \\mathrm{update\\_fn}(\\vec{h}'^1_i, \\vec{h}'^2_i, ..., \\vec{h}'^M_i)$$\n",
        "\n",
        "**Step 5b.**\n",
        "$$ \\mathrm{update\\_fn} = \\parallel_{m=1}^M(\\text{LeakyReLU}(\\vec{h}'^m_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJqT6mjgBBmQ"
      },
      "outputs": [],
      "source": [
        "def attention_logit_fn(sender_attr: jnp.ndarray, receiver_attr: jnp.ndarray,\n",
        "                       edges: jnp.ndarray) -> jnp.ndarray:\n",
        "  del edges\n",
        "  # 1b. Implement the function attention_fn. \n",
        "  # Remember that the output of this function must the \n",
        "  # unnormalized edge weight, i.e., \n",
        "  \n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  x = ...\n",
        "  x = ...\n",
        "  ################\n",
        "  return x\n",
        "\n",
        "# GAT implementation adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L442.\n",
        "def GAT(attention_query_fn: Callable,\n",
        "        attention_logit_fn: Callable,\n",
        "        node_update_fn: Callable,\n",
        "        add_self_edges: bool = True) -> Callable:\n",
        "  \"\"\"Returns a method that applies a Graph Attention Network layer.\n",
        "\n",
        "  Graph Attention message passing as described in\n",
        "  https://arxiv.org/pdf/1710.10903.pdf. This model expects node features as a\n",
        "  jnp.array, may use edge features for computing attention weights, and\n",
        "  ignore global features. It does not support nests.\n",
        "  Args:\n",
        "    attention_query_fn: function that generates attention queries from sender\n",
        "      node features.\n",
        "    attention_logit_fn: function that converts attention queries into logits for\n",
        "      softmax attention.\n",
        "    node_update_fn: function that updates the aggregated messages. If None, will\n",
        "      apply leaky relu and concatenate (if using multi-head attention).\n",
        "\n",
        "  Returns:\n",
        "    A function that applies a Graph Attention layer.\n",
        "  \"\"\"\n",
        "  # pylint: disable=g-long-lambda\n",
        "\n",
        "  def _ApplyGAT(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "    \"\"\"Applies a Graph Attention layer.\"\"\"\n",
        "    nodes, edges, receivers, senders, _, _, _ = graph\n",
        "\n",
        "    # Equivalent to the sum of n_node, but statically known.\n",
        "    try:\n",
        "      sum_n_node = nodes.shape[0]\n",
        "    except IndexError:\n",
        "      raise IndexError('GAT requires node features')\n",
        "\n",
        "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "    if add_self_edges:\n",
        "      # We add self edges to the senders and receivers so that each node\n",
        "      # includes itself in aggregation.\n",
        "      receivers, senders = add_self_edges_fn(receivers, senders,\n",
        "                                             total_num_nodes)\n",
        "\n",
        "    ################\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # 0. Pass nodes through the attention query function to transform\n",
        "    # node features, e.g. with an MLP.\n",
        "    nodes = ...\n",
        "\n",
        "    # 1a. We compute the softmax logits using a function that takes the\n",
        "    # embedded sender and receiver attributes.\n",
        "    \n",
        "    sent_attributes = nodes[senders]\n",
        "    received_attributes = nodes[receivers]\n",
        "    att_softmax_logits = attention_logit_fn(sent_attributes,\n",
        "                                            received_attributes, edges)\n",
        "\n",
        "    # 2. Compute the attention softmax weights on the entire tree. \n",
        "    # Hint: you can take advantage of segment_softmax\n",
        "    # https://jraph.readthedocs.io/en/latest/api.html#jraph.segment_softmax\n",
        "    \n",
        "    att_weights = ...\n",
        "\n",
        "    # 3. Apply attention weights.\n",
        "    messages = ...\n",
        "\n",
        "    # 4. Aggregate messages to nodes (use segment_sum).\n",
        "    nodes = ...\n",
        "\n",
        "    # 5a. Apply an update function to the aggregated messages.\n",
        "    nodes = ...\n",
        "\n",
        "    ################\n",
        "    return graph._replace(nodes=nodes)\n",
        "\n",
        "  # pylint: enable=g-long-lambda\n",
        "  return _ApplyGAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t9bNfSA9-HR"
      },
      "source": [
        "#### Test GAT Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjpYLFvv9-HT"
      },
      "outputs": [],
      "source": [
        "# 5b. Define the aggregation for multi-head: \n",
        "# apply the leaky relu and then concatenate \n",
        "# the heads on the feature axis.\n",
        "\n",
        "################\n",
        "# YOUR CODE HERE\n",
        "node_update_fn = ...\n",
        "################\n",
        "\n",
        "gat_layer = GAT(\n",
        "    attention_query_fn=lambda n: hk.Linear(8)(n),  # Applies W to the node features\n",
        "    attention_logit_fn=attention_logit_fn,\n",
        "    node_update_fn=node_update_fn,\n",
        "    add_self_edges=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV5PndNz9-HT"
      },
      "outputs": [],
      "source": [
        "graph = build_toy_graph()\n",
        "network = hk.without_apply_rng(hk.transform(gat_layer))\n",
        "params = network.init(jax.random.PRNGKey(42), graph)\n",
        "out_graph = network.apply(params, graph)\n",
        "out_graph.nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anfVGJwBe27v"
      },
      "source": [
        "### Train GAT Model on Karate Club Dataset\n",
        "We will now repeat the karate club experiment with a GAT network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUXJcRnVMr4X"
      },
      "outputs": [],
      "source": [
        "def gat_2_layers(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Defines a GAT network for the karate club node classification task.\n",
        "\n",
        "  Args:\n",
        "    graph: GraphsTuple the network processes.\n",
        "\n",
        "  Returns:\n",
        "    output graph with updated node values.\n",
        "  \"\"\"\n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # Implement a 2 layers GAT Network\n",
        "  # attention_query_fn must be linear projection with dimension 8 (in all layers).\n",
        "  # Reuse `_attention_logit_fn` and `_node_update_fn` from the previous block.\n",
        "  # node_update_fn of the final layer must be a linear projection to the 2 classes\n",
        "\n",
        "  def _attention_logit_fn(sender_attr: jnp.ndarray, receiver_attr: jnp.ndarray,\n",
        "                          edges: jnp.ndarray) -> jnp.ndarray:\n",
        "    del edges\n",
        "    ################\n",
        "    # YOUR CODE HERE\n",
        "    # -- !!SAME IMPLEMENTATION AS ABOVE!!\n",
        "    x = ...\n",
        "    x = ...\n",
        "    ################\n",
        "    return x\n",
        "  \n",
        "  ################\n",
        "  # YOUR CODE HERE\n",
        "  # -- SAME AS CGN ... but now use 2 GAT Layers!!\n",
        "  _node_update_fn = ...\n",
        "  gn = GAT( ... )\n",
        "  graph = gn(graph)\n",
        "\n",
        "  gn = GAT( ... )\n",
        "  graph = gn(graph)\n",
        "  ################\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51AwRipThKrH"
      },
      "source": [
        "Let's train the model!\n",
        "\n",
        "We expect the model to reach an accuracy of about 0.97."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEHco8n-Mr4b"
      },
      "outputs": [],
      "source": [
        "network = hk.without_apply_rng(hk.transform(gat_2_layers))\n",
        "result = optimize_club(network, num_steps=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Cns9EITp0k"
      },
      "source": [
        "The final node assignment predicted by the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QWqU8AfdkkV"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0leRi5igUUyd"
      },
      "outputs": [],
      "source": [
        "zacharys_karate_club = get_zacharys_karate_club()\n",
        "nx_graph = convert_jraph_to_networkx_graph(zacharys_karate_club)\n",
        "pos = nx.circular_layout(nx_graph)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 7))\n",
        "ax1 = fig.add_subplot(121)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=result.tolist(),\n",
        "    font_color='white')\n",
        "ax1.title.set_text('Predicted Node Assignments with GAT')\n",
        "\n",
        "gt_labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
        "ax2 = fig.add_subplot(122)\n",
        "nx.draw(\n",
        "    nx_graph,\n",
        "    pos=pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=gt_labels.tolist(),\n",
        "    font_color='white')\n",
        "ax2.title.set_text('Ground-Truth Node Assignments')\n",
        "fig.suptitle('Do you spot the difference? 😐', y=-0.01)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S8X2UovqE-I"
      },
      "source": [
        "## Summary\n",
        "In this tutorial, we learned about how to represent directed and undirected graphs in jraph, how to create the basic GNN blocks (GCN + GAT) and how to compose them into GNNs. We trained and tested them in the node prediction task using the simple ZKC dataset.\n",
        "\n",
        "\n",
        "To practice your knowledge, you can follow-up with the Part2 notebook, where you will learn how to exploit the generic class `jraph.GraphNetwork` and use it to tackle new tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wztiwpLIGh_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b70282eba0616fa5e4ed64a45ae049bb260824bbe5dfdc033db1152a6f241582"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}