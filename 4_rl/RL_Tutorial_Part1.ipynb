{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Tutorial_Part1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MwHaDeth-gSh",
        "I6KuVGSk4uc9",
        "_0YgLdsi3kXw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULdrhOaVbsdO"
      },
      "source": [
        "#RL Tutorial \n",
        "\n",
        "Contact us at adriap@google.com & bojanv@google.com for any questions/comments :)\n",
        "\n",
        "This tutorial was adapted from [EEML 2020 RL tutorial](https://github.com/eemlcommunity/PracticalSessions2020/tree/master/rl). Original author are Feryal Behbahani and Gheorghe Comanici. Special thanks to Diana Borsa and Loic Matthey.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv-846KxIqPD"
      },
      "source": [
        "The tutorial covers a number of important reinforcement learning (RL) algorithms, including multi-armed bandits, policy iteration, Q-Learning and deep RL. In the first part, we will guide you through the general interaction between RL agents and environments, where the agents ought to take actions in order to maximize returns (i.e. cumulative reward) in the simple multi-armed bandit setting. Next, we will implement Policy Iteration, SARSA, and Q-Learning for a simple tabular environment. The core ideas in the latter will be scaled to more complex MDPs through the use of function approximation. To do that, we will provide a short introduction to deep reinforcement learning and the DQN algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffeeXVm4AuZ6"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT8rkxTUAZzL"
      },
      "source": [
        "The agent interacts with the environment in a loop corresponding to the following diagram. The environment defines a set of <font color='blue'>**actions**</font>  that an agent can take.  The agent takes an action informed by the <font color='red'>**observations**</font> it receives, and will get a <font color='green'>**reward**</font> from the environment after each action. The goal in RL is to find an agent whose actions maximize the total accumulation of rewards obtained from the environment. \n",
        "\n",
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/rl_loop.png?raw=true)\n",
        "\n",
        "The tutorial is mainly focused on **value based methods**: agents are maintaining a value for all state-action pairs and use those estimates to choose actions that maximize that value (instead of maintaining a policy directly, like in policy gradient methods). \n",
        "\n",
        "We represent the action-value function (otherwise known as Q-function) associated with following/employing a policy $\\pi$ in a given MDP as:\n",
        "\n",
        "$$ Q^{\\pi}(\\color{red}{s},\\color{blue}{a}) = \\mathbb{E}_{\\tau \\sim P^{\\pi}} \\left[ \\sum_t \\gamma^t \\color{green}{R_t}| s_0=\\color{red}s,a=\\color{blue}{a_0} \\right]$$\n",
        "\n",
        "where $\\tau = \\{\\color{red}{s_0}, \\color{blue}{a_0}, \\color{green}{r_0}, \\color{red}{s_1}, \\color{blue}{a_1}, \\color{green}{r_1}, \\cdots \\}$\n",
        "\n",
        "\n",
        "Recall that efficient value estimations are based on the famous **_Bellman Optimality Equation_**:\n",
        "\n",
        "$$ Q^\\pi(\\color{red}{s},\\color{blue}{a}) =  \\color{green}{r}(\\color{red}{s},\\color{blue}{a}) + \\gamma  \\sum_{\\color{red}{s'}\\in \\color{red}{\\mathcal{S}}} P(\\color{red}{s'} |\\color{red}{s},\\color{blue}{a}) V^\\pi(\\color{red}{s'}) $$\n",
        "\n",
        "where $V^\\pi$ is the expected $Q^\\pi$ value for a particular state, i.e. $V^\\pi(\\color{red}{s}) = \\sum_{\\color{blue}{a} \\in \\color{blue}{\\mathcal{A}}} \\pi(\\color{blue}{a} |\\color{red}{s}) Q^\\pi(\\color{red}{s},\\color{blue}{a})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaJxoatMhJ71"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovuCuHCC78Zu"
      },
      "source": [
        "### Install required libraries\n",
        "\n",
        "1. [Acme](https://github.com/deepmind/acme) is a library of reinforcement learning (RL) agents and agent building blocks. Acme strives to expose simple, efficient, and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research. The design of Acme also attempts to provide multiple points of entry to the RL problem at differing levels of complexity.\n",
        "\n",
        "\n",
        "2. [Haiku](https://github.com/deepmind/dm-haiku) is a simple neural network library for JAX developed by some of the authors of Sonnet, a neural network library for TensorFlow.\n",
        "\n",
        "3. [dm_env](https://github.com/deepmind/dm_env): DeepMind Environment API, which will be covered in more details in the [Environment subsection](https://colab.research.google.com/drive/1oKyyhOFAFSBTpVnmuOm9HXh5D5ekqhh5#scrollTo=I6KuVGSk4uc9) below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH3O0zcXUeun",
        "cellView": "form"
      },
      "source": [
        "#@title Installations  { form-width: \"30%\" }\n",
        "\n",
        "!pip install dm-reverb[tensorflow]\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb]\n",
        "!pip install dm-acme[jax]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-acme[envs]\n",
        "!pip install dm-env\n",
        "!pip install dm-haiku\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install imageio\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-H2d6UZi7Sf"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ74Id-8MERq",
        "cellView": "form"
      },
      "source": [
        "#@title Imports  { form-width: \"30%\" }\n",
        "\n",
        "import IPython\n",
        "\n",
        "import acme\n",
        "from acme import environment_loop\n",
        "from acme import datasets\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.adders import reverb as adders_reverb\n",
        "from acme.wrappers import gym_wrapper\n",
        "from acme.agents.jax import dqn\n",
        "from acme.adders import reverb as adders\n",
        "from acme.utils import counting\n",
        "from acme.utils import loggers\n",
        "from acme.jax import networks as networks_lib\n",
        "from acme.jax import utils as acme_utils\n",
        "import base64\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import dm_env\n",
        "import enum\n",
        "import functools\n",
        "import gym\n",
        "import haiku as hk\n",
        "import io\n",
        "import imageio\n",
        "import itertools\n",
        "import jax\n",
        "from jax import tree_util\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import reverb\n",
        "import rlax\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeGPIOMkUTEn"
      },
      "source": [
        "# RL Lab - Part 0: Environment & Agent, Multi-armed Bandit Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHaDeth-gSh"
      },
      "source": [
        "## 0.0: Multi-armed bandit problem\n",
        "\n",
        "Let's start with the simplified RL problem - namely the multi-armed bandit problem. In this setup, the agent can \"pull one of $N$ arms\" i.e. perform one of $N$ <font color='blue'>**Actions**</font> that the environment offers.\n",
        "\n",
        "Each arm, after being pulled, returns a <font color='green'>**Reward**</font>, according to a stationary distribution specific for each arm. The episode then ends. Note that in this case the return is equal to this single reward. There are no observations nor discounting. \n",
        "\n",
        "Since the episodes last only 1 step, the agents <b>don't keep a state</b> within an episode. Therefore, the bandit agents (covered in this tutorial) will try to estimate the expected reward of an arm (i.e. action):\n",
        "\n",
        "$$ Q(\\color{blue}{a}) = \\mathbb{E} \\left[\\color{green}{R}|,a=\\color{blue}{a} \\right]$$\n",
        "\n",
        "And reach the optimal policy of picking the optimal action $$ \\pi_{greedy}(\\color{blue}a) = \\pi_{optimal}(\\color{blue}a) = \\begin{cases}\n",
        "      1, & \\text{if}\\ \\color{blue}a=\\arg\\max_{a'} Q(a') \\\\\n",
        "      0, & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFJAURPjB8ez"
      },
      "source": [
        "## 0.1: Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVrvdNd2J54B"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In this tutorial we will use [dm_env](https://github.com/deepmind/dm_env) interface for building environments. You don't need to worry about the implementation details of the environments, it's important to note that the environments have:\n",
        "\n",
        "\n",
        "> `__init__(self, parameter_1, parameter_2, ...)`\n",
        "\n",
        "The constructor will create and initialise the environment with the passed parameters.\n",
        "\n",
        "> `step(self, action)`\n",
        "\n",
        "The method to interact with the environment which returns a `TimeStep` namedtuple with fields: `step_type, reward, discount, observation`.\n",
        "\n",
        "> `reset(self)`\n",
        "\n",
        "The method that forces the start of a new sequence and returns the first `observation`.\n",
        "\n",
        "For more information, see [dm_env docs](https://github.com/deepmind/dm_env/blob/master/docs/index.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oe50DiUKIqJ"
      },
      "source": [
        "### Rooms with Rewards Environment\n",
        "\n",
        "For part 0 of the tutorial we will use one of the simplest possible environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dizwj8YNVGH_"
      },
      "source": [
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/rl_loop_bandit.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOnjK1KIMDQN"
      },
      "source": [
        "The agent has a choice to visit one of $N$ rooms. In each room, it will find a reward and the episode will terminate. Each room, once visited, samples a reward according to it's own distribution, unknown to the agent. The reward distributions (picked for this exercise) are uniform, with some rooms having potentially overlapping intervals (the more overlap - the more difficult to determine the best room)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbf5VQG3KZJp",
        "cellView": "form"
      },
      "source": [
        "#@title Rooms with Rewards Implementation  { form-width: \"30%\" }\n",
        "\n",
        "class RoomsWithRewardsWorld(dm_env.Environment):\n",
        "  def __init__(self,\n",
        "               num_rooms,\n",
        "               distributions_overlap=0.7,\n",
        "               max_reward=10.0):\n",
        "    \"\"\"Build the rooms with rewards environment.\n",
        "\n",
        "    Args:\n",
        "      num_rooms: int, greater or equal to 2.\n",
        "      distributions_overlap: float from [0, 1>. Determines how much the\n",
        "        intervals of uniform reward distributions of each room overlap and\n",
        "        therefore the difficulty of the problem.\n",
        "      max_reward: positive float, the maximum reward obtainable from any room. The min\n",
        "        is assumed to be 0.0.\n",
        "    \"\"\"\n",
        "    if num_rooms < 2:\n",
        "      raise ValueError('num_rooms must be greater or equal to 2.')\n",
        "    self._num_rooms = num_rooms\n",
        "\n",
        "    if distributions_overlap < 0.0 or distributions_overlap >= 1:\n",
        "      raise ValueError('distributions_overlap must be in [0, 1>.')\n",
        "    self._distributions_overlap = distributions_overlap\n",
        "\n",
        "    if max_reward <= 0.0:\n",
        "      raise ValueError('max_reward must be positive.')\n",
        "    self._max_reward = max_reward\n",
        "\n",
        "    # This scary-looking expression simply calculated the equal interval lengths\n",
        "    # of each room, such that the overlap between 2 rooms is equal to \n",
        "    # self._distributions_overlap.\n",
        "    self._interval_len_distributions = (\n",
        "        self._max_reward /\n",
        "        (self._num_rooms - self._distributions_overlap * (self._num_rooms - 1)))\n",
        "\n",
        "    # This calculates the expected reward for each room and the shuffles them. \n",
        "    self._expected_rewards_room = (\n",
        "        self._interval_len_distributions / 2.0 +\n",
        "        np.arange(self._num_rooms) * self._interval_len_distributions * (\n",
        "            1 - self._distributions_overlap)\n",
        "        )\n",
        "    self._expected_rewards_room = np.random.permutation(\n",
        "        self._expected_rewards_room)\n",
        "    \n",
        "    self._min_bounds_room = (\n",
        "        self._expected_rewards_room - self._interval_len_distributions / 2.0)\n",
        "    self._max_bounds_room = (\n",
        "        self._expected_rewards_room + self._interval_len_distributions / 2.0)\n",
        "    \n",
        "    self._room_names = tuple(\n",
        "        \"room_{}\".format(i) for i in range(self._num_rooms))\n",
        "    \n",
        "  def plot_distributions(self):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.ylim((0.0, self._max_reward))\n",
        "    plt.ylabel(\"reward distribution - uniform distribution interval\")\n",
        "\n",
        "    plt.xticks(np.arange(self._num_rooms), self._room_names, rotation=45.0)\n",
        "    ax = plt.gca()\n",
        "    ax.grid(axis=\"x\", b=None)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "\n",
        "    for room_index, (min_bound_room, max_bound_room) in enumerate(\n",
        "        zip(self._min_bounds_room, self._max_bounds_room)):\n",
        "      plt.plot(\n",
        "          [room_index, room_index], [min_bound_room, max_bound_room],\n",
        "          linewidth=4)\n",
        "  \n",
        "  def _sample_reward(self, room_index):\n",
        "    sampled_reward = np.random.uniform(\n",
        "        low=self._min_bounds_room[room_index],\n",
        "        high=self._max_bounds_room[room_index])\n",
        "    return np.round(sampled_reward, decimals=2)\n",
        "\n",
        "  def expected_reward_best_room(self):\n",
        "    return np.max(self._expected_rewards_room)\n",
        "\n",
        "  def best_room_index(self):\n",
        "    return np.argmax(self._expected_rewards_room)\n",
        "      \n",
        "  def observation_spec(self):\n",
        "    return None\n",
        "\n",
        "  def action_spec(self):\n",
        "    return specs.BoundedArray(\n",
        "        shape=(1,),\n",
        "        dtype=np.int32,\n",
        "        minimum=0,\n",
        "        maximum=self._num_rooms-1,\n",
        "        name=\"action_room_index_to_visit\")\n",
        "  \n",
        "  def reset(self):\n",
        "    return dm_env.TimeStep(\n",
        "        step_type=dm_env.StepType.FIRST,\n",
        "        reward=None,\n",
        "        discount=None,\n",
        "        observation=None)\n",
        "\n",
        "  def step(self, action):\n",
        "    room_index_to_visit = action\n",
        "    return dm_env.TimeStep(\n",
        "        step_type=dm_env.StepType.LAST,\n",
        "        reward=self._sample_reward(room_index_to_visit),\n",
        "        discount=1.0,\n",
        "        observation=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxPQWdquUhoT"
      },
      "source": [
        "Again, don't worry about implementation code.\n",
        "\n",
        "Run the cells below which plot the reward distributions of rooms (caution: privileged information, agent doesn't have access to this) and step through the environment to get the feeling for the problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY_lMi51Tm4w"
      },
      "source": [
        "num_rooms = 5  #@param\n",
        "distributions_overlap = 0.6 #@param\n",
        "\n",
        "environment = RoomsWithRewardsWorld(\n",
        "    num_rooms=num_rooms,\n",
        "    distributions_overlap=distributions_overlap)\n",
        "environment.plot_distributions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZyjvuVjU4mT"
      },
      "source": [
        "room_index_to_visit = 2 #@param\n",
        "# Action corresponds to the index of the room you\n",
        "# want to visit\n",
        "action = room_index_to_visit\n",
        "print(environment.step(action))\n",
        "print(environment.step(action))\n",
        "print(environment.step(action))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZjeiBbcDGVe"
      },
      "source": [
        "## 0.2: Agents\n",
        "\n",
        "We will be implementing Tabular & Function Approximation agents. Tabular agents are purely in Python (NumPy) while for Function Approximation agents, we will use JAX. We will follow an ACME `Actor` interface. While you can always write your own actor, we also provide a number of useful premade versions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MAYbEvtJ1kT"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Each agent implements the following functions:\n",
        "\n",
        "\n",
        "\n",
        "> `__init__(self, number_of_actions, (number_of_states), ...)`\n",
        "\n",
        "\n",
        "The constructor will provide the agent the number of actions, potentially number of\n",
        "states, alongside other parameters.\n",
        "\n",
        "> `select_action(self, observation)`:\n",
        "\n",
        "This is the policy used by the actor to interact with the environment.\n",
        "\n",
        "> `observe_first(self, timestep)`:\n",
        "\n",
        "This function provides the agent with initial timestep in a given episode. Note\n",
        "that this is not the result of an action choice by the agent, hence it will only\n",
        "have `timestep.observation` set to a proper value.\n",
        "\n",
        "> `observe(self, action, next_timestep)`:\n",
        "\n",
        "This function provides the agent with the timestep that resulted from the given\n",
        "action choice. The timestep provides a `reward`, a `discount`, and an\n",
        "`observation`, all results of the previous action choice.\n",
        "\n",
        "Note: `timestep.step_type` will be either `MID` or `LAST` and should be used to\n",
        "determine whether this is the last observation in the episode.\n",
        "\n",
        "> `update(self)`:\n",
        "\n",
        "This function is called after the `observe` function and performs an update of the actor parameters (and anything else if needed). There are no arguments to this function but the quantities necessary to perform the update can be read/written via class variables (e.g.\n",
        "\n",
        "```  self._q[self._state, self._action] += ...```\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XD9bXC3UCHd"
      },
      "source": [
        "### Random Bandit Agent\n",
        "\n",
        "Let's create the first agent for our simple rooms environment. We can just choose the room to visit randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lU-ybzz4Ng7",
        "cellView": "form"
      },
      "source": [
        "#@title Build a random Agent  { form-width: \"30%\" }\n",
        "\n",
        "class RandomAgent(acme.Actor):\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions):\n",
        "    self._num_actions = num_actions\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    # Uniform random policy\n",
        "    return np.random.randint(self._num_actions)   \n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    \"\"\"The agent is being notified that environment was reset.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    \"\"\"The agent is being notified of an environment step.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def update(self):    \n",
        "    \"\"\"Agent should update its parameters.\"\"\"\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85mX1RMbZ1Cq"
      },
      "source": [
        "num_actions = num_rooms\n",
        "random_bandit_agent = RandomAgent(num_actions)\n",
        "\n",
        "# Nones as observations because the rooms environment\n",
        "# doesn't return any observations.\n",
        "for _ in range(3):\n",
        "  action = random_bandit_agent.select_action(None)\n",
        "  print('Room picked: {}'.format(action))\n",
        "  reward = environment.step(action).reward\n",
        "  print('Reward obtained: {}'.format(reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqD1TXKhDr0x"
      },
      "source": [
        "### Run loop \n",
        "\n",
        "Here we provide the utility function for the agent-environment loop. It will be used throughout the tutorial. Make sure you read through the arguments section of the docstring to familiarise yourself with how to use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0tDi1sD-gSv"
      },
      "source": [
        "#@title Run loop  { form-width: \"30%\" }\n",
        "\n",
        "\n",
        "def run_loop(environment,\n",
        "             agent,\n",
        "             num_episodes=None,\n",
        "             num_steps=None,\n",
        "             logger_time_delta=1.,\n",
        "             label='training_loop',\n",
        "             log_loss=False,\n",
        "             ):\n",
        "  \"\"\"Perform the run loop.\n",
        "\n",
        "  We are following the Acme run loop.\n",
        "\n",
        "  Run the environment loop for `num_episodes` episodes. Each episode is itself\n",
        "  a loop which interacts first with the environment to get an observation and\n",
        "  then gives that observation to the agent in order to retrieve an action. Upon\n",
        "  termination of an episode a new episode will be started. If the number of\n",
        "  episodes is not given then this will interact with the environment\n",
        "  infinitely.\n",
        "\n",
        "  Args:\n",
        "    environment: dm_env used to generate trajectories.\n",
        "    agent: acme.Actor for selecting actions in the run loop.\n",
        "    num_episodes: number of episodes to run the loop for. If `None` (default),\n",
        "       runs without limit.\n",
        "    num_steps: number of steps (across episodes) to run the loop for. If `None` \n",
        "      (default), runs without limit.\n",
        "    logger_time_delta: time interval (in seconds) between consecutive logging\n",
        "      steps.\n",
        "    label: optional label used at logging steps.\n",
        "\n",
        "  Returns:\n",
        "    all_returns: a list of floats, return for each episode played.\n",
        "    all_actions: a list of lists of actions. The outer list has the same number\n",
        "      of elements as all_returns i.e. the number of episodes played. The inner\n",
        "      lists can have a variable number of elements, one action for each timestep\n",
        "      of an episode.\n",
        "  \"\"\"\n",
        "  logger = loggers.TerminalLogger(label=label, time_delta=logger_time_delta)\n",
        "  iterator = range(num_episodes) if num_episodes else itertools.count()\n",
        "  all_returns = []\n",
        "  all_actions = []\n",
        "  \n",
        "  num_total_steps = 0\n",
        "  for episode in iterator:\n",
        "    # Reset any counts and start the environment.\n",
        "    start_time = time.time()\n",
        "    episode_steps = 0\n",
        "    episode_return = 0\n",
        "    episode_loss = 0\n",
        "    all_actions.append([])\n",
        "\n",
        "    timestep = environment.reset()\n",
        "    \n",
        "    # Make the first observation.\n",
        "    agent.observe_first(timestep)\n",
        "\n",
        "    # Run an episode.\n",
        "    while not timestep.last():\n",
        "      # Generate an action from the agent's policy and step the environment.\n",
        "      action = agent.select_action(timestep.observation)\n",
        "      timestep = environment.step(action)\n",
        "\n",
        "      # Have the agent observe the timestep and let the agent update itself.\n",
        "      agent.observe(action, next_timestep=timestep)\n",
        "      agent.update()\n",
        "\n",
        "      # Book-keeping.\n",
        "      episode_steps += 1\n",
        "      num_total_steps += 1\n",
        "      episode_return += timestep.reward\n",
        "      all_actions[-1].append(action)\n",
        "\n",
        "      if log_loss:\n",
        "        episode_loss += agent.last_loss\n",
        "\n",
        "      if num_steps is not None and num_total_steps >= num_steps:\n",
        "        break\n",
        "\n",
        "    # Collect the results and combine with counts.\n",
        "    steps_per_second = episode_steps / (time.time() - start_time)\n",
        "    result = {\n",
        "        'episode': episode,\n",
        "        'episode_length': episode_steps,\n",
        "        'episode_return': episode_return,\n",
        "    }\n",
        "    if log_loss:\n",
        "      result['loss_avg'] = episode_loss/episode_steps\n",
        "\n",
        "    all_returns.append(episode_return)\n",
        "\n",
        "    # Log the given results.\n",
        "    logger.write(result)\n",
        "    \n",
        "    if num_steps is not None and num_total_steps >= num_steps:\n",
        "      break\n",
        "\n",
        "  return all_returns, all_actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb6WVbfALtLa"
      },
      "source": [
        "## 0.3: Epsilon-greedy Bandit Agent\n",
        "\n",
        "Epsilon-greedy strategy: \n",
        "\n",
        "- estimate $Q(\\color{blue}{a})$ as the mean of all the rewards $\\color{green}{R}$ received after performing $\\color{blue}{a}$ \n",
        "- balance exploration and exploitation using epsilon-greedy policy\n",
        "$$ \\begin{array}{l}\n",
        "      \\text{with probability } 1- \\epsilon, & \\color{blue}a=\\arg\\max_{a'} Q(a') \\\\\n",
        "      \\text{otherwise}, & \\text{random } \\color{blue}a\n",
        "\\end{array}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKIadbmOV48C"
      },
      "source": [
        "#@title **[Coding Task Easy]** Epsilon-greedy Bandit Agent { form-width: \"30%\" }\n",
        "\n",
        "class EpsilonGreedyBanditAgent(acme.Actor):\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions, \n",
        "      epsilon=0.1):\n",
        "    self._num_actions = num_actions\n",
        "    self._q = np.zeros((num_actions, ))\n",
        "    self._counts = np.zeros((num_actions, ), dtype=np.int32)\n",
        "    self._epsilon = epsilon\n",
        "\n",
        "    self._action_performed = None\n",
        "    self._reward_observed = None\n",
        "\n",
        "  def _epsilon_greedy(self):\n",
        "    if np.random.rand() < self._epsilon:\n",
        "      return np.random.randint(0, self._num_actions)\n",
        "\n",
        "    return np.argmax(self._q)\n",
        "\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    return self._epsilon_greedy()    \n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    \"\"\"The agent is being notified that environment was reset.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    \"\"\"The agent is being notified of an environment step.\"\"\"\n",
        "    self._action_performed = action\n",
        "    self._reward_observed = next_timestep.reward\n",
        "    self._counts[action] += 1\n",
        "\n",
        "  def update(self):    \n",
        "    \"\"\"Agent should update its parameters.\"\"\"\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # update the q-values for the performed action using incremental update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6enuXW0KbW_F",
        "cellView": "form"
      },
      "source": [
        "#@title Setup environment { form-width: \"30%\" }\n",
        "\n",
        "\n",
        "num_rooms = 20  #@param\n",
        "distributions_overlap = 0.8  #@param\n",
        "num_episodes = 3000  #@param\n",
        "env = RoomsWithRewardsWorld(num_rooms=num_rooms, distributions_overlap=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpTjUK4_ahZI",
        "cellView": "form"
      },
      "source": [
        "#@title Run epsilon-greedy { form-width: \"30%\" }\n",
        "\n",
        "epsilon = 0.07  #@param\n",
        "all_returns_epsilon_greedy, all_actions_epsilon_greedy = run_loop(\n",
        "    env,\n",
        "    EpsilonGreedyBanditAgent(num_actions=num_rooms, epsilon=epsilon),\n",
        "    num_episodes=num_episodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLi5rPvHfNCW"
      },
      "source": [
        "We will evaluate our strategies by plotting:\n",
        "- **total regret** - that is, the expected reward we failed to collect by not performing unknown optimal policy\n",
        "- percentage of picking the optimal action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhtABG4Qbn8y",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate epsilon-greedy { form-width: \"30%\" }\n",
        "\n",
        "def evaluate_bandit_agent_run(env, all_returns, all_actions):\n",
        "  regrets = env.expected_reward_best_room() - np.array(all_returns)\n",
        "  total_regrets = np.cumsum(regrets)\n",
        "\n",
        "  def moving_average(seq, window=100) :\n",
        "    cumsum_seq = np.cumsum(seq)\n",
        "    cumsum_seq[window:] = cumsum_seq[window:] - cumsum_seq[:-window]\n",
        "    return cumsum_seq[window - 1:] / window\n",
        "\n",
        "  plt.figure(figsize = (6, 8))\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.xlim([0, num_episodes])\n",
        "  plt.ylim([0, max(total_regrets) * 1.05])\n",
        "  plt.ylabel(\"total regret\");\n",
        "  plt.plot(total_regrets)\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.xlim([0, num_episodes])\n",
        "\n",
        "  window = 100\n",
        "  percentage_best_action_picked = moving_average(\n",
        "      np.squeeze(np.array(all_actions)) == env.best_room_index(),\n",
        "      window=window)\n",
        "  plt.plot(\n",
        "      np.arange(len(percentage_best_action_picked)) + window,\n",
        "      percentage_best_action_picked)\n",
        "  plt.xlabel(\"episodes\")\n",
        "  plt.ylabel(\"percentage optimal action\");\n",
        "\n",
        "evaluate_bandit_agent_run(\n",
        "    env,\n",
        "    all_returns_epsilon_greedy,\n",
        "    all_actions_epsilon_greedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPkrTXOHviEe"
      },
      "source": [
        "Note that with epsilon-greedy, it's never possible to keep picking the optimal action, even if it's found. This can easily be solved by decaying epsilon in the course of training (left to be tried for homework). Another thing that helps exploring the unvisited actions is optimistic initialisation (setting them to high initial value - why?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KoBXhDcXm7S"
      },
      "source": [
        "### **[Homework]** Epsilon-greedy with decaying epsilon and optimistic initialisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiFdFuz2MdPQ"
      },
      "source": [
        "## 0.4: UCB1 Bandit Agent\n",
        "\n",
        "UCB1 algorithm tries to account for **uncertainty** in the estimates of Q-values and favours the actions with larger uncertainty $U(\\color{blue}{a})$. The action selection strategy is:\n",
        "\n",
        "$$\\color{blue}{a}=\\arg\\max_{a'} \\left[Q(a') + U(a')\\right]$$\n",
        "where\n",
        "$$U(a) = \\sqrt{\\frac{2\\log t}{N(a)}}$$\n",
        "$t$ - total number of episodes/actions performed so far\n",
        "\n",
        "$N(a)$ - count of action $a$, how many times action $a$ was picked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZbuxcm1hgQ"
      },
      "source": [
        "#@title **[Coding task]** UCB1 bandit agent { form-width: \"30%\" }\n",
        "\n",
        "\n",
        "class UCB1Agent(acme.Actor):\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions):\n",
        "    self._num_actions = num_actions\n",
        "    self._q = np.zeros((num_actions, ))\n",
        "    self._counts = np.zeros((num_actions, ), dtype=np.int32) \n",
        "\n",
        "    self._action_performed = None\n",
        "    self._reward_observed = None\n",
        "\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # write the action selection rule for UCB1\n",
        "    return None\n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    \"\"\"The agent is being notified that environment was reset.\"\"\"\n",
        "    pass\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    \"\"\"The agent is being notified of an environment step.\"\"\"\n",
        "    self._action_performed = action\n",
        "    self._reward_observed = next_timestep.reward\n",
        "    self._counts[action] += 1\n",
        "\n",
        "  def update(self):    \n",
        "    \"\"\"Agent should update its parameters.\"\"\"\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # update the q-values for the performed action using incremental update\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UciiNeMpROq",
        "cellView": "form"
      },
      "source": [
        "#@title Run UCB1 { form-width: \"30%\" }\n",
        "\n",
        "ucb1_agent = UCB1Agent(num_actions=num_rooms)\n",
        "all_returns_ucb1, all_actions_ucb1 = run_loop(\n",
        "    env,\n",
        "    ucb1_agent,\n",
        "    num_episodes=num_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMFHO_tFpO4m",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate UCB1 { form-width: \"30%\" }\n",
        "\n",
        "evaluate_bandit_agent_run(\n",
        "    env,\n",
        "    all_returns_ucb1,\n",
        "    all_actions_ucb1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwUyC0C7pI5u"
      },
      "source": [
        "How does it compare to epsilon-greedy? What's the shape of the total regret curve?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jef_plXFSV9J"
      },
      "source": [
        "# RL Lab - Part 1: Full RL and Tabular Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAiZn23xICKh"
      },
      "source": [
        "\n",
        "## 1.0: Overview\n",
        "\n",
        "The next set of exercises are based on the full RL setup, where the episodes last more than one step. However, the problem we'll be solving is still simple enough (the number of states is small enough) for our agents to maintain a table of values for each individual state that it will ever encounter.\n",
        "\n",
        "In particular, we will consider a GridWorld environment with a fixed layout, and the goal is always to reach the same final location, hence the state is fully determined by the location of the agent. As such, the <font color='red'>observation</font> from the environment is an integer corresponding to each one of approximately 50 locations on the grid. Further explanation of the environment and implementation in the cells below.\n",
        "\n",
        "We will cover three basic RL tabular algorithms:\n",
        "- Policy iteration\n",
        "- SARSA Agent\n",
        "- Q-learning Agent\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6KuVGSk4uc9"
      },
      "source": [
        "## 1.1: Gridworld Environment\n",
        "**Note** Please expand this section (if the cells happen to be hidden by default) and go carefully through this section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZwB__DPcyM"
      },
      "source": [
        "\n",
        "In this part of the tutorial, we will focus on a simple grid world environment.\n",
        "\n",
        "\n",
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/rl_loop_gridworld.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "This environment consists of either walls and empty cells. The agent starts from an initial location and needs to navigate to reach a goal location. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inIAhwLKuHKr",
        "cellView": "form"
      },
      "source": [
        "#@title Gridworld Implementation { form-width: \"30%\" }\n",
        "\n",
        "\n",
        "class ObservationType(enum.IntEnum):\n",
        "  STATE_INDEX = enum.auto()\n",
        "  AGENT_ONEHOT = enum.auto()\n",
        "  GRID = enum.auto()\n",
        "  AGENT_GOAL_POS = enum.auto()\n",
        "\n",
        "\n",
        "class GridWorld(dm_env.Environment):\n",
        "\n",
        "  def __init__(self,\n",
        "               layout,\n",
        "               start_state,\n",
        "               goal_state=None,\n",
        "               observation_type=ObservationType.STATE_INDEX,\n",
        "               discount=0.9,\n",
        "               penalty_for_walls=-5,\n",
        "               reward_goal=10,\n",
        "               max_episode_length=None,\n",
        "               randomize_goals=False):\n",
        "    \"\"\"Build a grid environment.\n",
        "\n",
        "    Simple gridworld defined by a map layout, a start and a goal state.\n",
        "\n",
        "    Layout should be a NxN grid, containing:\n",
        "      * 0: empty\n",
        "      * -1: wall\n",
        "      * Any other positive value: value indicates reward; episode will terminate\n",
        "\n",
        "    Args:\n",
        "      layout: NxN array of numbers, indicating the layout of the environment.\n",
        "      start_state: Tuple (y, x) of starting location.\n",
        "      goal_state: Optional tuple (y, x) of goal location. Will be randomly\n",
        "        sampled once if None.\n",
        "      observation_type: Enum observation type to use. One of:\n",
        "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\n",
        "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the \n",
        "          agent is and 0 elsewhere.\n",
        "        * ObservationType.GRID: NxNx3 float32 grid of feature channels. \n",
        "          First channel contains walls (1 if wall, 0 otherwise), second the \n",
        "          agent position (1 if agent, 0 otherwise) and third goal position\n",
        "          (1 if goal, 0 otherwise)\n",
        "        * ObservationType.AGENT_GOAL_POS: float32 tuple with \n",
        "          (agent_y, agent_x, goal_y, goal_x)\n",
        "      discount: Discounting factor included in all Timesteps.\n",
        "      penalty_for_walls: Reward added when hitting a wall (should be negative).\n",
        "      reward_goal: Reward added when finding the goal (should be positive).\n",
        "      max_episode_length: If set, will terminate an episode after this many \n",
        "        steps.\n",
        "      randomize_goals: If true, randomize goal at every episode.\n",
        "    \"\"\"\n",
        "    if observation_type not in ObservationType:\n",
        "      raise ValueError('observation_type should be a ObservationType instance.')\n",
        "    self._layout = np.array(layout)\n",
        "    self._start_state = start_state\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._discount = discount\n",
        "    self._penalty_for_walls = penalty_for_walls\n",
        "    self._reward_goal = reward_goal\n",
        "    self._observation_type = observation_type\n",
        "    self._layout_dims = self._layout.shape\n",
        "    self._max_episode_length = max_episode_length\n",
        "    self._num_episode_steps = 0\n",
        "    self._randomize_goals = randomize_goals\n",
        "    if goal_state is None:\n",
        "      # Randomly sample goal_state if not provided\n",
        "      goal_state = self._sample_goal()\n",
        "    self.goal_state = goal_state\n",
        "\n",
        "  def _sample_goal(self):\n",
        "    \"\"\"Randomly sample reachable non-starting state.\"\"\"\n",
        "    # Sample a new goal\n",
        "    n = 0\n",
        "    max_tries = 1e5\n",
        "    while n < max_tries:\n",
        "      goal_state = tuple(np.random.randint(d) for d in self._layout_dims)\n",
        "      if goal_state != self._state and self._layout[goal_state] == 0:\n",
        "        # Reachable state found!\n",
        "        return goal_state\n",
        "      n += 1\n",
        "    raise ValueError('Failed to sample a goal state.')\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "    return self._number_of_states\n",
        "\n",
        "  @property\n",
        "  def goal_state(self):\n",
        "    return self._goal_state\n",
        "\n",
        "  def set_state(self, x, y):\n",
        "    self._state = (y, x)\n",
        "\n",
        "  @goal_state.setter\n",
        "  def goal_state(self, new_goal):\n",
        "    if new_goal == self._state or self._layout[new_goal] < 0:\n",
        "      raise ValueError('This is not a valid goal!')\n",
        "    # Zero out any other goal\n",
        "    self._layout[self._layout > 0] = 0\n",
        "    # Setup new goal location\n",
        "    self._layout[new_goal] = self._reward_goal\n",
        "    self._goal_state = new_goal\n",
        "\n",
        "  def observation_spec(self):\n",
        "    if self._observation_type is ObservationType.AGENT_ONEHOT:\n",
        "      return specs.Array(\n",
        "          shape=self._layout_dims,\n",
        "          dtype=np.float32,\n",
        "          name='observation_agent_onehot')\n",
        "    elif self._observation_type is ObservationType.GRID:\n",
        "      return specs.Array(\n",
        "          shape=self._layout_dims + (3,),\n",
        "          dtype=np.float32,\n",
        "          name='observation_grid')\n",
        "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\n",
        "      return specs.Array(\n",
        "          shape=(4,), dtype=np.float32, name='observation_agent_goal_pos')\n",
        "    elif self._observation_type is ObservationType.STATE_INDEX:\n",
        "      return specs.DiscreteArray(\n",
        "          self._number_of_states, dtype=int, name='observation_state_index')\n",
        "\n",
        "  def action_spec(self):\n",
        "    return specs.DiscreteArray(4, dtype=int, name='action')\n",
        "\n",
        "  def get_obs(self):\n",
        "    if self._observation_type is ObservationType.AGENT_ONEHOT:\n",
        "      obs = np.zeros(self._layout.shape, dtype=np.float32)\n",
        "      # Place agent\n",
        "      obs[self._state] = 1\n",
        "      return obs\n",
        "    elif self._observation_type is ObservationType.GRID:\n",
        "      obs = np.zeros(self._layout.shape + (3,), dtype=np.float32)\n",
        "      obs[..., 0] = self._layout < 0\n",
        "      obs[self._state[0], self._state[1], 1] = 1\n",
        "      obs[self._goal_state[0], self._goal_state[1], 2] = 1\n",
        "      return obs\n",
        "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\n",
        "      return np.array(self._state + self._goal_state, dtype=np.float32)\n",
        "    elif self._observation_type is ObservationType.STATE_INDEX:\n",
        "      y, x = self._state\n",
        "      return y * self._layout.shape[1] + x\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._start_state\n",
        "    self._num_episode_steps = 0\n",
        "    if self._randomize_goals:\n",
        "      self.goal_state = self._sample_goal()\n",
        "    return dm_env.TimeStep(\n",
        "        step_type=dm_env.StepType.FIRST,\n",
        "        reward=None,\n",
        "        discount=None,\n",
        "        observation=self.get_obs())\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'Invalid action: {} is not 0, 1, 2, or 3.'.format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    step_type = dm_env.StepType.MID\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = self._penalty_for_walls\n",
        "      discount = self._discount\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = self._discount\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "      step_type = dm_env.StepType.LAST\n",
        "\n",
        "    self._state = new_state\n",
        "    self._num_episode_steps += 1\n",
        "    if (self._max_episode_length is not None and\n",
        "        self._num_episode_steps >= self._max_episode_length):\n",
        "      step_type = dm_env.StepType.LAST\n",
        "    return dm_env.TimeStep(\n",
        "        step_type=step_type,\n",
        "        reward=np.float32(reward),\n",
        "        discount=discount,\n",
        "        observation=self.get_obs())\n",
        "\n",
        "  def plot_grid(self, add_start=True):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout <= -1, interpolation='nearest')\n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    # Add start/goal\n",
        "    if add_start:\n",
        "      plt.text(\n",
        "          self._start_state[1],\n",
        "          self._start_state[0],\n",
        "          r'$\\mathbf{S}$',\n",
        "          fontsize=16,\n",
        "          ha='center',\n",
        "          va='center')\n",
        "    plt.text(\n",
        "        self._goal_state[1],\n",
        "        self._goal_state[0],\n",
        "        r'$\\mathbf{G}$',\n",
        "        fontsize=16,\n",
        "        ha='center',\n",
        "        va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h - 1):\n",
        "      plt.plot([-0.5, w - 0.5], [y + 0.5, y + 0.5], '-k', lw=2)\n",
        "    for x in range(w - 1):\n",
        "      plt.plot([x + 0.5, x + 0.5], [-0.5, h - 0.5], '-k', lw=2)\n",
        "\n",
        "  def plot_state(self, return_rgb=False):\n",
        "    self.plot_grid(add_start=False)\n",
        "    # Add the agent location\n",
        "    plt.text(\n",
        "        self._state[1],\n",
        "        self._state[0],\n",
        "        u'😃',\n",
        "        fontname='symbola',\n",
        "        fontsize=18,\n",
        "        ha='center',\n",
        "        va='center',\n",
        "    )\n",
        "    if return_rgb:\n",
        "      fig = plt.gcf()\n",
        "      plt.axis('tight')\n",
        "      plt.subplots_adjust(0, 0, 1, 1, 0, 0)\n",
        "      fig.canvas.draw()\n",
        "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "      w, h = fig.canvas.get_width_height()\n",
        "      data = data.reshape((h, w, 3))\n",
        "      plt.close(fig)\n",
        "      return data\n",
        "\n",
        "  def plot_policy(self, policy):\n",
        "    action_names = [\n",
        "        r'$\\uparrow$', r'$\\rightarrow$', r'$\\downarrow$', r'$\\leftarrow$'\n",
        "    ]\n",
        "    self.plot_grid()\n",
        "    plt.title('Policy Visualization')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h):\n",
        "      for x in range(w):\n",
        "        # if ((y, x) != self._start_state) and ((y, x) != self._goal_state):\n",
        "        if (y, x) != self._goal_state:\n",
        "          action_name = action_names[policy[y, x]]\n",
        "          plt.text(x, y, action_name, ha='center', va='center')\n",
        "\n",
        "  def plot_greedy_policy(self, q):\n",
        "    greedy_actions = np.argmax(q, axis=2)\n",
        "    self.plot_policy(greedy_actions)\n",
        "\n",
        "\n",
        "def build_gridworld_task(task,\n",
        "                         discount=0.9,\n",
        "                         penalty_for_walls=-5,\n",
        "                         observation_type=ObservationType.STATE_INDEX,\n",
        "                         max_episode_length=200):\n",
        "  \"\"\"Construct a particular Gridworld layout with start/goal states.\n",
        "\n",
        "  Args:\n",
        "      task: string name of the task to use. One of {'simple', 'obstacle', \n",
        "        'random_goal'}.\n",
        "      discount: Discounting factor included in all Timesteps.\n",
        "      penalty_for_walls: Reward added when hitting a wall (should be negative).\n",
        "      observation_type: Enum observation type to use. One of:\n",
        "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\n",
        "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the \n",
        "          agent is and 0 elsewhere.\n",
        "        * ObservationType.GRID: NxNx3 float32 grid of feature channels. \n",
        "          First channel contains walls (1 if wall, 0 otherwise), second the \n",
        "          agent position (1 if agent, 0 otherwise) and third goal position\n",
        "          (1 if goal, 0 otherwise)\n",
        "        * ObservationType.AGENT_GOAL_POS: float32 tuple with \n",
        "          (agent_y, agent_x, goal_y, goal_x).\n",
        "      max_episode_length: If set, will terminate an episode after this many \n",
        "        steps.\n",
        "  \"\"\"\n",
        "  tasks_specifications = {\n",
        "      'simple': {\n",
        "          'layout': [\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "          ],\n",
        "          'start_state': (2, 2),\n",
        "          'goal_state': (7, 2)\n",
        "      },\n",
        "      'obstacle': {\n",
        "          'layout': [\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, -1, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "          ],\n",
        "          'start_state': (2, 2),\n",
        "          'goal_state': (2, 8)\n",
        "      },\n",
        "      'random_goal': {\n",
        "          'layout': [\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "          ],\n",
        "          'start_state': (2, 2),\n",
        "          # 'randomize_goals': True\n",
        "      },\n",
        "  }\n",
        "  return GridWorld(\n",
        "      discount=discount,\n",
        "      penalty_for_walls=penalty_for_walls,\n",
        "      observation_type=observation_type,\n",
        "      max_episode_length=max_episode_length,\n",
        "      **tasks_specifications[task])\n",
        "\n",
        "\n",
        "def setup_environment(environment):\n",
        "  # Make sure the environment outputs single-precision floats.\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "\n",
        "  # Grab the spec of the environment.\n",
        "  environment_spec = specs.make_environment_spec(environment)\n",
        "\n",
        "  return environment, environment_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZizdE9SQS-cN"
      },
      "source": [
        "\n",
        "We will use two distinct tabular GridWorlds:\n",
        "* `simple` where the goal is at the bottom left of the grid, little navigation required.\n",
        "* `obstacle` where the goal is behind an obstacle to avoid.\n",
        "\n",
        "You can visualize the grid worlds by running the cell below. \n",
        "\n",
        "Note that `S` indicates the start state and `G` indicates the goal. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xdnh3Odc63Q",
        "cellView": "form"
      },
      "source": [
        "# @title Visualise gridworlds { form-width: \"30%\" }\n",
        "\n",
        "# Instantiate two tabular environments, a simple task, and one that involves\n",
        "# the avoidance of an obstacle.\n",
        "simple_grid = build_gridworld_task(\n",
        "    task='simple', observation_type=ObservationType.GRID)\n",
        "obstacle_grid = build_gridworld_task(\n",
        "    task='obstacle', observation_type=ObservationType.GRID)\n",
        "\n",
        "# Plot them.\n",
        "simple_grid.plot_grid()\n",
        "plt.title('Simple')\n",
        "\n",
        "obstacle_grid.plot_grid()\n",
        "plt.title('Obstacle');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTsiWgDSCL7C"
      },
      "source": [
        "\n",
        "In this environment, the agent has four possible  <font color='blue'>**Actions**</font>: `up`, `right`, `down`, and `left`.  <font color='green'>**Reward**</font> is `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise. The episode ends when the agent reaches the goal, and otherwise continues. **Discount** on continuing steps, is $\\gamma = 0.9$. \n",
        "\n",
        "Before we start building an agent to interact with this environment, let's first look at the types of objects the environment either returns (e.g. observations) or consumes (e.g. actions). The `environment_spec` will show you the form of the *observations*, *rewards* and *discounts* that the environment exposes and the form of the *actions* that can be taken.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmKop4FECVV6"
      },
      "source": [
        "environment, environment_spec = setup_environment(simple_grid)\n",
        "\n",
        "print('actions:\\n', environment_spec.actions, '\\n')\n",
        "print('observations:\\n', environment_spec.observations, '\\n')\n",
        "print('rewards:\\n', environment_spec.rewards, '\\n')\n",
        "print('discounts:\\n', environment_spec.discounts, '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VVTmep2UK6U"
      },
      "source": [
        "\n",
        "We first set the environment to its initial location by calling the `reset` method which returns the first observation. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHden9m9FNPK"
      },
      "source": [
        "environment.reset()\n",
        "environment.plot_state()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXb7u9epFWnX"
      },
      "source": [
        "Now we want to take an action using the `step` method to move right:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY1eopIWFe95"
      },
      "source": [
        "timestep = environment.step(1)\n",
        "environment.plot_state()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOla1efZ4WX8"
      },
      "source": [
        "Apart from run_loop that will be used in training, evaluation loop will be used to visualise and evaluate our agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OL28CxC4-gSy"
      },
      "source": [
        "#@title Evaluation loop { form-width: \"30%\" }\n",
        "\n",
        "def evaluate(environment, agent, evaluation_episodes):\n",
        "  frames = []\n",
        "\n",
        "  for episode in range(evaluation_episodes):\n",
        "    timestep = environment.reset()\n",
        "    episode_return = 0\n",
        "    steps = 0\n",
        "    while not timestep.last():\n",
        "      frames.append(environment.plot_state(return_rgb=True))\n",
        "\n",
        "      action = agent.select_action(timestep.observation)\n",
        "      timestep = environment.step(action)\n",
        "      steps += 1\n",
        "      episode_return += timestep.reward\n",
        "    print(\n",
        "        f'Episode {episode} ended with reward {episode_return} in {steps} steps'\n",
        "    )\n",
        "  return frames\n",
        "\n",
        "def display_video(frames, filename='temp.mp4', frame_repeat=1):\n",
        "  \"\"\"Save and display video.\"\"\"\n",
        "  # Write video\n",
        "  with imageio.get_writer(filename, fps=60) as video:\n",
        "    for frame in frames:\n",
        "      for _ in range(frame_repeat):\n",
        "        video.append_data(frame)\n",
        "  # Read video and display the video\n",
        "  video = open(filename, 'rb').read()\n",
        "  b64_video = base64.b64encode(video)\n",
        "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
        "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
        "  return IPython.display.HTML(video_tag)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxjzoRO03jGH"
      },
      "source": [
        "#@title Visualise random agent's behaviour { form-width: \"30%\" }\n",
        "\n",
        "frames = evaluate(\n",
        "    environment,\n",
        "    RandomAgent(num_actions=4),\n",
        "    evaluation_episodes=1)\n",
        "display_video(frames, frame_repeat=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhHsnLcFID1u"
      },
      "source": [
        "## 1.2: Policy iteration\n",
        "\n",
        "The first RL learning algorithm we will explore is **policy iteration**, which is repeating (1) Policy Evaluation and (2) Greedy Improvement until convergence.\n",
        "\n",
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/policy_iteration.png?raw=true)\n",
        "\n",
        "For this exercise, we'll show you how to implement the \"first 2 arrows\", we will not repeat these steps to convergence yet.\n",
        "\n",
        "### 1. Policy Evaluation \n",
        "\n",
        "The purpose here is to evaluate a given policy $\\pi_e$. To make things interesting we will use **off-policy** learning, i.e. we will evaluate the **evaluation policy** $\\pi_e$, the policy we are interested in, by following a **behaviour policy** $\\pi_b$.\n",
        "\n",
        "\n",
        "Remember that evaluating a policy means to compute the value function associated with following/employing this policy in a given MDP (in other words, to compute how much total future discounted reward we expect to get by being in state $\\color{red}{s}$ and choosing action $\\color{blue}{a}$ and then following this policy).\n",
        "\n",
        "$$ Q^{\\pi_e}(\\color{red}{s},\\color{blue}{a}) = \\mathbb{E}_{\\tau \\sim P^{\\pi_e}} \\left[ \\sum_t \\gamma^t \\color{green}{R_t}| s_0=\\color{red}s,a=\\color{blue}{a_0} \\right]$$\n",
        "\n",
        "where $\\tau = \\{\\color{red}{s_0}, \\color{blue}{a_0}, \\color{green}{r_0}, \\color{red}{s_1}, \\color{blue}{a_1}, \\color{green}{r_1}, \\cdots \\}$.\n",
        "\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "**Initialize** $Q(\\color{red}{s}, \\color{blue}{a})$ for all $\\color{red}{s}$ ∈ $\\mathcal{\\color{red}S}$ and $\\color{blue}a$ ∈ $\\mathcal{\\color{blue}A}(\\color{red}s)$\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $\\color{red}{s} \\gets{}$current (nonterminal) state, either the initial state or $\\color{red}{s'}$ from the previous timestep\n",
        " \n",
        "2. $\\color{blue}{a} \\gets{} \\text{behaviour_policy }\\pi_b(\\color{red}s)$\n",
        " \n",
        "3. Take action $\\color{blue}{a}$; observe resulting reward $\\color{green}{r}$, discount $\\gamma$, and state, $\\color{red}{s'}$\n",
        "\n",
        "4. Compute TD-error: $\\delta = \\color{green}R + \\gamma Q(\\color{red}{s'}, \\underbrace{\\pi_e(\\color{red}{s'}}_{\\color{blue}{a'}})) − Q(\\color{red}s, \\color{blue}a)$\n",
        "\n",
        "4. Update Q-value with a small $\\alpha$ step: $Q(\\color{red}s, \\color{blue}a) \\gets Q(\\color{red}s, \\color{blue}a) + \\alpha \\delta$\n",
        "\n",
        "### 2. Greedy Policy Improvement\n",
        "\n",
        "Once a good approximation to the Q-value of a policy is obtained, we can improve this policy by simply changing action selection towards those that are evaluated higher. \n",
        "\n",
        "$$ \\pi_{greedy} (\\color{blue}a|\\color{red}s) = \\arg\\max_\\color{blue}a Q^{\\pi_e}(\\color{red}s,\\color{blue}a) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqrSos8dDPFb"
      },
      "source": [
        "### Create a policy evaluation agent\n",
        "\n",
        "An ACME `Actor` is the part of our framework that directly interacts with an environment by generating actions. Here we borrow a figure from Acme to show how this interaction occurs:\n",
        "\n",
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/acme_actor.png?raw=true)\n",
        "\n",
        "Tabular agents implement a function `q_values()` returning a matrix of Q values\n",
        "of shape: (`number_of_states`, `number_of_actions`)\n",
        "\n",
        "In this section, we will implement a `PolicyEvalAgent` as an ACME actor: given an `evaluation_policy` and a `behaviour_policy`, it will use the `behaviour_policy` to choose actions, and it will use the corresponding trajectory data to evaluate the `evaluation_policy` (i.e. compute the Q-values as if you were following the `evaluation_policy`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGtP3XRLF3qE"
      },
      "source": [
        "#@title **[Coding Task]** Policy Evaluation Agent { form-width: \"30%\" }\n",
        "\n",
        "class PolicyEvalAgent(acme.Actor):\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      number_of_states, \n",
        "      number_of_actions,\n",
        "      evaluated_policy, \n",
        "      behaviour_policy, \n",
        "      step_size=0.1):\n",
        "    self._state = None\n",
        "    self._number_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._evaluated_policy = evaluated_policy\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # initialize your q-values (this is a table of state and action pairs\n",
        "    # Note: this can be random, but the code was tested w/ zero-initialization \n",
        "    pass\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    pass\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    pass\n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    self._state = timestep.observation\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    s = self._state\n",
        "    a = action\n",
        "    r = next_timestep.reward\n",
        "    g = next_timestep.discount\n",
        "    next_s = next_timestep.observation\n",
        "\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Compute TD-Error.\n",
        "    # self._td_error =\n",
        "    pass\n",
        "\n",
        "  def update(self):    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # - Q-value table update\n",
        "    # - self._state update.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA8FRfY-Dsth",
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions for visualisation  { form-width: \"30%\" }\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_state_value(action_values, epsilon=0.1):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "def plot_action_values(action_values, epsilon=0.1):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  dif = vmax - vmin\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    \n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "      \n",
        "\n",
        "def smooth(x, window=10):\n",
        "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)\n",
        "  \n",
        "def plot_stats(stats, window=10):\n",
        "  plt.figure(figsize=(16,4))\n",
        "  plt.subplot(121)\n",
        "  xline = range(0, len(stats.episode_lengths), window)\n",
        "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
        "  plt.ylabel('Episode Length')\n",
        "  plt.xlabel('Episode Count')\n",
        "  plt.subplot(122)\n",
        "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
        "  plt.ylabel('Episode Return')\n",
        "  plt.xlabel('Episode Count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VL6pTUmZxyY"
      },
      "source": [
        "We will first see how this works on the `simple` GridWorld task.\n",
        "\n",
        "**Task 1**: Run the policy evaluation agent on the `simple` task. Evaluating the policy of always picking to go \"down\" (evaluation policy) by following uniformly random policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKyApFjfZo7U"
      },
      "source": [
        "# Policy of always choosing the action \"down\"\n",
        "def going_down_policy(q):\n",
        "  return np.array(2)\n",
        "\n",
        "# Uniform random policy over GridWorld actions\n",
        "def random_policy(q):\n",
        "  return np.random.randint(4)\n",
        "\n",
        "evaluated_policy = going_down_policy\n",
        "behaviour_policy = random_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5MI6AUPBba5x"
      },
      "source": [
        "# @title Visualize evaluated policy on `simple` { form-width: \"30%\" }\n",
        "\n",
        "grid = build_gridworld_task(\n",
        "    task='simple',\n",
        "    observation_type=ObservationType.STATE_INDEX,\n",
        "    max_episode_length=200)\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "evaluated_pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
        "for i in range(grid._layout_dims[0]):\n",
        "  for j in range(grid._layout_dims[1]):\n",
        "    evaluated_pi[i, j] = evaluated_policy(np.zeros((4,)))\n",
        "\n",
        "grid.plot_policy(evaluated_pi)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "x1UB4MMsb98c"
      },
      "source": [
        "# @title Visualize behaviour policy on `simple` { form-width: \"30%\" }\n",
        "\n",
        "grid = build_gridworld_task(\n",
        "    task='simple',\n",
        "    observation_type=ObservationType.STATE_INDEX,\n",
        "    max_episode_length=200)\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "behaviour_pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
        "for i in range(grid._layout_dims[0]):\n",
        "  for j in range(grid._layout_dims[1]):\n",
        "    behaviour_pi[i, j] = behaviour_policy(np.zeros((4,)))\n",
        "\n",
        "grid.plot_policy(behaviour_pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsuM-9sEKeT1"
      },
      "source": [
        "\n",
        "\n",
        "Try different number of training steps, e.g. $\\texttt{num_steps} = 1e3, 1e5$. \n",
        "\n",
        "Visualise the resulting value functions $Q(\\color{red}s,\\color{blue}a)$.\n",
        "The plotting function is provided for you and it takes in a table of q-values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMumNsJIKhn_",
        "cellView": "form"
      },
      "source": [
        "num_steps = 1e3  #@param {type:\"number\"}\n",
        "\n",
        "# environment\n",
        "grid = build_gridworld_task(task='simple')\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "# agent \n",
        "agent = PolicyEvalAgent(\n",
        "    number_of_states=environment_spec.observations.num_values, \n",
        "    number_of_actions=environment_spec.actions.num_values, \n",
        "    evaluated_policy=evaluated_policy,\n",
        "    behaviour_policy=behaviour_policy,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "returns, _ = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
        "\n",
        "# get the q-values\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q, epsilon=1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EfsRbSUN0B1"
      },
      "source": [
        "### Greedy Policy Improvement\n",
        "\n",
        "**Task 2**: Compute and Visualise the greedy policy based on the above evaluation, at the end of training.\n",
        "\n",
        "\n",
        "$$ \\pi_{greedy} (\\color{blue}a|\\color{red}s) = \\arg\\max_\\color{blue}a Q^{\\pi_e}(\\color{red}s,\\color{blue}a) $$\n",
        "\n",
        "**Q:** What do you observe? How does it compare to the behaviour policy we started from?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQFPI_0c1YXo"
      },
      "source": [
        "# @title **[Coding task]** Greedy policy\n",
        "def greedy(q_values):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCcnLnvLOBYZ",
        "cellView": "form"
      },
      "source": [
        "# @title Visualize the improved greedy policy on `simple` { form-width: \"30%\" }\n",
        "\n",
        "# Do here whatever works for you, but you should be able to see what the agent\n",
        "# would do at each step/state.\n",
        "\n",
        "pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
        "for i in range(grid._layout_dims[0]):\n",
        "  for j in range(grid._layout_dims[1]):\n",
        "    pi[i, j] = greedy(q[i, j])\n",
        "    \n",
        "\n",
        "grid = build_gridworld_task(\n",
        "    task='simple',\n",
        "    observation_type=ObservationType.STATE_INDEX,\n",
        "    max_episode_length=200)\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "grid.plot_policy(pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3fDpu9eOUm0"
      },
      "source": [
        "**Task 3**: Now try to evaluate **random policy** by following random policy on the harder `obstacle` task. Visualise the resulting value functions and the greedy policy on top of these values at the end of training.\n",
        "\n",
        "**Q:** What do you observe? \n",
        "- How does this policy compare with the optimal one?\n",
        "- Try running the training process longer -- what do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIpztRsrcYsV"
      },
      "source": [
        "evaluated_policy = random_policy\n",
        "behaviour_policy = random_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4dbI0DLOeqC",
        "cellView": "form"
      },
      "source": [
        "num_steps = 1e5 #@param {type:\"number\"}\n",
        "\n",
        "# environment\n",
        "grid = build_gridworld_task(task='obstacle')\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "# agent \n",
        "agent = PolicyEvalAgent(\n",
        "    number_of_states=environment_spec.observations.num_values, \n",
        "    number_of_actions=environment_spec.actions.num_values, \n",
        "    evaluated_policy=evaluated_policy,\n",
        "    behaviour_policy=behaviour_policy,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "returns, _ = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
        "\n",
        "# get the q-values\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q, epsilon=1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4KxWhW8PCD9",
        "cellView": "form"
      },
      "source": [
        "# @title Visualise the greedy policy on `obstacle` { form-width: \"30%\" }\n",
        "grid.plot_greedy_policy(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-L6y_XYcohm"
      },
      "source": [
        "**[Homework]** Feel free to play with different evaluation and behaviour policies and potentially perform multiple (evaluation, optimisation) steps to get closer to the optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcrhrNnIr3kX"
      },
      "source": [
        "## 1.3: On-policy control: SARSA Agent\n",
        "In this section, we are focusing on control RL algorithms, which perform the evaluation and improvement of the policy synchronously. That is, the policy that is being evaluated improves as the agent is using it to interact with the environment.\n",
        "\n",
        "\n",
        "The first algorithm we are going to be looking at is SARSA. This is an **on-policy algorithm** -- i.e: the data collection is done by leveraging the policy we're trying to optimize (and not just another fixed behaviour policy). \n",
        "\n",
        "As discussed during lectures, a greedy policy with respect to a given estimate of $Q^\\pi$ fails to explore the environment as needed; we will use instead an $\\epsilon$-greedy policy WRT $Q^\\pi$.\n",
        "\n",
        "### SARSA Algorithm\n",
        "\n",
        "\n",
        "**Initialize** $Q(\\color{red}{s}, \\color{blue}{a})$ for all $\\color{red}{s}$ ∈ $\\mathcal{\\color{red}S}$ and $\\color{blue}a$ ∈ $\\mathcal{\\color{blue}A}(\\color{red}s)$\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $\\color{red}s \\gets{}$current (nonterminal) state, either the initial state or $\\color{red}{s'}$ from the previous timestep\n",
        " \n",
        "2. $\\color{blue}a \\gets{} \\text{epsilon_greedy}(Q(\\color{red}s, \\cdot))$\n",
        " \n",
        "3. Take action $\\color{blue}a$; observe resultant reward $\\color{green}r$, discount $\\gamma$, and state, $\\color{red}{s'}$\n",
        "\n",
        "4. $Q(\\color{red}s, \\color{blue}a) \\gets Q(\\color{red}s, \\color{blue}a) + \\alpha (\\color{green}r + \\gamma Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}s, \\color{blue}a))$, where $\\color{blue}{a'} \\gets{} \\text{epsilon_greedy}(Q(\\color{red}{s'}, \\cdot))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNfVHzosN2P0"
      },
      "source": [
        "# @title **[Coding Task]** Epsilon-greedy policy { form-width: \"30%\" }\n",
        "# Input(s): Q(s,:), epsilon\n",
        "# Output:   Sampled action based on epsilon-Greedy(Q(s,:))\n",
        "def epsilon_greedy(q_values, epsilon=0.1):\n",
        "  pass\n",
        "  #return the epsilon greedy action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bmAV4Kcr7Zz"
      },
      "source": [
        "#@title **[Coding Task]** SARSA Agent  { form-width: \"30%\" }\n",
        "class SarsaAgent(acme.Actor):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, epsilon, step_size=0.1): \n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._step_size = step_size\n",
        "    self._epsilon = epsilon\n",
        "    self._state = None\n",
        "    self._action = None\n",
        "    self._next_state = None\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    return epsilon_greedy(self._q[observation], self._epsilon)\n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    self._state = timestep.observation\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    s = self._state\n",
        "    a = action\n",
        "    r = next_timestep.reward\n",
        "    g = next_timestep.discount\n",
        "    next_s = next_timestep.observation\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Online Q-value update\n",
        "    # self._td_error =\n",
        "    pass\n",
        "\n",
        "  def update(self):\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # - Q-value table update\n",
        "    # - self._state update.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8eBOcXZu1fM"
      },
      "source": [
        "### **Task**: Run your SARSA agent on the `obstacle` environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKYEB2d2uGaa",
        "cellView": "form"
      },
      "source": [
        "num_steps = 1e5 #@param {type:\"number\"}\n",
        "\n",
        "# environment\n",
        "grid = build_gridworld_task(task='obstacle')\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "# agent \n",
        "agent = SarsaAgent(\n",
        "    number_of_states=environment_spec.observations.num_values, \n",
        "    number_of_actions=environment_spec.actions.num_values, \n",
        "    epsilon=0.1,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "returns, _ = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
        "\n",
        "# get the q-values\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q, epsilon=1.)\n",
        "\n",
        "# visualise the greedy policy\n",
        "grid.plot_greedy_policy(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR3puDoRmHbx"
      },
      "source": [
        "This version of SARSA doesn't converge on optimal policy. Why? \n",
        "\n",
        "Hint (and **[Homework]**): implement SARSA with a decaying $\\epsilon$ schedule  $$\\epsilon \\gets{1/\\text{num_episodes}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFGX_zGcvb8D"
      },
      "source": [
        "## 1.4: Off-policy control: Q-learning Agent\n",
        "\n",
        "Reminder: Q-learning is a very powerful and general algorithm, that enables control (figuring out the optimal policy/value function) both on and off-policy.\n",
        "\n",
        "**Initialize** $Q(\\color{red}{s}, \\color{blue}{a})$ for all $\\color{red}{s} \\in \\color{red}{\\mathcal{S}}$ and $\\color{blue}{a} \\in \\color{blue}{\\mathcal{A}}(\\color{red}{s})$\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $\\color{red}{s} \\gets{}$current (nonterminal) state, either the initial state or $\\color{red}{s'}$ from the previous timestep\n",
        " \n",
        "2. $\\color{blue}{a} \\gets{} \\text{behaviour_policy}(\\color{red}{s})$\n",
        " \n",
        "3. Take action $\\color{blue}{a}$; observe resultant reward $\\color{green}{R}$, discount $\\gamma$, and state, $\\color{red}{s'}$\n",
        "\n",
        "4. $Q(\\color{red}{s}, \\color{blue}{a}) \\gets Q(\\color{red}{s}, \\color{blue}{a}) + \\alpha (\\color{green}{R} + \\gamma \\max_{\\color{blue}{a'}} Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}{s}, \\color{blue}{a}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6s820jAwoVA"
      },
      "source": [
        "#@title **[Coding Task]** Q-Learning Agent  { form-width: \"30%\" }\n",
        "class QLearningAgent(acme.Actor):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, behaviour_policy, step_size=0.1): \n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._state = None\n",
        "    self._action = None\n",
        "    self._next_state = None\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    return self._behaviour_policy(self._q[observation])\n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    self._state = timestep.observation\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    s = self._state\n",
        "    a = action\n",
        "    r = next_timestep.reward\n",
        "    g = next_timestep.discount\n",
        "    next_s = next_timestep.observation\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Offline Q-value update\n",
        "    # self._td_error =\n",
        "    pass\n",
        "\n",
        "  def update(self):\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # - Q-value table update\n",
        "    # - self._state update.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RqdV3rjwcAh"
      },
      "source": [
        "### **Task 1**: Run your Q-learning agent on `obstacle`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL4PgT-jwi3-",
        "cellView": "form"
      },
      "source": [
        "epsilon = 1  #@param {type:\"number\"} \n",
        "num_steps = 1e5  #@param {type:\"number\"}\n",
        "\n",
        "# environment\n",
        "grid = build_gridworld_task(task='obstacle')\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "# behavior policy\n",
        "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
        "\n",
        "# agent\n",
        "agent = QLearningAgent(\n",
        "    number_of_states=environment_spec.observations.num_values,\n",
        "    number_of_actions=environment_spec.actions.num_values,\n",
        "    behaviour_policy=behavior_policy,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "returns, _ = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
        "\n",
        "# get the q-values\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q, epsilon=epsilon)\n",
        "\n",
        "# visualise the greedy policy\n",
        "grid.plot_greedy_policy(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMk2ArG-weg_"
      },
      "source": [
        "### **Task 2:** Experiment with different levels of 'greediness'\n",
        "* The default was $\\epsilon=1.$, what does this correspond to?\n",
        "* Try also $\\epsilon =0.1, 0.5$. What do you observe? Does the behaviour policy affect the training in any way?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqg1n48y81ei"
      },
      "source": [
        "## 1.5: **[Homework]** Experience Replay\n",
        "\n",
        "Implement an agent that uses **Experience Replay** to learn action values, at each step:\n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning \n",
        "* apply multiple Q-learning updates based on transitions sampled from the *replay buffer* (in addition to the online updates).\n",
        "\n",
        "\n",
        "**Initialize** $Q(\\color{red}s, \\color{blue}a)$ for all $\\color{red}{s} ∈ \\mathcal{\\color{red}S}$ and $\\color{blue}a ∈ \\mathcal{\\color{blue}A}(\\color{red}s)$\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $\\color{red}{s} \\gets{}$current (nonterminal) state, either the initial state or $\\color{red}{s'}$ from the previous timestep\n",
        " \n",
        "2. $\\color{blue}{a} \\gets{} \\text{random_action}(\\color{red}{s})$\n",
        " \n",
        "3. Take action $\\color{blue}{a}$; observe resultant reward $\\color{green}{r}$, discount $\\gamma$, and state, $\\color{red}{s'}$\n",
        "\n",
        "4. $Q(\\color{red}{s}, \\color{blue}{a}) \\gets Q(\\color{red}{s}, \\color{blue}{a}) + \\alpha (\\color{green}{r} + \\gamma Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}{s}, \\color{blue}{a}))$\n",
        "\n",
        "5. $\\text{ReplayBuffer.append_transition}(s, a, r, \\gamma, s')$\n",
        "\n",
        "6. Loop repeat n times:\n",
        "\n",
        "  1. $\\color{red}{s}, \\color{blue}{a}, \\color{green}{r}, \\gamma, \\color{red}{s'} \\gets \\text{ReplayBuffer}.\\text{sample_transition}()$\n",
        "  \n",
        "  4. $Q(\\color{red}{s}, \\color{blue}{a}) \\gets Q(\\color{red}{s}, \\color{blue}{a}) + \\alpha (\\color{green}{r} + \\gamma \\max_\\color{blue}{a'} Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}{s}, \\color{blue}{a}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ietFnV739JwD",
        "cellView": "form"
      },
      "source": [
        "#@title **[Coding Task]** Q-learning AGENT with a simple replay buffer { form-width: \"30%\" }\n",
        "class ReplayQLearningAgent(acme.Actor):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, behaviour_policy, \n",
        "      num_offline_updates=0, step_size=0.1): \n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._num_offline_updates = num_offline_updates\n",
        "    self._state = None\n",
        "    self._action = None\n",
        "    self._next_state = None\n",
        "    self._replay_buffer = []\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    return self._behaviour_policy(self._q[observation])\n",
        "    \n",
        "  def observe_first(self, timestep):\n",
        "    self._state = timestep.observation\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    s = self._state\n",
        "    a = action\n",
        "    r = next_timestep.reward\n",
        "    g = next_timestep.discount\n",
        "    next_s = next_timestep.observation\n",
        "    \n",
        "    # Offline Q-value update\n",
        "    self._action = a\n",
        "    self._next_state = next_s\n",
        "    self._td_error = r + g * np.max(self._q[next_s]) - self._q[s, a]\n",
        "\n",
        "    if self._num_offline_updates > 0:\n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # Update replay buffer.\n",
        "      pass\n",
        "    \n",
        "  def update(self):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    self._q[s, a] += self._step_size * self._td_error\n",
        "    self._state = self._next_state\n",
        "\n",
        "    # Offline Q-value update\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J6CE2M_AdF"
      },
      "source": [
        "### **Task**: Compare Q-learning with/without experience replay\n",
        "\n",
        "Use a small number of training steps (e.g. `num_steps = 3e3`) and vary `num_offline_updates` between `0` and `30`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yLCXKBH_F0j",
        "cellView": "form"
      },
      "source": [
        "num_offline_updates = 0#@param {type:\"integer\"}\n",
        "num_steps = 3e3\n",
        "\n",
        "grid = build_gridworld_task(task='obstacle')\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "agent = ReplayQLearningAgent(\n",
        "    number_of_states=environment_spec.observations.num_values,\n",
        "    number_of_actions=environment_spec.actions.num_values,\n",
        "    behaviour_policy=random_policy,\n",
        "    num_offline_updates=num_offline_updates,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "returns, _ = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
        "\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)\n",
        "\n",
        "grid.plot_greedy_policy(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkn2ud_0Pn2o"
      },
      "source": [
        "# RL Lab - Part 2: Function Approximation with Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxqnvCLoe3KU"
      },
      "source": [
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/rl_loop_nn.png?raw=true)\n",
        "\n",
        "So far we only considered look-up tables. In all previous cases every state and action pair $(\\color{red}{s}, \\color{blue}{a})$, had an entry in our Q table. Again, this is possible in this environment as the number of states is a equal to the number of cells in the grid. But this is not scalable to situations where, say, the goal location changes or the obstacles are in different locations at every episode (consider how big the table should be in this situation?).\n",
        "\n",
        "As example (not covered in this tutorial) is ATARI from pixels, where the number of possible frames an agent can see is exponential in the number of pixels on the screen.\n",
        "\n",
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/atari.gif?raw=true)\n",
        "\n",
        "But what we **really** want is just being able to *compute* the Q-value, when fed with a particular $(\\color{red}{s}, \\color{blue}{a})$ pair. So if we had a way to get a function to do this work instead of keeping a big table, we'd get around this problem.\n",
        "\n",
        "To address this, we can use **Function Approximation** as a way to generalize Q-values over some representation of the very large state space, and **train** them to output the values they should.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjR8zkBdjIrB"
      },
      "source": [
        "\n",
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/atari_control.jpeg?raw=true)\n",
        "\n",
        "In this section, we will look at a deep RL Agent based on the following publication, [Playing Atari with Deep Reinforcement Learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning), which introduced the first deep learning model to successfully learn control policies directly from high-dimensional pixel inputs using RL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-omtUOQCS8VI"
      },
      "source": [
        "We represent $Q(\\color{red}s, \\color{blue}a)$ as a neural network $f()$. which given a vector $\\color{red}s$, will output a vector of Q-values for all possible actions $\\color{blue}a$.$^1$\n",
        "\n",
        "When introducing function approximations, and neural networks in particular, we need to have a loss to optimize. But looking back at the tabular setting above, you can see that we already have some notion of error: the **TD error**.\n",
        "\n",
        "By training our neural network to output values such that the *TD error is minimized*, we will also satisfy the Bellman Optimality Equation, which is a good sufficient condition to enforce, so that we may obtain an optimal policy.\n",
        "Thanks to automatic differentiation, we can just write the TD error as a loss (e.g. with a $L2$ loss, but others would work too), compute its gradient (which are now gradients with respect to individual parameters of the neural network) and slowly improve our Q-value approximation:\n",
        "\n",
        "$$Loss = \\mathbb{E}\\left[ \\left( \\color{green}{r} + \\gamma \\max_\\color{blue}{a'} Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}{s}, \\color{blue}{a})  \\right)^2\\right]$$\n",
        "\n",
        "\n",
        "If one were to update the Q-values online directly, the training can be unstable and very slow. Instead, DQN uses a Replay buffer, similar to what you just implemented above, to update the Q-value in a batched setting. The other important difference with tabular Q learning is that, in the formula above, the target generated by $\\color{green}{r} + \\gamma \\max_\\color{blue}{a'} Q(\\color{red}{s'}, \\color{blue}{a'})$ is not generated by the same parameters as $Q(\\color{red}{s}, \\color{blue}{a})$, namely $\\theta_i$. Instead, target parameters are used, $\\theta_i^{target}$, which are parameters that are periodically synced to $\\theta_i$. This is done for further optimization stability.\n",
        "\n",
        "The section will be structured as follows: firstly, we will build a DQN agent using the building blocks that ACME provides. After that, **[advanced]** we will implement the DQN learning loss.\n",
        "<br />\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<sub>*$^1$ we could feed it $\\color{blue}a$ as well and ask $f$ for a single scalar value, but given we have a fixed number of actions and we usually need to take an $argmax$ over them, it's easiest to just output them all in one pass.*</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NjO3wD-Sphk"
      },
      "source": [
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/acme_loop.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BukOfOsmtSQn"
      },
      "source": [
        " ## 2.1: Run an ACME DQN agent\n",
        "This section requires no coding, you can simply execute the cells. The code builds an puts together an out-of-the-box ACME DQN agent.\n",
        "The code should complete in reasonable time without connecting to a GPU/TPU but doing so may speed up training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbHdPc-nxO2j",
        "cellView": "form"
      },
      "source": [
        "#@title Create the `simple` gridworld environment\n",
        "grid = build_gridworld_task(\n",
        "    task='simple', \n",
        "    observation_type=ObservationType.GRID,\n",
        "    max_episode_length=200)\n",
        "environment, environment_spec = setup_environment(grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jcjk1w6oHVX",
        "cellView": "form"
      },
      "source": [
        "#@title Construct the agent and a training loop  { form-width: \"30%\" }\n",
        "\n",
        "# Build agent networks\n",
        "def network_fn(x):\n",
        "  model = hk.Sequential([\n",
        "      hk.Conv2D(32, kernel_shape=[4,4], stride=[2,2], padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(64, kernel_shape=[3,3], stride=[1,1], padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Flatten(),\n",
        "      hk.nets.MLP([50, 50, environment_spec.actions.num_values])\n",
        "  ])\n",
        "  return model(x)\n",
        "\n",
        "# Avoid logging from Acme\n",
        "class DummyLogger(object):\n",
        "\n",
        "  def write(self, data):\n",
        "    pass\n",
        "\n",
        "# Haiku-transform the network function and prepare network for ACME DQN API.\n",
        "network_hk = hk.without_apply_rng(hk.transform(network_fn))\n",
        "dummy_obs = acme_utils.add_batch_dim(\n",
        "        acme_utils.zeros_like(environment_spec.observations))\n",
        "network = networks_lib.FeedForwardNetwork(\n",
        "        lambda rng: network_hk.init(rng, dummy_obs), network_hk.apply)\n",
        "\n",
        "# Use library agent implementation.\n",
        "agent = dqn.DQN(\n",
        "    environment_spec=environment_spec,\n",
        "    network=network,\n",
        "    batch_size=10,\n",
        "    samples_per_insert=2,\n",
        "    epsilon=0.05,\n",
        "    min_replay_size=10,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dHDdPDr3QxI",
        "cellView": "form"
      },
      "source": [
        "# @title Run a training loop  { form-width: \"30%\" }\n",
        "\n",
        "# Run a `num_episodes` training episodes.\n",
        "# Rerun this cell until the agent has learned the given task.\n",
        "returns, _ = run_loop(\n",
        "    environment=environment,\n",
        "    agent=agent,\n",
        "    num_episodes=3000,\n",
        "    num_steps=3000000,\n",
        "    logger_time_delta=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksVITeN5_Vq",
        "cellView": "form"
      },
      "source": [
        "# @title Visualise the learned Q values { form-width: \"30%\" }\n",
        "\n",
        "# get agent parameters\n",
        "params = agent._learner.get_variables([])[0]\n",
        "\n",
        "# Evaluate the policy for every state, similar to tabular agents above.\n",
        "pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
        "q = np.zeros(grid._layout_dims + (4,))\n",
        "for y in range(grid._layout_dims[0]):\n",
        "  for x in range(grid._layout_dims[1]):\n",
        "    # Hack observation to see what the Q-network would output at that point.\n",
        "    environment.set_state(x, y)\n",
        "    obs = environment.get_obs()\n",
        "    q[y, x] = np.asarray(agent._learner.network.apply(params, np.expand_dims(obs, axis=0)))\n",
        "    pi[y, x] = np.asarray(agent.select_action(obs))\n",
        "    \n",
        "plot_action_values(q)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PQaQej4LsU-",
        "cellView": "form"
      },
      "source": [
        "#@title Compare the greedy policy with the agent's policy { form-width: \"30%\" }\n",
        "\n",
        "grid.plot_greedy_policy(q)\n",
        "plt.title('Greedy policy using the learnt Q-values')\n",
        "\n",
        "grid.plot_policy(pi)\n",
        "plt.title(\"Policy using the agent's policy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acqPbd8zXH_K"
      },
      "source": [
        "## 2.2: **[Advanced]** DQN Algorithm.\n",
        "\n",
        "The following coding exercise implements the loss function described in the DQN paper. This loss function is used by a learner class to compute gradients for the parameters $\\theta_i$ of the Q-network $Q( \\cdot; \\theta_i)$:\n",
        "\n",
        "```none\n",
        "loss(params: hk.Params, target_params: hk.Params, sample: reverb.ReplaySample)\n",
        "```\n",
        "which, at iteration `i` computes the DQN loss $L_i$ on the parameters $\\theta_i$, based on a the set of target parameters $\\theta_{i-1}$ and a given batch of sampled trajectories `sample`. As described in the manuscript, the loss function is defined as:\n",
        "\n",
        "$$L_i (\\theta_i) = \\mathbb{E}_{\\color{red}{s},\\color{blue}{a} \\sim \\rho(\\cdot)} \\left[ \\left( y_i - Q(\\color{red}{s},\\color{blue}{a} ;\\theta_i) \\right)^2\\right]$$\n",
        "\n",
        "where the target $y_i$ is computed using a bootstrap value computed from Q-value network with target parameters:\n",
        "\n",
        "$$ y_i = \\mathbb{E}_{\\color{red}{s'} \\sim \\mathcal{E}} \\left[ \\color{green}{r} + \\gamma \\max_{\\color{blue}{a'} \\in \\color{blue}{\\mathcal{A}}} Q(\\color{red}{s'}, \\color{blue}{a'} ; \\theta^{\\text{target}}_i) \\; | \\; \\color{red}{s}, \\color{blue}{a} \\right] $$\n",
        "\n",
        "The batch of data `sample` is prepackaged by the agent to match the sampling distributions $\\rho$ and $\\mathcal{E}$. To get the explicit data items, use the following:\n",
        "\n",
        "```none\n",
        "      o_tm1 = sample.data.observation\n",
        "      a_tm1 = sample.data.action\n",
        "      r_t = sample.data.reward\n",
        "      d_t = sample.data.discount\n",
        "      o_t = sample.data.next_observation\n",
        "```\n",
        "\n",
        "The function is expected to return  \n",
        "* `mean_loss` is the mean of the above loss over the batched data,\n",
        "* (`keys`, `priorities`) will pair the `keys` corresponding to each batch item to the absolute TD-error used to compute the `mean_loss` above. The agent uses these to update priorities for samples in the replay buffer.\n",
        "\n",
        "\n",
        "**Note**. A full implementation of a DQN agent is outside the scope of this tutorial, but we encourage you to explore the code (in a cell below) to understand where the learner fits with other the services used by the agent. Moreover, if you feel ambitious, we prepared a separate exercise where you are expected to implement the learner itself (see  *DQN Learner [Coding Task - Hard]*).\n",
        "\n",
        "\n",
        "\n",
        "**[Optional]**\n",
        "- use a Double-Q Learning Loss function instead of the original published loss (see [`rlax.double_q_learning`](https://github.com/deepmind/rlax/blob/870cba1ea8ad36725f4f3a790846298657b6fd4b/rlax/_src/value_learning.py#L233)) for more details.\n",
        "- for more stable optimization, use the Huber Loss instead of $L_2$, as prescribed in the manuscript (see [`rlax.huber_loss`](https://github.com/deepmind/rlax/blob/870cba1ea8ad36725f4f3a790846298657b6fd4b/rlax/_src/clipping.py#L31)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liZKoQbaj4Wq",
        "cellView": "form"
      },
      "source": [
        "# @title **[Coding Task - Easy]** DQN Loss function  { form-width: \"30%\" }\n",
        "\n",
        "TrainingState =  namedtuple('TrainingState', 'params, target_params, opt_state, step')\n",
        "LearnerOutputs =  namedtuple('LearnerOutputs', 'keys, priorities')\n",
        "\n",
        "class DQNLearner(acme.Learner):\n",
        "  \"\"\"DQN learner.\"\"\"\n",
        "\n",
        "  _state: TrainingState\n",
        "\n",
        "  def __init__(self,\n",
        "               network,\n",
        "               obs_spec,\n",
        "               discount,\n",
        "               importance_sampling_exponent,\n",
        "               target_update_period,\n",
        "               data_iterator,\n",
        "               optimizer,\n",
        "               rng,\n",
        "               replay_client,\n",
        "               max_abs_reward=1.,\n",
        "               huber_loss_parameter=1.,\n",
        "               ):\n",
        "    \"\"\"Initializes the learner.\"\"\"\n",
        "    def loss(params: hk.Params, target_params: hk.Params,\n",
        "             sample: reverb.ReplaySample):\n",
        "      o_tm1 = sample.data.observation\n",
        "      a_tm1 = sample.data.action\n",
        "      r_t = sample.data.reward\n",
        "      d_t = sample.data.discount\n",
        "      o_t = sample.data.next_observation\n",
        "      keys, probs = sample.info[:2]\n",
        "      \n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # return mean_loss, (keys, priorities)\n",
        "\n",
        "      pass\n",
        "      \n",
        "\n",
        "    def sgd_step(state, samples):\n",
        "      # Compute gradients on the given loss function and update the network\n",
        "      # using the optimizer provided at init time.\n",
        "      grad_fn = jax.grad(loss, has_aux=True)\n",
        "      gradients, (keys, priorities) = grad_fn(state.params, state.target_params,\n",
        "                                              samples)\n",
        "      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
        "      new_params = optix.apply_updates(state.params, updates)\n",
        "\n",
        "      # Update the internal state for the learner with (1) network parameters,\n",
        "      # (2) parameters of the target network, (3) the state of the optimizer,\n",
        "      # (4) Numbers of SGD steps performed by the agent.  \n",
        "      new_state = TrainingState(\n",
        "          params=new_params,\n",
        "          target_params=state.target_params,\n",
        "          opt_state=new_opt_state,\n",
        "          step=state.step + 1)\n",
        "\n",
        "      outputs = LearnerOutputs(keys=keys, priorities=priorities)\n",
        "      return new_state, outputs\n",
        "\n",
        "    # Internalise agent components (replay buffer, networks, optimizer).\n",
        "    self._replay_client = replay_client\n",
        "    self._iterator = data_iterator\n",
        "\n",
        "    # Since sampling is base on a priority experience replay, we need to pass\n",
        "    # the absolute td-loss values to the replay client to update priorities\n",
        "    # accordingly.\n",
        "    def update_priorities(outputs: LearnerOutputs):\n",
        "      for key, priority in zip(outputs.keys, outputs.priorities):\n",
        "        self._replay_client.mutate_priorities(\n",
        "            table='priority_table', \n",
        "            updates={int(key): float(priority)})\n",
        "        \n",
        "    self._update_priorities = update_priorities\n",
        "\n",
        "    # Internalise the hyperparameters.\n",
        "    self._target_update_period = target_update_period\n",
        "\n",
        "    # Internalise logging/counting objects.\n",
        "    self._counter = counting.Counter()\n",
        "    self._logger = loggers.TerminalLogger('learner', time_delta=1.)\n",
        "\n",
        "    # Initialise parameters and optimiser state.\n",
        "    def initialization_fn(values):\n",
        "      values = tree_util.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), values)\n",
        "      # Add batch dim.\n",
        "      return tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), values)\n",
        "\n",
        "    initial_params = network.init(next(rng))\n",
        "    initial_target_params = network.init(next(rng))\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "\n",
        "    self._state = TrainingState(\n",
        "        params=initial_params,\n",
        "        target_params=initial_target_params,\n",
        "        opt_state=initial_opt_state,\n",
        "        step=0)\n",
        "\n",
        "    self._forward = jax.jit(network.apply)\n",
        "    self._sgd_step = jax.jit(sgd_step)\n",
        "    \n",
        "  def step(self):\n",
        "    samples = next(self._iterator)\n",
        "    # Do a batch of SGD.\n",
        "    self._state, outputs = self._sgd_step(self._state, samples)\n",
        "\n",
        "    # Update our counts and record it.\n",
        "    result = self._counter.increment(steps=1)\n",
        "\n",
        "    # Periodically update target network parameters.\n",
        "    if self._state.step % self._target_update_period == 0:\n",
        "      self._state = self._state._replace(target_params=self._state.params)\n",
        "\n",
        "    # Update priorities in replay.\n",
        "    self._update_priorities(outputs)\n",
        "\n",
        "    # Write to logs.\n",
        "    self._logger.write(result)\n",
        "\n",
        "  def get_variables(self):\n",
        "    \"\"\"Network variables after a number of SGD steps.\"\"\"\n",
        "    return self._state.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbULeKGwURlL",
        "cellView": "form"
      },
      "source": [
        "# @title **[Coding Task - Hard]** DQN Learner  { form-width: \"30%\" }\n",
        "\n",
        "TrainingState =  namedtuple('TrainingState', 'params, target_params, opt_state, step')\n",
        "LearnerOutputs =  namedtuple('LearnerOutputs', 'keys, priorities')\n",
        "\n",
        "class DQNLearner(acme.Learner):\n",
        "  \"\"\"DQN learner.\"\"\"\n",
        "\n",
        "  _state: TrainingState\n",
        "\n",
        "  def __init__(self,\n",
        "               network,\n",
        "               obs_spec,\n",
        "               discount,\n",
        "               importance_sampling_exponent,\n",
        "               target_update_period,\n",
        "               data_iterator,\n",
        "               optimizer,\n",
        "               rng,\n",
        "               replay_client,\n",
        "               max_abs_reward=1.,\n",
        "               huber_loss_parameter=1.,\n",
        "               ):\n",
        "    \"\"\"Initializes the learner.\"\"\"\n",
        "\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Use provided params to initialize any jax functions used in the `step`\n",
        "    # function.\n",
        "    \n",
        "    # Internalise agent components (replay buffer, networks, optimizer).\n",
        "    self._replay_client = replay_client\n",
        "    self._iterator = data_iterator\n",
        "\n",
        "    # Since sampling is base on a priority experience replay, we need to pass\n",
        "    # the absolute td-loss values to the replay client to update priorities\n",
        "    # accordingly.\n",
        "    def update_priorities(outputs: LearnerOutputs):\n",
        "      for key, priority in zip(outputs.keys, outputs.priorities):\n",
        "        self._replay_client.mutate_priorities(\n",
        "            table='priority_table', \n",
        "            updates={int(key): float(priority)})\n",
        "        \n",
        "    self._update_priorities = update_priorities\n",
        "\n",
        "    # Internalise the hyperparameters.\n",
        "    self._target_update_period = target_update_period\n",
        "\n",
        "    # Internalise logging/counting objects.\n",
        "    self._counter = counting.Counter()\n",
        "    self._logger = loggers.TerminalLogger('learner', time_delta=1.)\n",
        "\n",
        "    # Initialise parameters and optimiser state.\n",
        "    def initialization_fn(values):\n",
        "      values = tree_util.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), values)\n",
        "      # Add batch dim.\n",
        "      return tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), values)\n",
        "\n",
        "    initial_params = network.init(next(rng))\n",
        "    initial_target_params = network.init(next(rng))\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "\n",
        "    self._state = TrainingState(\n",
        "        params=initial_params,\n",
        "        target_params=initial_target_params,\n",
        "        opt_state=initial_opt_state,\n",
        "        step=0)\n",
        "\n",
        "  def step(self):\n",
        "    samples = next(self._iterator)\n",
        "    \n",
        "    # Do a batch of SGD and update self._state accordingly.\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    \n",
        "    # Update our counts and record it.\n",
        "    result = self._counter.increment(steps=1)\n",
        "\n",
        "    # Periodically update target network parameters.\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    \n",
        "    # Update priorities in replay.\n",
        "    self._update_priorities(outputs)\n",
        "\n",
        "    # Write to logs.\n",
        "    self._logger.write(result)\n",
        "\n",
        "  def get_variables(self):\n",
        "    \"\"\"Network variables after a number of SGD steps.\"\"\"\n",
        "    return self._state.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZuigF_bD0DP",
        "cellView": "form"
      },
      "source": [
        "# @title **[Solution]** DQN Learner  { form-width: \"30%\" }\n",
        "\n",
        "TrainingState =  namedtuple('TrainingState', 'params, target_params, opt_state, step')\n",
        "LearnerOutputs =  namedtuple('LearnerOutputs', 'keys, priorities')\n",
        "\n",
        "class DQNLearner(acme.Learner):\n",
        "  \"\"\"DQN learner.\"\"\"\n",
        "\n",
        "  _state: TrainingState\n",
        "\n",
        "  def __init__(self,\n",
        "               network,\n",
        "               obs_spec,\n",
        "               discount,\n",
        "               importance_sampling_exponent,\n",
        "               target_update_period,\n",
        "               data_iterator,\n",
        "               optimizer,\n",
        "               rng,\n",
        "               replay_client,\n",
        "               max_abs_reward=1.,\n",
        "               huber_loss_parameter=1.,\n",
        "               ):\n",
        "    \"\"\"Initializes the learner.\"\"\"\n",
        "    def loss(params: hk.Params, target_params: hk.Params,\n",
        "             sample: reverb.ReplaySample):\n",
        "      o_tm1 = sample.data.observation\n",
        "      a_tm1 = sample.data.action\n",
        "      r_t = sample.data.reward\n",
        "      d_t = sample.data.discount\n",
        "      o_t = sample.data.next_observation\n",
        "      keys, probs = sample.info[:2]\n",
        "\n",
        "      # Forward pass.\n",
        "      q_tm1 = network.apply(params, o_tm1)\n",
        "      q_t_value = network.apply(target_params, o_t)\n",
        "      q_t_selector = network.apply(params, o_t)\n",
        "\n",
        "      # Cast and clip rewards.\n",
        "      d_t = (d_t * discount).astype(jnp.float32)\n",
        "      r_t = jnp.clip(r_t, -max_abs_reward, max_abs_reward).astype(jnp.float32)\n",
        "\n",
        "      # Compute double Q-learning n-step TD-error.\n",
        "      batch_error = jax.vmap(rlax.double_q_learning)\n",
        "      td_error = batch_error(q_tm1, a_tm1, r_t, d_t, q_t_value, q_t_selector)\n",
        "      batch_loss = rlax.huber_loss(td_error, huber_loss_parameter)\n",
        "\n",
        "      # Importance weighting.\n",
        "      importance_weights = (1. / probs).astype(jnp.float32)\n",
        "      importance_weights **= importance_sampling_exponent\n",
        "      importance_weights /= jnp.max(importance_weights)\n",
        "\n",
        "      # Reweight.\n",
        "      mean_loss = jnp.mean(importance_weights * batch_loss)  # []\n",
        "\n",
        "      priorities = jnp.abs(td_error).astype(jnp.float64)\n",
        "\n",
        "      return mean_loss, (keys, priorities)\n",
        "\n",
        "    def sgd_step(state, samples):\n",
        "      # Compute gradients on the given loss function and update the network\n",
        "      # using the optimizer provided at init time.\n",
        "      grad_fn = jax.grad(loss, has_aux=True)\n",
        "      gradients, (keys, priorities) = grad_fn(state.params, state.target_params,\n",
        "                                              samples)\n",
        "      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
        "      new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "      # Update the internal state for the learner with (1) network parameters,\n",
        "      # (2) parameters of the target network, (3) the state of the optimizer,\n",
        "      # (4) Numbers of SGD steps performed by the agent.  \n",
        "      new_state = TrainingState(\n",
        "          params=new_params,\n",
        "          target_params=state.target_params,\n",
        "          opt_state=new_opt_state,\n",
        "          step=state.step + 1)\n",
        "\n",
        "      outputs = LearnerOutputs(keys=keys, priorities=priorities)\n",
        "\n",
        "      return new_state, outputs\n",
        "\n",
        "    # Internalise agent components (replay buffer, networks, optimizer).\n",
        "    self._replay_client = replay_client\n",
        "    self._iterator = data_iterator\n",
        "\n",
        "    # Since sampling is base on a priority experience replay, we need to pass\n",
        "    # the absolute td-loss values to the replay client to update priorities\n",
        "    # accordingly.\n",
        "    def update_priorities(outputs: LearnerOutputs):\n",
        "      for key, priority in zip(outputs.keys, outputs.priorities):\n",
        "        self._replay_client.mutate_priorities(\n",
        "            table='priority_table', \n",
        "            updates={int(key): float(priority)})\n",
        "        \n",
        "    self._update_priorities = update_priorities\n",
        "\n",
        "    # Internalise the hyperparameters.\n",
        "    self._target_update_period = target_update_period\n",
        "\n",
        "    # Internalise logging/counting objects.\n",
        "    self._counter = counting.Counter()\n",
        "    self._logger = loggers.TerminalLogger('learner', time_delta=1.)\n",
        "\n",
        "    # Initialise parameters and optimiser state.\n",
        "    def initialization_fn(values):\n",
        "      values = tree_util.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), values)\n",
        "      # Add batch dim.\n",
        "      return tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), values)\n",
        "\n",
        "    initial_params = network.init(next(rng))\n",
        "    initial_target_params = network.init(next(rng))\n",
        "    initial_opt_state = optimizer.init(initial_params)\n",
        "\n",
        "    self._state = TrainingState(\n",
        "        params=initial_params,\n",
        "        target_params=initial_target_params,\n",
        "        opt_state=initial_opt_state,\n",
        "        step=0)\n",
        "\n",
        "    self._forward = jax.jit(network.apply)\n",
        "    self._sgd_step = jax.jit(sgd_step)\n",
        "    \n",
        "  def step(self):\n",
        "    samples = next(self._iterator)\n",
        "    # Do a batch of SGD.\n",
        "    self._state, outputs = self._sgd_step(self._state, samples)\n",
        "\n",
        "    # Update our counts and record it.\n",
        "    result = self._counter.increment(steps=1)\n",
        "\n",
        "    # Periodically update target network parameters.\n",
        "    if self._state.step % self._target_update_period == 0:\n",
        "      self._state = self._state._replace(target_params=self._state.params)\n",
        "\n",
        "    # Update priorities in replay.\n",
        "    self._update_priorities(outputs)\n",
        "\n",
        "    # Write to logs.\n",
        "    self._logger.write(result)\n",
        "\n",
        "  def get_variables(self):\n",
        "    \"\"\"Network variables after a number of SGD steps.\"\"\"\n",
        "    return self._state.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywObWtqgaSXx",
        "cellView": "form"
      },
      "source": [
        "# @title DQN Agent implementation (use for reference only) { form-width: \"30%\" }\n",
        "class DQN(acme.Actor):\n",
        "  def __init__(\n",
        "    self,\n",
        "    environment_spec,\n",
        "    network,\n",
        "    batch_size=256,\n",
        "    prefetch_size=4,\n",
        "    target_update_period=100,\n",
        "    samples_per_insert=32.0,\n",
        "    min_replay_size=1000,\n",
        "    max_replay_size=1000000,\n",
        "    importance_sampling_exponent=0.2,\n",
        "    priority_exponent=0.6,\n",
        "    n_step=5,\n",
        "    epsilon=0.,\n",
        "    learning_rate=1e-3,\n",
        "    discount=0.99,\n",
        "  ):\n",
        "    # Create a replay server to add data to. This is initialized as a\n",
        "    # table, and a Learner (defined separately) will be in charge of updating\n",
        "    # sample priorities based on the corresponding learner loss. \n",
        "    replay_table = reverb.Table(\n",
        "        name='priority_table',\n",
        "        sampler=reverb.selectors.Prioritized(priority_exponent),\n",
        "        remover=reverb.selectors.Fifo(),\n",
        "        max_size=max_replay_size,\n",
        "        rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "        signature=adders_reverb.NStepTransitionAdder.signature(\n",
        "                environment_spec))\n",
        "    self._server = reverb.Server([replay_table], port=None)\n",
        "    address = f'localhost:{self._server.port}'\n",
        "\n",
        "    # Use ACME reverb adder as a tool to add transition data into the replay\n",
        "    # buffer defined above.\n",
        "    self._adder = adders.NStepTransitionAdder(\n",
        "        client=reverb.Client(address),\n",
        "        n_step=n_step,\n",
        "        discount=discount)\n",
        "\n",
        "    # ACME datasets provides an interface to easily sample from a replay server.\n",
        "    dataset = datasets.make_reverb_dataset(\n",
        "        server_address=address,\n",
        "        batch_size=batch_size,\n",
        "        prefetch_size=prefetch_size,\n",
        "        transition_adder=True)\n",
        "    data_iterator = dataset.as_numpy_iterator()\n",
        "\n",
        "    # Create a learner that updates the parameters (and initializes them).\n",
        "    self._learner = DQNLearner(\n",
        "        network=network,\n",
        "        obs_spec=environment_spec.observations,\n",
        "        rng=hk.PRNGSequence(1),\n",
        "        optimizer=optax.adam(learning_rate),\n",
        "        discount=discount,\n",
        "        importance_sampling_exponent=importance_sampling_exponent,\n",
        "        target_update_period=target_update_period,\n",
        "        data_iterator=data_iterator,\n",
        "        replay_client=reverb.Client(address),\n",
        "    )\n",
        "    \n",
        "    # Create a feed forward actor that obtains its variables from the DQNLearner\n",
        "    # above.\n",
        "    def policy(params, key, observation):\n",
        "      action_values = network.apply(params, observation)\n",
        "      return rlax.epsilon_greedy(epsilon).sample(key, action_values)\n",
        "\n",
        "    self._policy = policy\n",
        "    self._rng = hk.PRNGSequence(1)\n",
        " \n",
        "    # We'll ignore the first min_observations when determining whether to take\n",
        "    # a step and we'll do so by making sure num_observations >= 0.\n",
        "    self._num_observations = -max(batch_size, min_replay_size)\n",
        "\n",
        "    observations_per_step = float(batch_size) / samples_per_insert\n",
        "    if observations_per_step >= 1.0:\n",
        "      self._observations_per_update = int(observations_per_step)\n",
        "      self._learning_steps_per_update = 1\n",
        "    else:\n",
        "      self._observations_per_update = 1\n",
        "      self._learning_steps_per_update = int(1.0 / observations_per_step)\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    observation = tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), \n",
        "                                     observation)\n",
        "    \n",
        "    key = next(self._rng)\n",
        "    params = self._learner.get_variables()\n",
        "    action = self._policy(params, key, observation)\n",
        "    action = tree_util.tree_map(lambda x: np.array(x).squeeze(axis=0), action)\n",
        "    return action \n",
        "\n",
        "  def observe_first(self, timestep):\n",
        "    self._adder.add_first(timestep)\n",
        "\n",
        "  def observe(self, action, next_timestep):\n",
        "    self._num_observations += 1\n",
        "    self._adder.add(action, next_timestep)\n",
        "\n",
        "  def update(self):\n",
        "    # Only allow updates after some minimum number of observations have been and\n",
        "    # then at some period given by observations_per_update.\n",
        "    if (self._num_observations >= 0 and\n",
        "        self._num_observations % self._observations_per_update == 0):\n",
        "      self._num_observations = 0\n",
        "\n",
        "      # Run a number of learner steps (usually gradient steps).\n",
        "      for _ in range(self._learning_steps_per_update):\n",
        "        self._learner.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iBoBqLvcy14",
        "cellView": "form"
      },
      "source": [
        "# @title Run a training loop  { form-width: \"30%\" }\n",
        "\n",
        "# Run a `num_episodes` training episodes.\n",
        "# Rerun this cell until the agent has learned the given task.\n",
        "\n",
        "grid = build_gridworld_task(\n",
        "    task='simple', \n",
        "    observation_type=ObservationType.GRID, \n",
        "    max_episode_length=100,\n",
        ")\n",
        "environment, environment_spec = setup_environment(grid)\n",
        "\n",
        "agent = DQN(\n",
        "    environment_spec=environment_spec,\n",
        "    network=network,\n",
        "    batch_size=16,\n",
        "    samples_per_insert=2,\n",
        "    epsilon=0.1,\n",
        "    min_replay_size=100)\n",
        "\n",
        "returns, _ = run_loop(\n",
        "    environment=environment,\n",
        "    agent=agent,\n",
        "    num_episodes=200, \n",
        "    logger_time_delta=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJi7LDrn0eO4"
      },
      "source": [
        "### DQN agent on the Gym Cartpole environment\n",
        "\n",
        "Here we show that you can apply what you learned to other environments such as Cartpole in [Gym](https://gym.openai.com/).\n",
        "\n",
        "![](https://github.com/m2lschool/tutorials2022/blob/main/assets/cartpole.gif?raw=true)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIERzZVk0xIh",
        "cellView": "form"
      },
      "source": [
        "#@title Construct the agent and run the training loop { form-width: \"30%\" }\n",
        "\n",
        "#  Try different parameters to see how learning is affected.\n",
        "\n",
        "env = gym_wrapper.GymWrapper(gym.make('CartPole-v0'))\n",
        "env = wrappers.SinglePrecisionWrapper(env)\n",
        "\n",
        "environment, environment_spec = setup_environment(env)\n",
        "\n",
        "# Build agent networks\n",
        "def network_fn(x):\n",
        "  model = hk.Sequential([\n",
        "      hk.Flatten(),\n",
        "      hk.nets.MLP([100, environment_spec.actions.num_values])\n",
        "  ])\n",
        "  return model(x)\n",
        "\n",
        "# Haiku-transform the network function and prepare network for ACME DQN API.\n",
        "network_hk = hk.without_apply_rng(hk.transform(network_fn))\n",
        "dummy_obs = acme_utils.add_batch_dim(\n",
        "        acme_utils.zeros_like(environment_spec.observations))\n",
        "network = networks_lib.FeedForwardNetwork(\n",
        "        lambda rng: network_hk.init(rng, dummy_obs), network_hk.apply)\n",
        "\n",
        "agent = dqn.DQN(\n",
        "    environment_spec=environment_spec,\n",
        "    network=network,\n",
        "    batch_size=64,\n",
        "    epsilon=0.01,\n",
        "    learning_rate=1e-3,\n",
        "    min_replay_size=100)\n",
        "\n",
        "returns, _ = run_loop(\n",
        "    environment=environment,\n",
        "    agent=agent,\n",
        "    num_episodes=150, \n",
        "    logger_time_delta=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDmLcICc98Z8",
        "cellView": "form"
      },
      "source": [
        "#@title Visualise training curve { form-width: \"30%\" }\n",
        "\n",
        "# Compute rolling average over returns\n",
        "returns_avg = pd.Series(returns).rolling(10, center=True).mean()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(len(returns)), returns_avg)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Total reward');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzqrYxAtH11S"
      },
      "source": [
        "# Want to learn more?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odBz1OO0JIXY"
      },
      "source": [
        "This Colab was inspired by the [EEML 2019 RL practical](https://github.com/eemlcommunity/PracticalSessions2019/blob/master/rl/RL_Tutorial.ipynb) and the [Acme tutorial](https://github.com/deepmind/acme/blob/master/examples/tutorial.ipynb). \n",
        "\n",
        "Books and lecture notes\n",
        "*   [Reinforcement Learning: an Introduction by Sutton & Barto](http://incompleteideas.net/book/RLbook2018.pdf)\n",
        "* [Algorithms for Reinforcement Learning by Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\n",
        "\n",
        "Lectures and course \n",
        "*   [RL Course by David Silver](https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-)\n",
        "*   [Reinforcement Learning Course | UCL & DeepMind](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb)\n",
        "*   [Emma Brunskill Stanford RL Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)\n",
        "*   [RL Course on Coursera by Martha White & Adam White](https://www.coursera.org/specializations/reinforcement-learning)\n",
        "\n",
        "More practical:\n",
        "* [Spinning Up in Deep RL by Josh Achiam](https://spinningup.openai.com/en/latest/)\n",
        "*   [Acme white paper](http://go/arxiv/2006.00979)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}