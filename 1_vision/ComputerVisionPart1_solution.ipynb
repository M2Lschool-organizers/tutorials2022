{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi1dtiTtK1Ov"
   },
   "source": [
    "## M2L: Computer Vision Tutorial (PART I)\n",
    "\n",
    "### by Luigi Celona and Flavio Piccoli\n",
    "\n",
    "Implement and train the Vision transformer proposed by [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) for classifying CIFAR10 images using supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmGVZkJUkmpS"
   },
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ho_PioVRL_o"
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "\n",
    "## tqdm for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
    "# import jax.tools.colab_tpu\n",
    "# jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from typing import (Any, Callable, Optional, Tuple)\n",
    "PRNGKey = Any\n",
    "Shape = Tuple[int, ...]\n",
    "Dtype = Any\n",
    "Array = Any\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../../saved_models/tutorial_part1\"\n",
    "\n",
    "# Seeding for random operations\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P65vVFkUbTu"
   },
   "source": [
    "## Download dataset to be used for training and testing\n",
    "* CIFAR10 dataset\n",
    "\n",
    "* 60,000 32x32 colour images in 10 classes, with 6000 images per class\n",
    "\n",
    "* train: 50,000; test: 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEEkhc5KRa8t"
   },
   "outputs": [],
   "source": [
    "# Transformations applied on each image => bring them into a numpy array\n",
    "DATA_MEANS = np.array([0.49139968, 0.48215841, 0.44653091])\n",
    "DATA_STD = np.array([0.24703223, 0.24348513, 0.26158784])\n",
    "\n",
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255. - DATA_MEANS) / DATA_STD\n",
    "    return img\n",
    "\n",
    "# We need to stack the batch elements\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "test_transform = image_to_numpy\n",
    "\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      image_to_numpy\n",
    "                                     ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for training and validation\n",
    "train_loader = data.DataLoader(train_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)\n",
    "val_loader   = data.DataLoader(val_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)\n",
    "test_loader  = data.DataLoader(test_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vMPjp0UU4Mx"
   },
   "source": [
    "## Display the images\n",
    "The gallery function below shows sample images from the data, together with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy0BWFwFUQ0J"
   },
   "outputs": [],
   "source": [
    "def gallery(dataset, num_images=5, title='Input images'):\n",
    "    classes = dataset.classes\n",
    "    j = 1\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "    resize = transforms.Resize(512,\n",
    "                               interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    for c, name in enumerate(classes):\n",
    "        i = 0\n",
    "        im2class = 0\n",
    "        while im2class < num_images:\n",
    "            image, target = dataset[i]\n",
    "            if target == c:\n",
    "                plt.subplot(len(classes), num_images, j)\n",
    "                image = (image * DATA_STD + DATA_MEANS)\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "                plt.imshow(resize(Image.fromarray(image)))\n",
    "                plt.axis('off')\n",
    "                j += 1\n",
    "                im2class += 1\n",
    "            i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kGaRa23RfjT"
   },
   "outputs": [],
   "source": [
    "gallery(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pgKO2uEU_tn"
   },
   "source": [
    "## Prepare the data for training and testing\n",
    "* We use PyTorch readers; JAX does not have support for input data reading and pre-processing\n",
    "* for training, we use stochastic optimizers (e.g. AdamX), so we need to sample at random mini-batches from the training dataset\n",
    "* for testing, we iterate sequentially through the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OalRJVrVR2up"
   },
   "source": [
    "## Transformers for image classification\n",
    "\n",
    "Transformers have been originally proposed to process sets since it is a permutation-equivariant architecture, i.e., producing the same output permuted if the input is permuted. To apply Transformers to sequences, we have simply added a positional encoding to the input feature vectors, and the model learned by itself what to do with it. So, why not do the same thing on images? This is exactly what [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) proposed in their paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". Specifically, the Vision Transformer is a model for image classification that views images as sequences of smaller patches. As a preprocessing step, we split an image of, for example, $48\\times 48$ pixels into 9 $16\\times 16$ patches. Each of those patches is considered to be a \"word\"/\"token\" and projected to a feature space. With adding positional encodings and a token for classification on top, we can apply a Transformer as usual to this sequence and start training it for our task. A nice GIF visualization of the architecture is shown below (figure credit - [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/m2lschool/tutorials2022/blob/main/assets/1_vision_vit.gif?raw=true\" width=\"600px\"></center>\n",
    "\n",
    "We will walk step by step through the Vision Transformer, and implement all parts by ourselves. First, let's implement the image preprocessing: an image of size $N\\times N$ has to be split into $(N/M)^2$ patches of size $M\\times M$. These represent the input words to the Transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_LaVpUHj_hW"
   },
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, H, W, C]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.reshape(B, H//patch_size, patch_size, W//patch_size, patch_size, C)\n",
    "    x = x.transpose(0, 1, 3, 2, 4, 5)    # [B, H', W', p_H, p_W, C]\n",
    "    x = x.reshape(B, -1, *x.shape[3:])   # [B, H'*W', p_H, p_W, C]\n",
    "    if flatten_channels:\n",
    "        x = x.reshape(B, x.shape[1], -1) # [B, H'*W', p_H*p_W*C]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkGQo_yfwffs"
   },
   "source": [
    "Let's take a look at how that works for our CIFAR examples above. For our images of size $32\\times 32$, we choose a patch size of 4. Hence, we obtain sequences of 64 patches of size $4\\times 4$. We visualize them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytZ8-PlJ-tyB"
   },
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "def numpy_to_torch(array):\n",
    "    array = jax.device_get(array)\n",
    "    tensor = torch.from_numpy(array)\n",
    "    tensor = tensor.permute(0, 3, 1, 2)\n",
    "    return tensor\n",
    "\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_images = np.stack([test_set[idx][0] for idx in range(NUM_IMAGES)], axis=0)\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(numpy_to_torch(CIFAR_images), \n",
    "                                       nrow=4, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(numpy_to_torch(img_patches[i]), \n",
    "                                           nrow=64, normalize=True, pad_value=0.9)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjSL6XF6wilp"
   },
   "source": [
    "Compared to the original images, it is much harder to recognize the objects from those patch lists now. Still, this is the input we provide to the Transformer for classifying the images. The model has to learn itself how it has to combine the patches to recognize the objects. The inductive bias in CNNs that an image is a grid of pixels, is lost in this input format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZma9QNBPVqL"
   },
   "source": [
    "## ViT model implementation\n",
    "After we have looked at the preprocessing, we can now start building the Transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibWcEXfRp4OG"
   },
   "source": [
    "### Scaled Dot Product Attention\n",
    "One of the key concepts behind Transformers is self-attention consisting of the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/m2lschool/tutorials2022/blob/main/assets/1_vision_scaled_dot_product.png?raw=true\" width=\"210px\"></center>\n",
    "\n",
    "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma^2$ results in a scalar having $d_k$-times higher variance: \n",
    "\n",
    "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n",
    "\n",
    "\n",
    "If we do not scale down the variance back to $\\sim\\sigma^2$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. Note that the extra factor of $\\sigma^2$, i.e., having $\\sigma^4$ instead of $\\sigma^2$, is usually not an issue, since we keep the original variance $\\sigma^2$ close to $1$ anyways.\n",
    "\n",
    "The block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value. \n",
    "\n",
    "After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brJvspw4p4OH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Computes dot-product attention given multi-headed query, key, and value.\n",
    "\n",
    "    Inputs:\n",
    "        q - queries for calculating attention with shape of \n",
    "            `[batch, length, heads, embed_dim]`.\n",
    "        k - keys for calculating attention with shape of \n",
    "            `[batch, length, heads, embed_dim]`.\n",
    "        v - values for calculating attention with shape of \n",
    "            `[batch, length, heads, embed_dim]`.\n",
    "        mask - mask for the attention logits. This should be broadcastable\n",
    "            to the shape of `[batch, heads, length, length].\n",
    "        \n",
    "    \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = jnp.matmul(jax.numpy.transpose(q, (0, 2, 1, 3)),\n",
    "                             jax.numpy.transpose(k, (0, 2, 3, 1))\n",
    "                            )\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
    "    attention = nn.softmax(attn_logits, axis=-1)\n",
    "    values = jnp.matmul(attention, jnp.swapaxes(v, 1, 2))\n",
    "    return jnp.swapaxes(values, 1, 2) # [b, h, l, d] -> [b, l, h, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pONy4al6iT6t"
   },
   "outputs": [],
   "source": [
    "## Test scaled_dot_product implementation\n",
    "# Example q,k,v as inputs\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "q = random.normal(x_rng, (3, 16, 4, 32))\n",
    "k = random.normal(x_rng, (3, 16, 4, 32))\n",
    "v = random.normal(x_rng, (3, 16, 4, 32))\n",
    "\n",
    "# scaled dot product\n",
    "scaled_dot_product(q, k, v).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdBlzpKap4OI"
   },
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
    "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We refer to this as Multi-Head Attention layer with the learnable parameters $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$, and $W^{O}\\in\\mathbb{R}^{h\\cdot d_k\\times d_{out}}$ ($D$ being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/m2lschool/tutorials2022/blob/main/assets/1_vision_multi_head_attention.png?raw=true\" width=\"400px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPpfkmrJ_kXw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadDotProductAttention(nn.Module):\n",
    "    # Number of attention heads. Features (i.e. x.shape[-1])\n",
    "    num_heads: int\n",
    "\n",
    "    # Initializer for the kernel of the Dense layers\n",
    "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.linear.default_kernel_init\n",
    "    \n",
    "    # Initializer for the bias of the Dense layers\n",
    "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
    "\n",
    "    # Methods wrapped in @compact can define submodules directly within the method.\n",
    "    # It is useful in this case because the dimensions of the feature map are not\n",
    "    # known a priori\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Applies multi-head dot product attention on the input data.\n",
    "\n",
    "        Projects the input into multi-headed query, key, and value vectors,\n",
    "        applies dot-product attention and project the results to an output vector.\n",
    "\n",
    "        Args:\n",
    "          x: input of shape `[batch_size, length, features]`.\n",
    "\n",
    "        Returns:\n",
    "            output of shape `[batch_size, length, features]`.\n",
    "        \"\"\"\n",
    "        features = x.shape[-1]\n",
    "        qkv_features = x.shape[-1]\n",
    "        assert qkv_features % self.num_heads == 0, (\n",
    "            'Memory dimension must be divisible by number of heads.')\n",
    "        head_dim = qkv_features // self.num_heads\n",
    "        \n",
    "        # project x to multi-headed q/k/v\n",
    "        # dimensions are then [batch_size, length, n_heads, n_features_per_head]\n",
    "        query = nn.linear.DenseGeneral(features=(self.num_heads, head_dim),\n",
    "                                       kernel_init=self.kernel_init,\n",
    "                                       bias_init=self.bias_init,\n",
    "                                       use_bias=True,\n",
    "                                       name='query')(x)\n",
    "        \n",
    "        key   = nn.linear.DenseGeneral(features=(self.num_heads, head_dim),\n",
    "                                       kernel_init=self.kernel_init,\n",
    "                                       bias_init=self.bias_init,\n",
    "                                       use_bias=True,\n",
    "                                       name='key')(x)\n",
    "\n",
    "        value = nn.linear.DenseGeneral(features=(self.num_heads, head_dim),\n",
    "                                       kernel_init=self.kernel_init,\n",
    "                                       bias_init=self.bias_init,\n",
    "                                       use_bias=True,\n",
    "                                       name='value')(x)\n",
    "        \n",
    "        # apply attention\n",
    "        x = scaled_dot_product(query, key, value)\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        out = nn.linear.DenseGeneral(features=features,\n",
    "                                     axis=(-2, -1),\n",
    "                                     kernel_init=self.kernel_init,\n",
    "                                     bias_init=self.bias_init,\n",
    "                                     use_bias=True,\n",
    "                                     name='out')(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVC3euN0_YgJ"
   },
   "source": [
    "Further, we use the Pre-Layer Normalization version of the Transformer blocks proposed by [Ruibin Xiong et al.](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf) in 2020. The idea is to apply Layer Normalization not in between residual blocks, but instead as a first layer in the residual blocks. This reorganization of the layers supports better gradient flow and removes the necessity of a warm-up stage. A visualization of the difference between the standard Post-LN and the Pre-LN version is shown below.\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/m2lschool/tutorials2022/blob/main/assets/1_vision_pre_layer_norm.svg?raw=true\" width=\"400px\"></center>\n",
    "\n",
    "The implementation of the Pre-LN attention block looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BxZchwC_Dv4"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    embed_dim : int   # Dimensionality of input and attention feature vectors\n",
    "    hidden_dim : int  # Dimensionality of hidden layer in feed-forward network \n",
    "    num_heads : int   # Number of heads to use in the Multi-Head Attention block\n",
    "    dropout_prob : float = 0.0  # Amount of dropout to apply in the feed-forward network\n",
    "    \n",
    "    def setup(self):\n",
    "        self.mha = MultiHeadDotProductAttention(num_heads=self.num_heads)\n",
    "        self.ffn = [\n",
    "            nn.Dense(self.hidden_dim),\n",
    "            nn.gelu,\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.Dense(self.embed_dim)\n",
    "        ]\n",
    "        self.layer_norm_1 = nn.LayerNorm()\n",
    "        self.layer_norm_2 = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        attn_out = self.mha(inp_x)\n",
    "        x = x + self.dropout(attn_out, deterministic=not train)\n",
    "        \n",
    "        linear_out = self.layer_norm_2(x)\n",
    "        for l in self.ffn:\n",
    "            linear_out = l(linear_out) if not isinstance(l, nn.Dropout) else l(linear_out, deterministic=not train)\n",
    "        x = x + self.dropout(linear_out, deterministic=not train)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUqeTU5RAkm_"
   },
   "outputs": [],
   "source": [
    "## Test AttentionBlock implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "# Create attention block\n",
    "attnblock = AttentionBlock(embed_dim=128, hidden_dim=512, num_heads=4, dropout_prob=0.1)\n",
    "# Initialize parameters of attention block with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = attnblock.init({'params': init_rng, 'dropout': dropout_init_rng}, x, True)['params']\n",
    "# Apply encoder block with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "out = attnblock.apply({'params': params}, x, train=True, rngs={'dropout': dropout_apply_rng})\n",
    "print('Out', out.shape)\n",
    "\n",
    "del attnblock, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkhQYRJVw0ZN"
   },
   "source": [
    "Now we have all modules ready to build our own Vision Transformer. Besides the Transformer encoder, we need the following modules:\n",
    "\n",
    "* A **linear projection** layer that maps the input patches to a feature vector of larger size. It is implemented by a simple linear layer that takes each $M\\times M$ patch independently as input.\n",
    "* A **classification token** that is added to the input sequence. We will use the output feature vector of the classification token (CLS token in short) for determining the classification prediction.\n",
    "* Learnable **positional encodings** that are added to the tokens before being processed by the Transformer. Those are needed to learn position-dependent information, and convert the set to a sequence. Since we usually work with a fixed resolution, we can learn the positional encodings instead of having the pattern of sine and cosine functions.\n",
    "* An **MLP head** that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually implemented by a small feed-forward network or even a single linear layer.\n",
    "\n",
    "With those components in mind, let's implement the full Vision Transformer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSYh1ejRkDFv"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    embed_dim : int     # Dimensionality of input and attention feature vectors\n",
    "    hidden_dim : int    # Dimensionality of hidden layer in feed-forward network \n",
    "    num_heads : int     # Number of heads to use in the Multi-Head Attention block\n",
    "    num_channels : int  # Number of channels of the input (3 for RGB)\n",
    "    num_layers : int    # Number of layers to use in the Transformer\n",
    "    num_classes : int   # Number of classes to predict\n",
    "    patch_size : int    # Number of pixels that the patches have per dimension\n",
    "    num_patches : int   # Maximum number of patches an image can have\n",
    "    dropout_prob : float = 0.0  # Amount of dropout to apply in the feed-forward network\n",
    "    \n",
    "    def setup(self):\n",
    "        # Layers/Networks\n",
    "        self.to_patch_embedding = nn.Dense(self.embed_dim)\n",
    "        self.transformer = [AttentionBlock(self.embed_dim, \n",
    "                                           self.hidden_dim, \n",
    "                                           self.num_heads, \n",
    "                                           self.dropout_prob) for _ in range(self.num_layers)]\n",
    "        self.mlp_head = nn.Sequential([\n",
    "            nn.LayerNorm(),\n",
    "            nn.Dense(self.num_classes)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = self.param('cls_token', \n",
    "                                    nn.initializers.normal(stddev=1.0), \n",
    "                                    (1, 1, self.embed_dim))\n",
    "        self.pos_embedding = self.param('pos_embedding', \n",
    "                                        nn.initializers.normal(stddev=1.0), \n",
    "                                        (1, 1+self.num_patches, self.embed_dim))\n",
    "    \n",
    "    \n",
    "    def __call__(self, x, train=True):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.to_patch_embedding(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, axis=0)\n",
    "        x = jnp.concatenate([cls_token, x], axis=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transformer\n",
    "        x = self.dropout(x, deterministic=not train)\n",
    "        for attn_block in self.transformer:\n",
    "            x = attn_block(x, train=train)\n",
    "        \n",
    "        # Perform classification prediction\n",
    "        cls = x[:,0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceXoZkkvkJKL"
   },
   "outputs": [],
   "source": [
    "## Test VisionTransformer implementation\n",
    "# Example features as input\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (5, 32, 32, 3))\n",
    "# Create vision transformer\n",
    "visntrans = VisionTransformer(embed_dim=128, \n",
    "                              hidden_dim=512, \n",
    "                              num_heads=4, \n",
    "                              num_channels=3, \n",
    "                              num_layers=6,\n",
    "                              num_classes=10, \n",
    "                              patch_size=4, \n",
    "                              num_patches=64,\n",
    "                              dropout_prob=0.1)\n",
    "# Initialize parameters of the Vision Transformer with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = visntrans.init({'params': init_rng, 'dropout': dropout_init_rng}, x, True)['params']\n",
    "# Apply encoder block with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "out = visntrans.apply({'params': params}, x, train=True, rngs={'dropout': dropout_apply_rng})\n",
    "print('Out', out.shape)\n",
    "\n",
    "del visntrans, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBMc90uFA1Aq"
   },
   "source": [
    "Finally, we can put everything into a trainer module. We use `optax.adamw` as the optimizer, which is Adam with a corrected weight decay implementation. Since we use the Pre-LN Transformer version, we do not need to use a learning rate warmup stage anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKw0Kk5QkNCD"
   },
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self, exmp_imgs, lr=1e-3, weight_decay=0.01, seed=42, **model_hparams):\n",
    "        \"\"\"\n",
    "        Module for summarizing all training functionalities for classification on CIFAR10.\n",
    "\n",
    "        Inputs:\n",
    "            exmp_imgs - Example imgs, used as input to initialize the model\n",
    "            lr - Learning rate of the optimizer to use\n",
    "            weight_decay - Weight decay to use in the optimizer\n",
    "            seed - Seed to use in the model initialization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.seed = seed\n",
    "        self.rng = random.PRNGKey(self.seed)\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = VisionTransformer(**model_hparams)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH, 'ViT/')\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "        # Create jitted training and eval functions\n",
    "        self.create_functions()\n",
    "        # Initialize model\n",
    "        self.init_model(exmp_imgs)\n",
    "\n",
    "    def create_functions(self):\n",
    "        # Function to calculate the classification loss and accuracy for a model\n",
    "        def calculate_loss(params, rng, batch, train):\n",
    "            imgs, labels = batch\n",
    "            rng, dropout_apply_rng = random.split(rng)\n",
    "            logits = self.model.apply({'params': params},\n",
    "                                      imgs,\n",
    "                                      train=train,\n",
    "                                      rngs={'dropout': dropout_apply_rng})\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            acc = (logits.argmax(axis=-1) == labels).mean()\n",
    "            return loss, (acc, rng)\n",
    "\n",
    "        # Training function\n",
    "        def train_step(state, rng, batch):\n",
    "            loss_fn = lambda params: calculate_loss(params, rng, batch, train=True)\n",
    "            # Get loss, gradients for loss, and other outputs of loss function\n",
    "            (loss, (acc, rng)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            # Update parameters and batch statistics\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, rng, loss, acc\n",
    "\n",
    "        # Eval function\n",
    "        def eval_step(state, rng, batch):\n",
    "            # Return the accuracy for a single batch\n",
    "            _, (acc, rng) = calculate_loss(state.params, rng, batch, train=False)\n",
    "            return rng, acc\n",
    "        # jit for efficiency\n",
    "        self.train_step = jax.jit(train_step)\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def init_model(self, exmp_imgs):\n",
    "        # Initialize model\n",
    "        self.rng, init_rng, dropout_init_rng = random.split(self.rng, 3)\n",
    "        self.init_params = self.model.init({'params': init_rng, 'dropout': dropout_init_rng}, \n",
    "                                           exmp_imgs, \n",
    "                                           train=True)['params']\n",
    "        self.state = None\n",
    "\n",
    "    def init_optimizer(self, num_epochs, num_steps_per_epoch):\n",
    "        # We decrease the learning rate by a factor of 0.1 after 60% and 85% of the training\n",
    "        lr_schedule = optax.piecewise_constant_schedule(\n",
    "            init_value=self.lr,\n",
    "            boundaries_and_scales=\n",
    "                {int(num_steps_per_epoch*num_epochs*0.6): 0.1,\n",
    "                 int(num_steps_per_epoch*num_epochs*0.85): 0.1}\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adamw(lr_schedule, weight_decay=self.weight_decay)\n",
    "        )\n",
    "\n",
    "        # Initialize training state\n",
    "        self.state = train_state.TrainState.create(\n",
    "                                       apply_fn=self.model.apply,\n",
    "                                       params=self.init_params if self.state is None else self.state.params,\n",
    "                                       tx=optimizer)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=200):\n",
    "        # Train model for defined number of epochs\n",
    "        # We first need to create optimizer and the scheduler for the given number of epochs\n",
    "        self.init_optimizer(num_epochs, len(train_loader))\n",
    "        # Track best eval accuracy\n",
    "        best_eval = 0.0\n",
    "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
    "            self.train_epoch(epoch=epoch_idx)\n",
    "            if epoch_idx % 2 == 0:\n",
    "                eval_acc = self.eval_model(val_loader)\n",
    "                self.logger.add_scalar('val/acc', eval_acc, global_step=epoch_idx)\n",
    "                if eval_acc >= best_eval:\n",
    "                    best_eval = eval_acc\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        metrics = defaultdict(list)\n",
    "        for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "            self.state, self.rng, loss, acc = self.train_step(self.state, self.rng, batch)\n",
    "            metrics['loss'].append(loss)\n",
    "            metrics['acc'].append(acc)\n",
    "        for key in metrics:\n",
    "            avg_val = np.stack(jax.device_get(metrics[key])).mean()\n",
    "            self.logger.add_scalar('train/'+key, avg_val, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "        # Test model on all images of a data loader and return avg loss\n",
    "        correct_class, count = 0, 0\n",
    "        for batch in data_loader:\n",
    "            self.rng, acc = self.eval_step(self.state, self.rng, batch)\n",
    "            correct_class += acc * batch[0].shape[0]\n",
    "            count += batch[0].shape[0]\n",
    "        eval_acc = (correct_class / count).item()\n",
    "        return eval_acc\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
    "                                    target=self.state.params,\n",
    "                                    step=step,\n",
    "                                    overwrite=True)\n",
    "\n",
    "    def load_model(self, pretrained=False):\n",
    "        # Load model. We use different checkpoint for pretrained models\n",
    "        if not pretrained:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        else:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'ViT.ckpt'), target=None)\n",
    "        self.state = train_state.TrainState.create(\n",
    "                                       apply_fn=self.model.apply,\n",
    "                                       params=params,\n",
    "                                       tx=self.state.tx if self.state else optax.adamw(self.lr)  # Default optimizer\n",
    "                                      )\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        # Check whether a pretrained model exist for this autoencoder\n",
    "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'ViT.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gcsY1Mjaf_s"
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Commonly, Vision Transformers are applied to large-scale image classification benchmarks such as ImageNet to leverage their full potential. However, here we take a step back and ask: can Vision Transformer also succeed on classical, small benchmarks such as CIFAR10? To find this out, we train a Vision Transformer from scratch on the CIFAR10 dataset. Let's first create a training function for our PyTorch Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYekdfZBaiTy"
   },
   "outputs": [],
   "source": [
    "def train_model(*args, num_epochs=30, **kwargs):\n",
    "    # Create a trainer module with specified hyperparameters\n",
    "    trainer = TrainerModule(*args, **kwargs)\n",
    "    trainer.train_model(train_loader, val_loader, num_epochs=num_epochs)\n",
    "    trainer.load_model()\n",
    "\n",
    "    # Test trained model\n",
    "    val_acc = trainer.eval_model(val_loader)\n",
    "    test_acc = trainer.eval_model(test_loader)\n",
    "    return trainer, {'val': val_acc, 'test': test_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPex0rz3auId"
   },
   "source": [
    "Now, we can already start training our model. As seen in our implementation, we have a couple of hyperparameters that we have to set. When creating this notebook, we have performed a small grid search over hyperparameters and listed the best hyperparameters in the cell below. Nevertheless, it is worth discussing the influence that each hyperparameter has, and what intuition we have for choosing its value.\n",
    "\n",
    "First, let's consider the patch size. The smaller we make the patches, the longer the input sequences to the Transformer become. While in general, this allows the Transformer to model more complex functions, it requires a longer computation time due to its quadratic memory usage in the attention layer. Furthermore, small patches can make the task more difficult since the Transformer has to learn which patches are close-by, and which are far away. We experimented with patch sizes of 2, 4, and 8 which gives us the input sequence lengths of 256, 64, and 16 respectively. We found 4 to result in the best performance and hence pick it below. \n",
    "\n",
    "Next, the embedding and hidden dimensionality have a similar impact on a Transformer as to an MLP. The larger the sizes, the more complex the model becomes, and the longer it takes to train. In Transformers, however, we have one more aspect to consider: the query-key sizes in the Multi-Head Attention layers. Each key has the feature dimensionality of `embed_dim/num_heads`. Considering that we have an input sequence length of 64, a minimum reasonable size for the key vectors is 16 or 32. Lower dimensionalities can restrain the possible attention maps too much. We observed that more than 8 heads are not necessary for the Transformer, and therefore pick an embedding dimensionality of `256`. The hidden dimensionality in the feed-forward networks is usually 2-4x larger than the embedding dimensionality, and thus we pick `512`. \n",
    "\n",
    "Finally, the learning rate for Transformers is usually relatively small, and in papers, a common value to use is 3e-5. However, since we work with a smaller dataset and have a potentially easier task, we found that we are able to increase the learning rate to 3e-4 without any problems. To reduce overfitting, we use a dropout value of 0.2. Remember that we also use small image augmentations as regularization during training.\n",
    "\n",
    "Feel free to explore the hyperparameters yourself by changing the values below. In general, the Vision Transformer did not show to be too sensitive to the hyperparameter choices on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfHEpqp3DZbR"
   },
   "outputs": [],
   "source": [
    "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
    "%tensorboard --logdir ../../saved_models/tutorial_part1/ViT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoTsOIRQSPV0"
   },
   "outputs": [],
   "source": [
    "model, results = train_model(exmp_imgs=next(iter(train_loader))[0],\n",
    "                             embed_dim=256,\n",
    "                             hidden_dim=512,\n",
    "                             num_heads=8,\n",
    "                             num_layers=6,\n",
    "                             patch_size=4,\n",
    "                             num_channels=3,\n",
    "                             num_patches=64,\n",
    "                             num_classes=10,\n",
    "                             dropout_prob=0.2,\n",
    "                             lr=3e-4)\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFbyKo__iT6y"
   },
   "source": [
    "Please note that at the end of the 30 training epochs, the accuracy on the validation set should be about 71%. By performing a training of 100 epochs, on the other hand, the achievable accuracy is approximately 76%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMorvahXs9wi"
   },
   "source": [
    "---\n",
    "\n",
    "This notebook is based on the work featured in the following [tutorial](https://github.com/phlippe/uvadlc_notebooks/)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
